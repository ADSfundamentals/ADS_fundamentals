{"title":"Lab 4","markdown":{"yaml":{"title":"Lab 4","output-file":"lab4.html","params":{"answers":false}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\n\n------------------------------------------------------------------------\n\nIn this practical, the assumptions of the linear regression model will be discussed. You will practice with checking the different assumptions, and practice with accounting for some of the assumptions with additional steps. Please note that these assumptions can only be checked once you have selected a (final) model, as these assumptions 'need' a model that they apply to.\n\nWe will use the following packages in this practical:\n\n```{r, message=FALSE, warning=FALSE}\nlibrary(magrittr)\nlibrary(ggplot2) \nlibrary(regclass)\nlibrary(MASS) \n```\n\n------------------------------------------------------------------------\n\n# Data set: Loading & Inspection\n\nFor the first part of this practical, a [dataset](Fish.csv) from a fish market is used. The variables in this fish data set are:\n\n-   Species of the fish\n\n-   Weight of the fish in grams\n\n-   Vertical, length of the fish in cm\n\n-   Diagonal length of the fish in cm\n\n-   Cross length of the fish in cm\n\n-   Height of the fish in cm\n\n-   Diagonal width of the fish in cm\n\n    </li>\n\nDownload the dataset from the GitHub link, store it in the folder of your `Rproject` for this practical and open it in `R`. Also, adjust the column names according to the code below to make them a bit more intuitive.\n\n```{r}\n# Read in the data set\ndata_fish <- read.csv(\"Fish.csv\")\n\ncolnames(data_fish) <- c(\"species\", \"weigth\", \"vertical_length\", \"diagonal_length\", \"cross_length\", \"height\", \"diagonal_width\")\n\n# Check the data set with the 'head' function to have a general impression.\ndata_fish %>%\n  head()\n```\n\n------------------------------------------------------------------------\n\n# Model assumptions\n\nWe will now discuss and check the various model assumptions of linear regression. If steps can be taken to account for a violated assumption, this will also be touched upon.\n\n------------------------------------------------------------------------\n\n## Linearity\n\nWith the assumption of linearity, it is assumed that the relation between the dependent and independent variables is (more or less) linear. You can check this by generating a scatterplot using a predictor variable and outcome variable of the regression model.\n\n1.  **Check whether there is a linear relation between the variables vertical length and the cross length.**\n\n```{r message=FALSE, warning=FALSE, include=params$answers}\nggplot(data_fish, \n       aes(x = vertical_length, y = cross_length)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ggtitle(\"linear relation is present\") +\n  xlab(\"vertical length in cm\") +\n  ylab(\"cross length in cm\")\n```\n\n2.  **Next check the relation between weight and height.**\n\n```{r message=FALSE, warning=FALSE, include=params$answers}\nggplot(data_fish, aes(x = weigth, y = height)) + \n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ggtitle(\"linear relation is missing\") +\n  xlab(\"Weigth in gram\") +\n  ylab(\"heigth in cm\")\n\n```\n\n3.  **Describe both plots. What differences do you see?**\n\n```{r, include = params$answers}\n# The first plot shows a case where there is a more or less linear relation (Vertical length of the fish and cross length of the fish). In the second plot, the relation is clearly not linear.\n```\n\nWhen a non-linear relation is present, you can either choose another model to use, or transform the predictor before adding it to the model, for example using a log-transformation. Applying a transformation, however, will not always solve the problem, and makes interpretation of the model less intuitive.\n\n4.  **Apply a log-transformation to the weight variable.**\n\n```{r, include = params$answers}\ndata_fish$weigth_trans <- data_fish$weigth %>% \n  log()\n```\n\n5.  **Plot the relation between length and weight again, but now including the transformed variable.**\n\n```{r message=FALSE, warning=FALSE, include=params$answers}\nggplot(data_fish, aes(x = weigth_trans, y = height)) + \n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  ggtitle(\"linear relation improved\") +\n  xlab(\"Weigth in gram\") +\n  ylab(\"heigth in cm\")\n```\n\n6.  **Describe if the transformation improved the linear relation.**\n\n```{r, include = params$answers}\n# You see that the relation is still not completely linear, but a lot more linear than before the transformation (plot 2).\n```\n\n------------------------------------------------------------------------\n\n## Predictor matrix full rank\n\nThis assumption states that:\n\n<li>\n\nthere need to be more observations than predictors (n \\> P).\n\n<li>no predictor can be a linear combination of other predictors; predictors cannot have a very high correlation (multicollinearity).</li>\n\n<br>\n\nThe first part of this assumption is easy to check: see if the number of observations minus the number of predictors is a positive number. The second part can be checked by either obtaining correlations between the predictors, or by determining the VIF (variance inflation factor). When the VIF is above 10, this indicate high multicollinearity. To account for this, predictors can be excluded from the model, or a new variable can be constructed based on predictors with a high correlation.\n\nTo examine VIF scores, the function `VIF()` from the `regclass` can be used on a prespecified model. If this model includes a categorical variable with multiple categories, such as 'species' in the example data, the generalized VIF is used, and we have to look at the third column (GVIF\\^\\*(1/(2\\*Df))), these values can be compared to normal VIF values.\n\n7.  **Specify a linear model with `weight` as outcome variable using all other variables in the dataset as predictors. Save this model as `model_fish1`. Calculate VIF values for this model.**\n\n```{r, include = params$answers}\nmodel_fish1 <- lm(weigth ~., \n                  data = data_fish[,1:7])\n\nmodel_fish1 %>%\n  VIF()\n```\n\n8.  **Check the VIF scores. If VIF scores exceed a score of 10, give substantial explanation why the VIF scores are this high.**\n\n```{r, include = params$answers}\n# The VIF values in this first model are extreme in some cases, more specifically for the three variables that all measure some aspect of length, it makes sense that these values can be highly correlated. One way to solve this, is excluding predictors that almost measure the same thing as another variable.\n```\n\n9.  **What adjustments can be made to the model to account for multicollinearity in this case?**\n\n```{r, include = params$answers}\n# A straightforward solution is to include only one of the variables measuring an aspect of length. More elaborate solutions exist but are not covered in this course.\n```\n\n10. **Run a new model which only includes one of the three length variables and call it `model_fish2`. Describe if there is an improvement.**\n\n```{r, include = params$answers}\nmodel_fish2 <- lm(weigth ~ species + diagonal_length + height + diagonal_width, \n             data = data_fish)\n\nmodel_fish2 %>%\n  VIF()\n\n# We chose to go with a model which only includes `diagonal_length`, as this variable had the highest VIF value and therefore is able to best grasp the variance that is also measured by the other two length variables. However which strategy is most appropriate can differ per situation. We see now that the VIF values have returned to 'normal' values (although still a bit high).\n```\n\n11. **What happens with the regression model when there are more predictors than observations?**\n\n```{r, include = params$answers}\n# If there are more predictors than observations, the model can not be identified and the parameters cannot be estimated\n```\n\n------------------------------------------------------------------------\n\n## Exogenous predictors\n\nFor this assumption, the expected value of the errors (mean of the errors) must be 0. Furthermore, The errors must be independent of the predictors.\n\n12. **What is the possible consequence of not meeting this assumption?**\n\n```{r, include = params$answers}\n# Not meeting this assumption results in biased estimates for the regression coefficients.\n```\n\n------------------------------------------------------------------------\n\n## Constant, finite error variance\n\nThis assumptions is also called 'the assumption of homoscedasticity'. It states that the variance of the error terms should be constant over all levels of the predictors. This can be checked by plotting the residuals against the fitted values. These plots can be obtained by simply taking the first plot of a specified model, `plot(model_x)`.\n\n13. **Create a residual vs fitted values plot for `model_fish1`, which is the first plot generated by the `plot()` function.**\n\n```{r, include = params$answers}\nmodel_fish1 %>%\n  plot(1)\n```\n\n14. **Load in the `iris` data, and specify a model where sepal length is predicted by all other variables and save this as `model_iris1`.**\n\n```{r, include = params$answers}\ndata_iris   <- iris\nmodel_iris1 <- lm(Sepal.Length ~ ., \n                  data = data_iris)\n```\n\n15. **Create a residual vs fitted plot for this model as well.**\n\n```{r, include = params$answers}\nmodel_iris1 %>%\n  plot(1)\n```\n\n16. **Discuss both plots and indicate whether the assumption is met.**\n\n```{r, include = params$answers}\n# In the `iris_data` plot, it can be seen that the red line is quite constant. Also, the dots seem to have a rather constant variance. In the `fish_data` plot, however, the variance in error terms seems smaller for the lower values than for the higher values. This second plot indicates heteroscedasticity and indicates that the assumption is violated.\n```\n\n17. **Discuss what the consequence would be if this assumption is violated.**\n\n```{r, include = params$answers}\n# If this assumption is violated, estimated standard errors are biased.\n```\n\n------------------------------------------------------------------------\n\n## Independent errors\n\nThis assumption states that error terms should have no correlation. Dependence of the errors can result from multiple things. First, there is a possible dependence in the error terms when there is serial dependence, for example because the data contains variables that are measured over time. Another reason can be when there is a cluster structure in the data, for example students in classes in schools.\n\n18. **How can both causes of correlated error terms be detected, and what can be done to solve the problem?**\n\n```{r, include = params$answers}\n# Temporal dependence can be checked by investigating the autocorrelation, while clustered data can be found by investigating the intra class correlation (ICC).\n\n# More important: dealing with these dependencies requires another model (multilevel for clustered data, or a model that account for the time aspect). Those models are out of the scope of this course, but always be aware of a theoretical dependency between your errors.\n```\n\n------------------------------------------------------------------------\n\n## Normally distributed errors\n\nThis assumption states that errors should be roughly normally distributed. Like the assumption of homoscedasticity, this can be checked by model plots, provided by R.\n\n19. **Create a QQ plot for `model_iris1`, which is the second plot generated by the `plot()` function. Indicate whether the assumption is met.**\n\n```{r, include = params$answers}\nmodel_iris1 %>%\n  plot(2)\n```\n\n20. **Create a new model using the fish data, where `diagonal_width` is predicted by `cross_length`, and store the model as `model_fish3`.**\n\n```{r, include = params$answers}\nmodel_fish3 <- lm(diagonal_width ~ cross_length, \n                  data = data_fish)\n```\n\n21. **Create a QQ plot for `model_fish3`.**\n\n```{r, include = params$answers}\nmodel_fish3 %>%\n  plot(2)\n```\n\n22. **Interpret the two plots. Is the assumption met in both cases?**\n\n```{r, include = params$answers}\n# In the two plots above, QQ plots are provided for the 2 models. For the first model, the error terms follow the ideal line pretty well, and the assumption holds. In the second plot, the tails deviate quite a lot from the intended line, and it can be debated that the assumption is violated.\n```\n\n23. **In what cases is it problematic that the assumption is not met? And in what cases is it no problem?**\n\n```{r, include = params$answers}\n# The assumption is important in smaller samples (n < 30). In bigger samples, violating the assumption is less of a big problem. For prediction intervals however, normality of errors is always wanted.\n```\n\n------------------------------------------------------------------------\n\n# Influential observations\n\n------------------------------------------------------------------------\n\n## Outliers\n\nOutliers are observations that show extreme outcomes compared to the other data, or observations with outcome values that fit the model very badly. Outliers can be detected by inspecting the *externally studentized residuals*.\n\n24. **Make a plot of studentized residuals by using the functions `rstudent` and `plot` for \\`model_fish1. What do you conclude?**\n\n```{r, include = params$answers}\nmodel_fish1 %>%\n  rstudent() %>%\n  plot()\n\n# There is at least one clear outlier around observation number 70.\n```\n\n25. **Make a plot of studentized residuals for `model_iris1`.**\n\n```{r, include = params$answers}\nmodel_iris1 %>%\n  rstudent() %>%\n  plot()\n```\n\n26. **Store the dataset `Animals` from the `MASS` package. Define a regression model where animals' body weight is predicted by brain weight and store it as `model_animals1`.**\n\n```{r, include = params$answers}\ndata_animals   <- Animals\nmodel_animals1 <- lm(body ~ brain,\n                     data = data_animals)\n\n# There are not really any clear outliers to worry about.\n```\n\n27. **Make a plot of the studentized residuals for `model_animals1`.**\n\n```{r, include = params$answers}\nmodel_animals1 %>%\n  rstudent() %>%\n  plot()\n\n# Observation number 26 is an extreme outlier.\n```\n\n------------------------------------------------------------------------\n\n## High-leverage observations\n\nHigh-leverage observations are observations with extreme predictor values. To detect these observations, we look at their *leverage values*. These values can be summarized in a leverage plot.\n\n28. **For the model specified under model_animals1, create a leverage plot by plotting the `hatvalues()` of the model.**\n\n```{r, include = params$answers}\nmodel_animals1 %>%\n  hatvalues() %>%\n  plot()\n\n# In the leverage plot, observation 7 and 15 stand out from the other observations. When you look at the data set, you can notice that both of these observations are elephant species.\n\n# A case with high leverage is not necessarily bad: the influence on the model is more important.\n```\n\n------------------------------------------------------------------------\n\n## Influence on the model\n\nBoth outliers and observations with high leverage are not necessarily a problem. Cases that are both, however, seem to form more of a problem. These cases can influence the model heavily and can therefore be problematic.\n\nInfluence measures come in two sorts: Cook's distance checks for influential observations, while DFBETAS check for influential, and possible problematic, observations per regression coefficients.\n\n29. **For `model_animals1`, check Cooks distance by plotting the `cooks.distance` of the model.**\n\n```{r, include = params$answers}\nmodel_animals1 %>%\n  cooks.distance() %>%\n  plot()\n```\n\n30. **For `model_animals1`, check the DFBETAS by using the function `dfbetas`.**\n\n```{r, include = params$answers}\nplot(dfbetas(model_animals1)[,1],\n     main = \"intercept\")\n\nplot(dfbetas(model_animals1)[,2],\n     main = \"slope\")\n\n# Note that because of the structure of the output of `dfbetas` it is not very convenient to process it using a pipe structure.\n```\n\n31. **Describe what you see in the plots for Cook's distance and DFBETAS. What do you conclude?**\n\n```{r, include = params$answers}\n# Case 26, the earlier spotted outlier, has in all three plots an outstanding value. There is reason to assume that this observation is problematic.\n```\n\n32. **Delete the problematic observation that you found in Question 12 and store the dataset under a new name.**\n\n```{r, include = params$answers}\ndata_animals2 <- data_animals[-26,]\n```\n\n33. **Fit the regression model where animals' body weight is predicted by brain weight using the adjusted dataset and store it as `model_animals2`.**\n\n```{r, include = params$answers}\nmodel_animals2 <- lm(body ~ brain, \n                     data = data_animals2)\n```\n\n34. **Compare the output to `model_animals1` and describe the changes.**\n\n```{r, include = params$answers}\nsummary(model_animals1)\nsummary(model_animals2)\n\n# We see that the model changes quite a bit: the intercept becomes much lower, and the slope even changes direction (negative to positive).\n```\n\n35. **Run the plots for influential observations again on this new model and see if anything changes.**\n\n```{r, include = params$answers}\nmodel_animals2 %>%\n  cooks.distance() %>%\n  plot()\n\nplot(dfbetas(model_animals2)[,1],\n     main = \"intercept\")\n\nplot(dfbetas(model_animals2)[,2],\n     main = \"slope\")\n\n# We see that new influential observations arise. These were earlier overshadowed by observation 26. If you look at these cases, you see these are the cases with very heavy animals. In this case the solution should be to transform the data and take the log of the weights, instead of these values. This means that the assumption of linearity was probably not met for this data set.*\n```\n","srcMarkdownNoYaml":"\n\n# Introduction\n\n------------------------------------------------------------------------\n\nIn this practical, the assumptions of the linear regression model will be discussed. You will practice with checking the different assumptions, and practice with accounting for some of the assumptions with additional steps. Please note that these assumptions can only be checked once you have selected a (final) model, as these assumptions 'need' a model that they apply to.\n\nWe will use the following packages in this practical:\n\n```{r, message=FALSE, warning=FALSE}\nlibrary(magrittr)\nlibrary(ggplot2) \nlibrary(regclass)\nlibrary(MASS) \n```\n\n------------------------------------------------------------------------\n\n# Data set: Loading & Inspection\n\nFor the first part of this practical, a [dataset](Fish.csv) from a fish market is used. The variables in this fish data set are:\n\n-   Species of the fish\n\n-   Weight of the fish in grams\n\n-   Vertical, length of the fish in cm\n\n-   Diagonal length of the fish in cm\n\n-   Cross length of the fish in cm\n\n-   Height of the fish in cm\n\n-   Diagonal width of the fish in cm\n\n    </li>\n\nDownload the dataset from the GitHub link, store it in the folder of your `Rproject` for this practical and open it in `R`. Also, adjust the column names according to the code below to make them a bit more intuitive.\n\n```{r}\n# Read in the data set\ndata_fish <- read.csv(\"Fish.csv\")\n\ncolnames(data_fish) <- c(\"species\", \"weigth\", \"vertical_length\", \"diagonal_length\", \"cross_length\", \"height\", \"diagonal_width\")\n\n# Check the data set with the 'head' function to have a general impression.\ndata_fish %>%\n  head()\n```\n\n------------------------------------------------------------------------\n\n# Model assumptions\n\nWe will now discuss and check the various model assumptions of linear regression. If steps can be taken to account for a violated assumption, this will also be touched upon.\n\n------------------------------------------------------------------------\n\n## Linearity\n\nWith the assumption of linearity, it is assumed that the relation between the dependent and independent variables is (more or less) linear. You can check this by generating a scatterplot using a predictor variable and outcome variable of the regression model.\n\n1.  **Check whether there is a linear relation between the variables vertical length and the cross length.**\n\n```{r message=FALSE, warning=FALSE, include=params$answers}\nggplot(data_fish, \n       aes(x = vertical_length, y = cross_length)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ggtitle(\"linear relation is present\") +\n  xlab(\"vertical length in cm\") +\n  ylab(\"cross length in cm\")\n```\n\n2.  **Next check the relation between weight and height.**\n\n```{r message=FALSE, warning=FALSE, include=params$answers}\nggplot(data_fish, aes(x = weigth, y = height)) + \n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ggtitle(\"linear relation is missing\") +\n  xlab(\"Weigth in gram\") +\n  ylab(\"heigth in cm\")\n\n```\n\n3.  **Describe both plots. What differences do you see?**\n\n```{r, include = params$answers}\n# The first plot shows a case where there is a more or less linear relation (Vertical length of the fish and cross length of the fish). In the second plot, the relation is clearly not linear.\n```\n\nWhen a non-linear relation is present, you can either choose another model to use, or transform the predictor before adding it to the model, for example using a log-transformation. Applying a transformation, however, will not always solve the problem, and makes interpretation of the model less intuitive.\n\n4.  **Apply a log-transformation to the weight variable.**\n\n```{r, include = params$answers}\ndata_fish$weigth_trans <- data_fish$weigth %>% \n  log()\n```\n\n5.  **Plot the relation between length and weight again, but now including the transformed variable.**\n\n```{r message=FALSE, warning=FALSE, include=params$answers}\nggplot(data_fish, aes(x = weigth_trans, y = height)) + \n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  ggtitle(\"linear relation improved\") +\n  xlab(\"Weigth in gram\") +\n  ylab(\"heigth in cm\")\n```\n\n6.  **Describe if the transformation improved the linear relation.**\n\n```{r, include = params$answers}\n# You see that the relation is still not completely linear, but a lot more linear than before the transformation (plot 2).\n```\n\n------------------------------------------------------------------------\n\n## Predictor matrix full rank\n\nThis assumption states that:\n\n<li>\n\nthere need to be more observations than predictors (n \\> P).\n\n<li>no predictor can be a linear combination of other predictors; predictors cannot have a very high correlation (multicollinearity).</li>\n\n<br>\n\nThe first part of this assumption is easy to check: see if the number of observations minus the number of predictors is a positive number. The second part can be checked by either obtaining correlations between the predictors, or by determining the VIF (variance inflation factor). When the VIF is above 10, this indicate high multicollinearity. To account for this, predictors can be excluded from the model, or a new variable can be constructed based on predictors with a high correlation.\n\nTo examine VIF scores, the function `VIF()` from the `regclass` can be used on a prespecified model. If this model includes a categorical variable with multiple categories, such as 'species' in the example data, the generalized VIF is used, and we have to look at the third column (GVIF\\^\\*(1/(2\\*Df))), these values can be compared to normal VIF values.\n\n7.  **Specify a linear model with `weight` as outcome variable using all other variables in the dataset as predictors. Save this model as `model_fish1`. Calculate VIF values for this model.**\n\n```{r, include = params$answers}\nmodel_fish1 <- lm(weigth ~., \n                  data = data_fish[,1:7])\n\nmodel_fish1 %>%\n  VIF()\n```\n\n8.  **Check the VIF scores. If VIF scores exceed a score of 10, give substantial explanation why the VIF scores are this high.**\n\n```{r, include = params$answers}\n# The VIF values in this first model are extreme in some cases, more specifically for the three variables that all measure some aspect of length, it makes sense that these values can be highly correlated. One way to solve this, is excluding predictors that almost measure the same thing as another variable.\n```\n\n9.  **What adjustments can be made to the model to account for multicollinearity in this case?**\n\n```{r, include = params$answers}\n# A straightforward solution is to include only one of the variables measuring an aspect of length. More elaborate solutions exist but are not covered in this course.\n```\n\n10. **Run a new model which only includes one of the three length variables and call it `model_fish2`. Describe if there is an improvement.**\n\n```{r, include = params$answers}\nmodel_fish2 <- lm(weigth ~ species + diagonal_length + height + diagonal_width, \n             data = data_fish)\n\nmodel_fish2 %>%\n  VIF()\n\n# We chose to go with a model which only includes `diagonal_length`, as this variable had the highest VIF value and therefore is able to best grasp the variance that is also measured by the other two length variables. However which strategy is most appropriate can differ per situation. We see now that the VIF values have returned to 'normal' values (although still a bit high).\n```\n\n11. **What happens with the regression model when there are more predictors than observations?**\n\n```{r, include = params$answers}\n# If there are more predictors than observations, the model can not be identified and the parameters cannot be estimated\n```\n\n------------------------------------------------------------------------\n\n## Exogenous predictors\n\nFor this assumption, the expected value of the errors (mean of the errors) must be 0. Furthermore, The errors must be independent of the predictors.\n\n12. **What is the possible consequence of not meeting this assumption?**\n\n```{r, include = params$answers}\n# Not meeting this assumption results in biased estimates for the regression coefficients.\n```\n\n------------------------------------------------------------------------\n\n## Constant, finite error variance\n\nThis assumptions is also called 'the assumption of homoscedasticity'. It states that the variance of the error terms should be constant over all levels of the predictors. This can be checked by plotting the residuals against the fitted values. These plots can be obtained by simply taking the first plot of a specified model, `plot(model_x)`.\n\n13. **Create a residual vs fitted values plot for `model_fish1`, which is the first plot generated by the `plot()` function.**\n\n```{r, include = params$answers}\nmodel_fish1 %>%\n  plot(1)\n```\n\n14. **Load in the `iris` data, and specify a model where sepal length is predicted by all other variables and save this as `model_iris1`.**\n\n```{r, include = params$answers}\ndata_iris   <- iris\nmodel_iris1 <- lm(Sepal.Length ~ ., \n                  data = data_iris)\n```\n\n15. **Create a residual vs fitted plot for this model as well.**\n\n```{r, include = params$answers}\nmodel_iris1 %>%\n  plot(1)\n```\n\n16. **Discuss both plots and indicate whether the assumption is met.**\n\n```{r, include = params$answers}\n# In the `iris_data` plot, it can be seen that the red line is quite constant. Also, the dots seem to have a rather constant variance. In the `fish_data` plot, however, the variance in error terms seems smaller for the lower values than for the higher values. This second plot indicates heteroscedasticity and indicates that the assumption is violated.\n```\n\n17. **Discuss what the consequence would be if this assumption is violated.**\n\n```{r, include = params$answers}\n# If this assumption is violated, estimated standard errors are biased.\n```\n\n------------------------------------------------------------------------\n\n## Independent errors\n\nThis assumption states that error terms should have no correlation. Dependence of the errors can result from multiple things. First, there is a possible dependence in the error terms when there is serial dependence, for example because the data contains variables that are measured over time. Another reason can be when there is a cluster structure in the data, for example students in classes in schools.\n\n18. **How can both causes of correlated error terms be detected, and what can be done to solve the problem?**\n\n```{r, include = params$answers}\n# Temporal dependence can be checked by investigating the autocorrelation, while clustered data can be found by investigating the intra class correlation (ICC).\n\n# More important: dealing with these dependencies requires another model (multilevel for clustered data, or a model that account for the time aspect). Those models are out of the scope of this course, but always be aware of a theoretical dependency between your errors.\n```\n\n------------------------------------------------------------------------\n\n## Normally distributed errors\n\nThis assumption states that errors should be roughly normally distributed. Like the assumption of homoscedasticity, this can be checked by model plots, provided by R.\n\n19. **Create a QQ plot for `model_iris1`, which is the second plot generated by the `plot()` function. Indicate whether the assumption is met.**\n\n```{r, include = params$answers}\nmodel_iris1 %>%\n  plot(2)\n```\n\n20. **Create a new model using the fish data, where `diagonal_width` is predicted by `cross_length`, and store the model as `model_fish3`.**\n\n```{r, include = params$answers}\nmodel_fish3 <- lm(diagonal_width ~ cross_length, \n                  data = data_fish)\n```\n\n21. **Create a QQ plot for `model_fish3`.**\n\n```{r, include = params$answers}\nmodel_fish3 %>%\n  plot(2)\n```\n\n22. **Interpret the two plots. Is the assumption met in both cases?**\n\n```{r, include = params$answers}\n# In the two plots above, QQ plots are provided for the 2 models. For the first model, the error terms follow the ideal line pretty well, and the assumption holds. In the second plot, the tails deviate quite a lot from the intended line, and it can be debated that the assumption is violated.\n```\n\n23. **In what cases is it problematic that the assumption is not met? And in what cases is it no problem?**\n\n```{r, include = params$answers}\n# The assumption is important in smaller samples (n < 30). In bigger samples, violating the assumption is less of a big problem. For prediction intervals however, normality of errors is always wanted.\n```\n\n------------------------------------------------------------------------\n\n# Influential observations\n\n------------------------------------------------------------------------\n\n## Outliers\n\nOutliers are observations that show extreme outcomes compared to the other data, or observations with outcome values that fit the model very badly. Outliers can be detected by inspecting the *externally studentized residuals*.\n\n24. **Make a plot of studentized residuals by using the functions `rstudent` and `plot` for \\`model_fish1. What do you conclude?**\n\n```{r, include = params$answers}\nmodel_fish1 %>%\n  rstudent() %>%\n  plot()\n\n# There is at least one clear outlier around observation number 70.\n```\n\n25. **Make a plot of studentized residuals for `model_iris1`.**\n\n```{r, include = params$answers}\nmodel_iris1 %>%\n  rstudent() %>%\n  plot()\n```\n\n26. **Store the dataset `Animals` from the `MASS` package. Define a regression model where animals' body weight is predicted by brain weight and store it as `model_animals1`.**\n\n```{r, include = params$answers}\ndata_animals   <- Animals\nmodel_animals1 <- lm(body ~ brain,\n                     data = data_animals)\n\n# There are not really any clear outliers to worry about.\n```\n\n27. **Make a plot of the studentized residuals for `model_animals1`.**\n\n```{r, include = params$answers}\nmodel_animals1 %>%\n  rstudent() %>%\n  plot()\n\n# Observation number 26 is an extreme outlier.\n```\n\n------------------------------------------------------------------------\n\n## High-leverage observations\n\nHigh-leverage observations are observations with extreme predictor values. To detect these observations, we look at their *leverage values*. These values can be summarized in a leverage plot.\n\n28. **For the model specified under model_animals1, create a leverage plot by plotting the `hatvalues()` of the model.**\n\n```{r, include = params$answers}\nmodel_animals1 %>%\n  hatvalues() %>%\n  plot()\n\n# In the leverage plot, observation 7 and 15 stand out from the other observations. When you look at the data set, you can notice that both of these observations are elephant species.\n\n# A case with high leverage is not necessarily bad: the influence on the model is more important.\n```\n\n------------------------------------------------------------------------\n\n## Influence on the model\n\nBoth outliers and observations with high leverage are not necessarily a problem. Cases that are both, however, seem to form more of a problem. These cases can influence the model heavily and can therefore be problematic.\n\nInfluence measures come in two sorts: Cook's distance checks for influential observations, while DFBETAS check for influential, and possible problematic, observations per regression coefficients.\n\n29. **For `model_animals1`, check Cooks distance by plotting the `cooks.distance` of the model.**\n\n```{r, include = params$answers}\nmodel_animals1 %>%\n  cooks.distance() %>%\n  plot()\n```\n\n30. **For `model_animals1`, check the DFBETAS by using the function `dfbetas`.**\n\n```{r, include = params$answers}\nplot(dfbetas(model_animals1)[,1],\n     main = \"intercept\")\n\nplot(dfbetas(model_animals1)[,2],\n     main = \"slope\")\n\n# Note that because of the structure of the output of `dfbetas` it is not very convenient to process it using a pipe structure.\n```\n\n31. **Describe what you see in the plots for Cook's distance and DFBETAS. What do you conclude?**\n\n```{r, include = params$answers}\n# Case 26, the earlier spotted outlier, has in all three plots an outstanding value. There is reason to assume that this observation is problematic.\n```\n\n32. **Delete the problematic observation that you found in Question 12 and store the dataset under a new name.**\n\n```{r, include = params$answers}\ndata_animals2 <- data_animals[-26,]\n```\n\n33. **Fit the regression model where animals' body weight is predicted by brain weight using the adjusted dataset and store it as `model_animals2`.**\n\n```{r, include = params$answers}\nmodel_animals2 <- lm(body ~ brain, \n                     data = data_animals2)\n```\n\n34. **Compare the output to `model_animals1` and describe the changes.**\n\n```{r, include = params$answers}\nsummary(model_animals1)\nsummary(model_animals2)\n\n# We see that the model changes quite a bit: the intercept becomes much lower, and the slope even changes direction (negative to positive).\n```\n\n35. **Run the plots for influential observations again on this new model and see if anything changes.**\n\n```{r, include = params$answers}\nmodel_animals2 %>%\n  cooks.distance() %>%\n  plot()\n\nplot(dfbetas(model_animals2)[,1],\n     main = \"intercept\")\n\nplot(dfbetas(model_animals2)[,2],\n     main = \"slope\")\n\n# We see that new influential observations arise. These were earlier overshadowed by observation 26. If you look at these cases, you see these are the cases with very heavy animals. In this case the solution should be to transform the data and take the log of the weights, instead of these values. This means that the assumption of linearity was probably not met for this data set.*\n```\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"lab4.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","theme":["lumen"],"title":"Lab 4","params":{"answers":false}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}