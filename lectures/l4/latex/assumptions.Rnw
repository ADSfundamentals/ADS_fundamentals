%%% Title:    Regression Assumptions & Diagnostics
%%% Author:   Kyle M. Lang
%%% Created:  2018-04-12
%%% Modified: 2025-11-28

\documentclass[10pt]{beamer}
\usetheme{Utrecht}

\usepackage{graphicx}
\usepackage[natbibapa]{apacite}
\usepackage[libertine]{newtxmath}
\usepackage{caption}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{booktabs}
\usepackage{relsize}
\usepackage{tikz} 
\usetikzlibrary{arrows,calc,patterns,positioning,shapes,decorations.markings} 
\usetikzlibrary{decorations.pathmorphing} 

\graphicspath{{images/}}

\hypersetup{
  colorlinks = false,
  linkcolor = blue,
  filecolor = blue,
  citecolor = black,
  urlcolor = blue
}

\definecolor{codebackground}{RGB}{224,234,238}
\definecolor{codestring}{RGB}{191,3,3}
\definecolor{codekeyword}{RGB}{1,1,129}
\definecolor{codecomment}{RGB}{131,129,131}

\newtcbox{\src}
  {%
    verbatim,
    fontupper = \ttfamily,
    colback = codebackground, 
    colframe = codebackground,
    left = 0pt, 
    right = 0pt, 
  }

\newtcbox{\srcT}
  {%
    verbatim,
    fontupper = \ttfamily,
    colback = codebackground, 
    colframe = codebackground,
    left = 0pt, 
    right = 0pt, 
    height = 14pt,
    valign = bottom,
  }

\newcommand{\eqit}[1]{\textrm{\textit{#1}}}
\newcommand{\rmsc}[1]{\textrm{\textsc{#1}}}
\newcommand{\pkg}[1]{\textbf{#1}}

\newcommand{\tightpipe}{\texttt{|>}}
\newcommand{\pipe}{\tightpipe~}
\newcommand{\expipe}{\texttt{\%\$\%}}
\newcommand{\apipe}{\texttt{\%<>\%}}

\title{Regression Assumptions \& Diagnostics}
\subtitle{Fundamental Techniques in Data Science with R}
\author{Kyle M. Lang}
\institute{Department of Methodology \& Statistics\\Utrecht University}
\date{}

\begin{document}

<<setup, include = FALSE, cache = FALSE>>=
set.seed(235711)

dataDir <- "data"
codeDir <- "code"

library(knitr)
library(ggplot2)
library(MASS)
library(DAAG)
library(xtable)
library(MLmetrics)
library(dplyr)

source(here::here(codeDir, "supportFunctions.R"))

options(width = 60)
opts_chunk$set(size = "footnotesize",
               fig.align = "center",
               fig.path = "figure/assumptions-",
               message = FALSE,
               comment = "",
               root.dir = here::here("./")
)
knit_theme$set('edit-kwrite')
@

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}[t,plain]
  \titlepage
\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Outline}
  \tableofcontents
\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\section{Motivation}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Assumptions for Identification}

  Consider the following equation:
  \begin{align*}
    5 = x + y
  \end{align*}
  What are the values of $x$ and $y$?
  \pause
  \begin{align*}
    y = 5 - x
  \end{align*}
  \pause
  What if we assume that $y = x$?
  \pause
  \begin{align*}
    5 &= x + y\\
    0 &= x - y
  \end{align*}
  \pause
  Now we have enough information:
  \begin{align*}
    5 = x + x = 2x~~\Rightarrow~~x = y = 2.5
  \end{align*}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\def\fw{25mm}\relax  % Factor node width

\begin{frame}{Assumptions for Logical Inference}

  In statistical modeling, we combine a priori \emph{assumptions} and empirical \emph{observations} to make logical
  \emph{inferences} about the population.

  \begin{align*}
    \{\mathcal{A}, \mathcal{O}\} \Rightarrow \mathcal{I}
  \end{align*}

  \begin{figure}
    \scalebox{0.7}{
      \begin{tikzpicture}[
          box/.style = {rectangle, draw = black, very thick, minimum size = 7mm},
          arrow/.style = {->, very thick},
        ]
        \usetikzlibrary{shapes.geometric}

        %% Nodes
        \node[box, draw] at (0, 1) (assume) {
          \parbox{\fw}{\centering Assumptions}
        };

        \node[box, draw] at (0, -1) (observe) {
          \parbox{\fw}{\centering Observations}
        };

        \node[box, right = 3cm of assume] at (0, 0) (infer) {
          \parbox{\fw}{\centering Inference}
        };

        %% Arrows

        \draw[arrow] (assume.east) -- ($(infer.west) + (0, 0.1)$);
        \draw[arrow] (observe.east) -- ($(infer.west) - (0, 0.1)$);

      \end{tikzpicture}
    }
  \end{figure}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Assumptions for Logical Inference}

  Alice consults the Oracle. The Oracle is \emph{omniscient} and \emph{never lies}.

  \vb

  \begin{description}
    \item[Alice:] "How's the weather today?"
      \vc
    \item[Oracle:] "It will rain today."
  \end{description}
  
  \vb
  \pause

  \emph{Should Alice bring an umbrella?}

  \vb
  \pause

  \begin{description}
    \item[Assumption:] All of the Oracle's statements are true.\\
      \vc
    \item[Observation:] The Oracle predicts rain.\\
      \vc
    \item[Conclusion:] It will rain. So, Alice should bring an umbrella.
  \end{description}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Assumptions for Logical Inference}


  Bob consults the Elcaro. The Elcaro is \emph{omniscient} but \emph{always lies}.

  \vb

  \begin{description}
    \item[Bob:] "How's the weather today?"
      \vc
    \item[Elcaro:] "It will rain today."
  \end{description}
  
  \vb
  \pause

  \emph{Should Bob bring an umbrella?}

  \vb
  \pause

  \begin{description}
    \item[Assumption:] All of the Elcaro's statements are false.\\
      \vc
    \item[Observation:] The Elcaro predicts rain.\\
      \vc
    \item[Conclusion:] It will not rain. So, Bob doesn't need an umbrella.
  \end{description}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Assumptions for Simplification \& Focus}

  Simplifying assumptions serve to focus our analysis by ignoring unnecessary detail.
  \vc
  \begin{itemize}
    \item Point masses when computing mechanical trajectories
      \vc
    \item Ideal gasses when computing thermodynamic properties
      \vc
    \item Efficient markets for economic projections
      \vc
    \item Full compliance when testing pharmaceutical interventions
      \vc
    \item Infinite sample sizes for asymptotic derivations
      \vc
    \item Linear associations when modeling complex processes
      \vc
    \item Accurate responses in survey research
      \vc
    \item Representative samples in inferential modeling
  \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Assumptions vs. Estimates}

  All statistical models must balance assumptions and empirical observations/estimates:
  \emph{Model = Assumptions + Estimates}
  \begin{align*}
    M &= \gamma_1 A + \gamma_2 E\\[3pt]
    1 &= \gamma A + (1 - \gamma) E
  \end{align*}
  Conceptually speaking, we must choose an appropriate value for $\gamma$.
  \vc
  \begin{itemize}
    \item $\gamma = 1$: I know everything, Infinite researcher hubris
      \vc
    \item $\gamma = 0$: I know nothing, Infinite model complexity
      \vc
    \item $0 \ll \gamma \ll 1$: Useful models
  \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\sectionslide{Assumptions of MLR}

%----------------------------------------------------------------------------------------------------------------------%
\begin{frame}[allowframebreaks]{Assumptions of MLR}

  The assumptions of the linear model can be stated as follows:
  \vb
  \begin{enumerate}
  \item The model is linear in the parameters.
    \vc
    \begin{itemize}
    \item This is OK: $Y = \beta_0 + \beta_1X + \beta_2Z + \beta_3XZ + \beta_4X^2 + \beta_5X^3 + \varepsilon$
      \vc
    \item This is not: $Y = \beta_0 X^{\beta_1} + \varepsilon$
    \end{itemize}
    \vb
  \item The predictor matrix is \emph{full rank}.
    \vc
    \begin{itemize}
    \item $N > P$
      \vc
    \item No $X_p$ can be a linear combination of other predictors.
    \end{itemize}

    \pagebreak

  \item The predictors are strictly exogenous.\label{exo}
    \vc
    \begin{itemize}
    \item The predictors do not correlated with the errors.
      \vc
    \item $\textrm{Cov}(\hat{Y}, \varepsilon) = 0$
      \vc
    \item $\textrm{E}[\varepsilon_n] = 0$
    \end{itemize}
    \vb
  \item The errors have constant, finite variance.\label{constVar}
    \vc
    \begin{itemize}
    \item $\textrm{Var}(\varepsilon_n) = \sigma^2 < \infty$
    \end{itemize}
    \vb
  \item The errors are uncorrelated.\label{indErr}
    \vc
    \begin{itemize}
    \item $\textrm{Cov}(\varepsilon_i, \varepsilon_j) = 0, ~ i \neq j$
    \end{itemize}
    \vb
  \item The errors are normally distributed.\label{normErr}
    \vc
    \begin{itemize}
    \item $\varepsilon \sim \textrm{N}(0, \sigma^2)$
    \end{itemize}
  \end{enumerate}

  \pagebreak

  The assumption of \emph{spherical errors} combines Assumptions \ref{constVar} and \ref{indErr}.
  \begin{align*}
    \textrm{Var}(\varepsilon) =
    \begin{bmatrix}
      \sigma^2 & 0 & \cdots & 0\\
      0 & \sigma^2 & \cdots & 0\\
      0 & 0 & \ddots & 0\\
      0 & 0 & \cdots & \sigma^2
    \end{bmatrix} =
    \sigma^2\mathbf{I}_N
  \end{align*}
  We can combine Assumptions \ref{exo}, \ref{constVar}, \ref{indErr}, and \ref{normErr} by assuming independent and
  identically distributed normal errors:
  \begin{itemize}
  \item $\varepsilon \overset{iid}{\sim} \textrm{N}(0, \sigma^2)$
  \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Consequences of Violating Assumptions}

  \begin{enumerate}
  \item<1-> If the model is not linear in the parameters, then we're not even working with linear regression.
    \begin{itemize}
    \item We need to move to entirely different modeling paradigm.
    \end{itemize}
    \vb
  \item<2-> If the predictor matrix is not full rank, the model is not estimable.
    \begin{itemize}
    \item The parameter estimates cannot be uniquely determined from the data.
    \end{itemize}
    \vb
  \item<3-> If the predictors are not exogenous, the estimated regression coefficients will be biased.
    \vb
  \item<3-> If the errors are not spherical, the standard errors will be biased.
    \begin{itemize}
    \item The estimated regression coefficients will be unbiased, though.
    \end{itemize}
    \vb
  \item<3-> If errors are non-normal, small-sample inferences may be biased.
    \begin{itemize}
    \item The justification for some tests and procedures used in regression analysis may not hold.
    \end{itemize}
  \end{enumerate}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\sectionslide{Regression Diagnostics}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Regression Diagnostics}

  If some of the assumptions are (grossly) violated, the inferences we make using the model may be wrong.
  \begin{itemize}
  \item We need to check the tenability of our assumptions before leaning too heavily on the model estimates.
  \end{itemize}
  \vb
  These checks are called \emph{regression diagnostics}.
  \begin{itemize}
  \item Graphical visualizations
    \vc
  \item Quantitative indices/measures
    \vc
  \item Formal statistical tests
  \end{itemize}

\end{frame}

\watermarkoff %--------------------------------------------------------------------------------------------------------%

\begin{frame}{Residual Plots}

  One of the most useful diagnostic graphics is the plot of residuals vs. predicted values.

  \vb

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
data(anscombe)
out1 <- lm(y1 ~ x1, data = anscombe)

p0 <- with(anscombe, gg0(x = x1, y = y1))
p0 + geom_smooth(method = "lm", se = FALSE) +
    xlab("X") +
    ylab("Y")
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
p0 <- gg0(x = predict(out1), y = resid(out1))
p0 + geom_hline(yintercept = 0, colour = "gray") +
    xlab("Predicted Y") +
    ylab("Residual Y")
@

    \end{column}
  \end{columns}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}[fragile]{Residual Plots}

  We can easily generate a simple plot of residuals vs. fitted values by plotting the fitted lm object in R.

  \begin{columns}
    \begin{column}{0.5\textwidth}

      <<eval = FALSE>>=
      out1 <- lm(Price ~ Horsepower,
                 data = Cars93)

      plot(out1, 1)
      @

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE>>=
out1 <- lm(Price ~ Horsepower, data = Cars93)
plot(out1, 1)
@

    \end{column}
  \end{columns}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\subsection{Heteroscedasticity}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Heteroscedasticity}

  Non-constant error variance (\emph{heteroscedasticity}) violates Assumption \ref{constVar}.

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<>>=
plot(out1, 1)
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<>>=
plot(out1, 3)
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %---------------------------------------------------------------------------------------------------------%

\begin{frame}{Consequences of Heteroscedasticity}

  Non-constant error variance will not bias the parameter estimates.
  \begin{itemize}
  \item The best fit line is still correct.
  \item Our measure of uncertainty around that best fit line is wrong.
  \end{itemize}
  \vb
  Heteroscedasticity will bias standard errors (usually downward).
  \begin{itemize}
  \item Test statistics will be too large.
  \item CIs will be too narrow.
  \item We will have inflated Type I error rates.
  \end{itemize}
  \vb
  To get valid inference, we need to address (severe) heteroscedasticity.

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Treating Heteroscedasticity}

  \begin{enumerate}
  \item Transform your outcome using a concave function (e.g., $\ln(Y)$,
    $\sqrt{Y}$).
    \vc
    \begin{itemize}
    \item These transformations will shrink extreme values more than small/moderate ones.
      \vc
    \item It's usually a good idea to first shift the variable's scale by setting the minimum value to 1.
    \end{itemize}
    \vb
  \item Refit the model using \emph{weighted least squares}.
    \vc
    \begin{itemize}
    \item Create inverse weights using functions of the residual variances or quantities highly correlated therewith.
    \end{itemize}
    \vb
  \item Use a \emph{Heteroscedasticity Consistent} (HC) estimate of the asymptotic covariance matrix.
    \vc
    \begin{itemize}
    \item Robust SEs, Huber-White SEs, Sandwich estimators
    \item HC estimators correct the standard errors for non-constant error variance.
    \end{itemize}
  \end{enumerate}

\end{frame}

\watermarkoff %--------------------------------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

<<>>=
## The 'sandwich' package provides several HC estimators:
library(sandwich)

## the 'lmtest' package provides fancy testing tools for linear models:
library(lmtest)

## Use sandwich estimator to compute ACOV matrix:
hcCov <- vcovHC(out1)

## Test coefficients with robust SEs:
robTest <- coeftest(out1, vcov = hcCov)

## Test coefficients with default SEs:
defTest <- summary(out1)$coefficients
@

\pagebreak

<<>>=
## Compare robust and default approaches:
robTest
defTest
@

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\subsection{Correlated Errors}

\watermarkon %---------------------------------------------------------------------------------------------------------%

\begin{frame}{Correlated Errors}

  Errors can become correlated in two basic ways:
  \vb
  \begin{enumerate}
  \item Serial dependence
    \begin{itemize}
    \item When modeling longitudinal data, the errors for a given observational unit are correlated over time.
      \vc
    \item We can detect temporal dependence by examining the \emph{autocorrelation} of the residuals.
    \end{itemize}
    \vb
  \item Clustering
    \begin{itemize}
    \item Your data have some important, un-modeled, grouping structure.
      \begin{itemize}
      \item Children nested within classrooms
      \item Romantic couples
      \item Departments within a company
      \end{itemize}
      \vc
    \item We can detect problematic levels of clustering with the \emph{intraclass correlation coefficient} (ICC).
      \begin{itemize}
      \item We need to know the clustering variable to apply the ICC.
      \end{itemize}
    \end{itemize}
  \end{enumerate}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Treating Correlated Errors}

  Serially dependent errors in a longitudinal model usually indicate an inadequate model.
  \vc
  \begin{itemize}
  \item Your model is ignoring some important aspect of the temporal variation that is being absorbed by the error
    terms.
    \vc
  \item Hopefully, you can add the missing component to your model.
  \end{itemize}

  \pagebreak

  Clustering can be viewed as theoretically meaningful or as a nuisance factor that just needs to be controlled.
  \vb
  \begin{itemize}
  \item If the clustering is meaningful, you should model the data using \emph{multilevel modeling}.
    \vc
    \begin{itemize}
    \item Hierarchical linear regression
      \vc
    \item Mixed models
      \vc
    \item Random effects models
    \end{itemize}
    \vc
  \item If the clustering is an uninteresting nuisance, you can use specialized HC variance estimators that deal with
    clustering.
  \end{itemize}

\end{frame}

\watermarkoff %--------------------------------------------------------------------------------------------------------%

\begin{frame}[fragile, allowframebreaks]{Example}

<<warning = FALSE>>=
## Read in some data:
LeeBryk <- readRDS(here::here("data", "lee_bryk.rds"))

## Check the data:
str(LeeBryk, vec.len = 3)

## Estimate a linear regression model:
fit <- lm(math ~ ses + sector, data = LeeBryk)

## Calculate the residual ICC:
ICC::ICCbare(x = LeeBryk$schoolid, y = resid(fit))
@

\pagebreak

<<>>=
## Robust tests:
coeftest(fit, vcov = vcovCL(fit, ~ schoolid))

## Raw tests:
summary(fit)$coefficients
@

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\subsection{Linearity}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Linearity}

 Each modeled $X$ must exhibit a linear relation with $Y$.
  \begin{itemize}
    \item We can define $X$ via nonlinear transformations of the original data.
  \end{itemize}
  
  \vb

  \begin{columns}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
out2 <- lm(MPG.highway ~ Horsepower, data = Cars93)

p2 <- gg0(x = predict(out2), y = resid(out2))
p3 <- p2 + geom_hline(yintercept = 0, colour = "gray") +
    xlab("Predicted MPG") +
    ylab("Residual MPG")
p4 <- gg0(x = Cars93$Horsepower, y = Cars93$MPG.highway)
p5 <- p4 + geom_smooth(method = "lm", se = FALSE) +
    xlab("Horsepower") +
    ylab("MPG")
p5
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
p3 + geom_smooth(method = "loess", se = FALSE)
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %---------------------------------------------------------------------------------------------------------%

\begin{frame}{Treating Residual Nonlinearity}

  Nonlinearity in the residual plots is usually a sign of either:
  \begin{enumerate}
  \item Model misspecification
  \item Influential observations
  \end{enumerate}
  \vb
  This type of model misspecification usually implies omitted functions of modeled variables.
  \begin{itemize}
  \item Polynomial terms
  \item Interactions
  \end{itemize}
  \vb
  The solution is to include the omitted term into the model and refit.
  \begin{itemize}
  \item This is very much easier said than done.
  \end{itemize}

\end{frame}

\watermarkoff %--------------------------------------------------------------------------------------------------------%

\begin{frame}[fragile]{Limitations of Residual Plots}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      In multiple regression models, basic residual plots won't tell us which predictors exhibit nonlinear associations.

      \vb

<<>>=
out3 <- 
  lm(MPG.highway ~ Horsepower + RPM, 
     data = Cars93)
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
plot(out3, 1)
@

    \end{column}
  \end{columns}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}[fragile]{Component + Residual Plots}

  \begin{columns}
    \begin{column}{0.5\textwidth}

  We can use \emph{Component + Residual Plots} (AKA, partial residual plots) to visualize the unique effects of each $X$
  variable.

<<eval = FALSE, message = FALSE>>=
library(car)
crPlots(out3)
@

    \end{column}
    \begin{column}{0.5\textwidth}

<<echo = FALSE, message = FALSE>>=
car::crPlots(out3, layout = c(2, 1))
@

    \end{column}
  \end{columns}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\subsection{Omitted Variables}

\watermarkon %---------------------------------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Omitted Variables}

  The most common cause of endogeneity (i.e., violating Assumption \ref{exo}) is \emph{omitted variable bias}.
  \vb
  \begin{itemize}
  \item If we leave an important predictor variable out of our equation, some modeled predictors will become endogenous
    and their estimated regression slopes will be biased.
    \vb
  \item The omitted variable must be correlated with $Y$ and at least one of the modeled $X_p$, to be a problem.
  \end{itemize}

  \pagebreak

  Assume the following is the true regression model.
  \begin{align*}
    Y = \beta_0 + \beta_1X + \beta_2Z + \varepsilon
  \end{align*}
  Now, suppose we omit $Z$ from the model:
  \begin{align*}
    Y &= \beta_0 + \beta_1X + \omega\\
    \omega &= \varepsilon + \beta_2Z
  \end{align*}
  Our new error, $\omega$, is a combination of the true error, $\varepsilon$, and the omitted term, $\beta_2Z$.
  \begin{itemize}
  \item Consequently, if $X$ and $Z$ are correlated, omitting $Z$ induces a correlation between $X$ and $\omega$ (i.e.,
    endogeneity).
  \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Treating Omitted Variable Bias}

  Omitted variable bias can have severe consequences, but you can't really test for it.
  \vb
  \begin{itemize}
  \item The \emph{errors} are correlated with the predictors, but our model is estimated under the assumption of
    exogeneity, so the \emph{residuals} from our model will generally be uncorrelated with the predictors.
    \vb
  \item We mostly have to pro-actively work to include all relevant variables in our model.
  \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\subsection{Normality}

\watermarkoff %--------------------------------------------------------------------------------------------------------%

\begin{frame}{Normality Assumption}

  \begin{columns}
    \begin{column}{0.5\textwidth}

      One of the best ways to evaluate the normality of the error distribution with a Q-Q Plot.
      \vc
      \begin{itemize}
      \item Plot the quantiles of the residual distribution against the theoretically ideal quantiles.
        \vc
      \item We can actually use a Q-Q Plot to compare any two distributions.
      \end{itemize}

    \end{column}
    \begin{column}{0.5\textwidth}

<<>>=
plot(out1, 2)
@

<<echo = FALSE, eval = FALSE>>=
out1 <- lm(Price ~ Horsepower, data = Cars93)
qqnorm(resid(out1))
qqline(resid(out1))
@

    \end{column}
  \end{columns}

\end{frame}

\watermarkon %---------------------------------------------------------------------------------------------------------%

\begin{frame}[allowframebreaks]{Consequences of Violating Normality}

  In small samples, with \emph{\underline{fixed}} predictors, normally distributed errors imply normal sampling
  distributions for the regression coefficients.
  \vc
  \begin{itemize}
  \item In large samples, the central limit theorem implies normal sampling distributions for the coefficients,
    regardless of the error distribution.
  \end{itemize}
  \va
  If we view our regression equation as a statistical model, then non-normal errors imply a misspecified model.
  \vc
  \begin{itemize}
    \item From a modeling perspective, violating the normality assumption tells us that we've gone wrong somewhere in
      our model definition.
  \end{itemize}
  
  \pagebreak

  Prediction intervals require normally distributed errors.
  \vc
  \begin{itemize}
  \item Confidence intervals for predictions share the same normality requirements as the coefficients' sampling
    distributions.
  \end{itemize}
  \va
  Parameter estimates will not be fully efficient.
  \vc
  \begin{itemize}
    \item Standard errors will be larger than they would have been with normally distributed errors.
  \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\begin{frame}{Treating Violations of Normality}

  We usually don't need to do anything about non-normal errors.
  \begin{itemize}
  \item The CLT will protect our inferences.
  \end{itemize}
  \vb
  \pause
  We can use \emph{bootstrapping} to get around the need for normality.
  \begin{enumerate}
  \item Treat your sample as a synthetic population from which you draw many new samples (with replacement).
  \item Estimate your model in each new sample.
  \item The replicates of your estimated parameters generate an empirical sampling distribution that you can use for
    inference.
  \end{enumerate}
  \vb
  \pause
  Bootstrapping can be used for inference on pretty much any estimable parameter, but it won't work with small samples.
  \begin{itemize}
  \item Need to assume that your sample is representative of the population
  \end{itemize}

\end{frame}

%----------------------------------------------------------------------------------------------------------------------%

\end{document}
