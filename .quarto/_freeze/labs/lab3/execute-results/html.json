{
  "hash": "bda89c807c90f5b453f8321c506bcddd",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 3\"\n#output-file: \"lab3_answers.html\"\noutput-file: \"lab3.html\"\nparams:\n  answers: false\n---\n\n# Part 1: Repeat from last week\n\nWe will use the following packages in this practical:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(gridExtra)\n```\n:::\n\n\nWe will use the same data that we used last week to perform a simple linear regression, the Iris dataset. But now, we will extend on this simple model with multiple variables. \n\nIn order to do this, we first need to load the data again and run the simple model where Sepal length is predicted by Petal width. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- iris # load the data\nhead(iris)   # inspect the data\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n\n\n:::\n:::\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# specify model\nmodel1 <- lm(Sepal.Length ~ Petal.Width, \n             data = data)\n\n# ask for summary\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Width, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.38822 -0.29358 -0.04393  0.26429  1.34521 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.77763    0.07293   65.51   <2e-16 ***\nPetal.Width  0.88858    0.05137   17.30   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.478 on 148 degrees of freedom\nMultiple R-squared:  0.669,\tAdjusted R-squared:  0.6668 \nF-statistic: 299.2 on 1 and 148 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n:::\n\n\n# Part 2: Multiple linear regression\n\nYou can add additional predictors to a model. This can improve the fit and the predictions. When multiple predictors are used in a regression model, it's called a Multiple linear regression. You specify this model as `outcome_variable ~ predictor_1 + predictor_2 + ... + predictor_n`.\n\n1. __Add Petal length as a second predictor to the model specified as `model1` and store this under the name `model2`, and supply summary statistics. Again, give a substantive interpretation of the coefficients and the model.__\n\n\n\n## Categorical predictors\n\nUp to here, we only included continuous predictors in our models. We will now include a categorical predictor in the model as well. \n\nWhen a categorical predictor is added, this predictor is split in several contrasts (or dummies), where each group is compared to a reference group. In our example Iris data, the variable 'Species' is a categorical variable that indicate the species of flower. This variable can be added as example for a categorical predictor. Contrasts, and thus the dummy coding, can be inspected through `contrasts()`.\n\n\n2. __Add species as a predictor to the model specified as `model2`, store it under the name `model3` and interpret the categorical coefficients of this new model.__\n\n\n\n\n# Part 3: Model comparison\n\nNow you have created multiple models, you can compare how well these models function (compare the model fit). There are multiple ways of testing the model fit and to compare models, as explained in the lecture and the reading material. In this practical, we use the following:\n\n* AIC (use the function `AIC()` on the model object)\n* BIC (use the function `BIC()` on the model object)\n* MSE (use `MSE()` of the `MLmetrics` package, or calculate by transforming the `model$residuals`)\n* Deviance test (use `anova()` to compare 2 models)\n\n3. __Compare the fit of the model specified under question 5 and the model specified under question 8. Use all four fit comparison methods listed above. Interpret the fit statistics you obtain/tests you use to compare the fit.__\n\n\n\n\n# Part 4: Residuals: observed vs. predicted\n\nWhen fitting a regression line, the predicted values have some error in comparison to the observed values. The sum of the squared values of these errors is the sum of squares. A regression analysis finds the line such that the lowest sum of squares possible is obtained.\n\nThe image below shows how the predicted (on the blue regression line) and observed values (black dots) differ and how the predicted values have some error (red vertical lines).\n\n![](errorterms.PNG)\n\nWhen having multiple predictors, it becomes harder or impossible to make such a plot as above (you need a plot with more dimensions). You can, however, still plot the observed values against the predicted values and infer the error terms from there.\n\n4. __Create a dataset of predicted values for model 1 by taking the outcome variable `Sepal.Length` and the `fitted.values` from the model.__\n\n\n5. __Create an observed vs. predicted plot for model 1 (the red vertial lines are no must).__\n\n\n6. __Create a dataset of predicted values and create a plot for model 2.__\n\n\n7. __Compare the two plots and discuss the fit of the models based on what you see in the plots. You can combine them in one figure using the `grid.arrange()` function.__\n\n\n\n## Calculating new predicted values\n\nA regression model can be used to predict values for new cases that were not used to built the model.\nThe regression equation always consists of coefficients ($\\beta$s) and observed variables ($X$):\n\n<br>\n\n$$\\hat{y} = \\beta_0 + \\beta_1 * X_{a}* + \\beta_2 * X_b +  \\ldots  + \\beta_n * X_n$$",
    "supporting": [
      "lab3_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}