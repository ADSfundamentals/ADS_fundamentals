[
  {
    "objectID": "workgroups/WG1.html#today",
    "href": "workgroups/WG1.html#today",
    "title": "Work Group Meeting 1",
    "section": "Today",
    "text": "Today\n\nIntroduction\nCourse structure\nWork group structure\nAssignments\n\nbreak\n\nStart first assignment"
  },
  {
    "objectID": "workgroups/WG1.html#introduction",
    "href": "workgroups/WG1.html#introduction",
    "title": "Work Group Meeting 1",
    "section": "Introduction",
    "text": "Introduction\n\nIntroduce yourself\nWhat is your background (study)?\nWhy did you choose this minor/course?"
  },
  {
    "objectID": "workgroups/WG1.html#course-structure",
    "href": "workgroups/WG1.html#course-structure",
    "title": "Work Group Meeting 1",
    "section": "Course structure",
    "text": "Course structure\n\nMonday 15:15: Lecture by Dr. K. Lang on location\nTuesday 13:15: Q&A by Dr. K. Lang online\nThursday 09:00 & 11:00: Workgroup meetings on location\n\nBy yourself:\n\nDo the required reading\nWork on practicals (deadline is next Monday 15:00, before the next lecture)\nContinue work on group assignment"
  },
  {
    "objectID": "workgroups/WG1.html#workgroup-meetings",
    "href": "workgroups/WG1.html#workgroup-meetings",
    "title": "Work Group Meeting 1",
    "section": "Workgroup meetings",
    "text": "Workgroup meetings\n\nApply the lecture topics to your own (real!) datasets\nMeet your group to work together"
  },
  {
    "objectID": "workgroups/WG1.html#assignments",
    "href": "workgroups/WG1.html#assignments",
    "title": "Work Group Meeting 1",
    "section": "Assignments",
    "text": "Assignments\n\n\nAssignment 1\n\nGroup assignment\nGroups with max. 4 members\nLinear regression\nHand in written report on Monday Dec. 8, 15:00\n\n\nAssignment 2\n\nGroup assignment\nSame group as assignment 1\nLogistic regression\nPresentation on Thursday Jan. 15 at workgroup meeting"
  },
  {
    "objectID": "workgroups/WG1.html#assignments-1",
    "href": "workgroups/WG1.html#assignments-1",
    "title": "Work Group Meeting 1",
    "section": "Assignments",
    "text": "Assignments\nGrading:\n\nBoth group assignments contribute 25% to your final grade\nAll group members receive the same grade\nIf names are omitted from groups/assignments, we assume everyone is aware of this."
  },
  {
    "objectID": "workgroups/WG1.html#assignments-2",
    "href": "workgroups/WG1.html#assignments-2",
    "title": "Work Group Meeting 1",
    "section": "Assignments",
    "text": "Assignments\nHand in:\n\nThrough Filedrop (link is on website)\nHand in a .zip file with R project and all files (html, qmd, dataset)\nMake sure all code and text are visible in the html files\nOnly content in html files is graded!!!!\nCheck the course website for more information"
  },
  {
    "objectID": "workgroups/WG1.html#this-week",
    "href": "workgroups/WG1.html#this-week",
    "title": "Work Group Meeting 1",
    "section": "This week:",
    "text": "This week:\n\nMake groups of max. 4 students\nLook for a dataset\nInspect variables\nCome up with research questions for linear and logistic regression\nPre-processing of the dataset where needed"
  },
  {
    "objectID": "workgroups/WG1.html#dataset-requirements",
    "href": "workgroups/WG1.html#dataset-requirements",
    "title": "Work Group Meeting 1",
    "section": "Dataset requirements",
    "text": "Dataset requirements\nThe dataset should at least contain:\n\nA continuous outcome variable\nAt least two continuous predictor variables\nA categorical predictor variable\nA dichotomous outcome variable"
  },
  {
    "objectID": "workgroups/WG1.html#tips-for-datasets",
    "href": "workgroups/WG1.html#tips-for-datasets",
    "title": "Work Group Meeting 1",
    "section": "Tips for datasets",
    "text": "Tips for datasets\nLook for datasets online:\n\nKaggle\nGitHub\nCBS/Eurostat\nRIVM\nThrough scientific papers\nLarge panel studies (PISA, LISS, EVS, ESS, TIMSS)\nDANS KNAW or ODISSEI portal"
  },
  {
    "objectID": "workgroups/WG1.html#tips-for-datasets-1",
    "href": "workgroups/WG1.html#tips-for-datasets-1",
    "title": "Work Group Meeting 1",
    "section": "Tips for datasets",
    "text": "Tips for datasets\nDatasets from courses can be difficult for this purpose, don’t use them!"
  },
  {
    "objectID": "workgroups/WG1.html#example-research-questions",
    "href": "workgroups/WG1.html#example-research-questions",
    "title": "Work Group Meeting 1",
    "section": "Example research questions",
    "text": "Example research questions\nLinear regression:\n\nCan body weight predict the level of cholesterol in Dutch adults?\nWhat variables are related to income in the Netherlands?\n\nLogistic regression:\n\nDo body weight, calorie intake, fate intake and age have an influence on the occurence of a heart attack?\nWhat variables can be used to predict whether students pass a driving test?"
  },
  {
    "objectID": "workgroups/WG1.html#data-processing",
    "href": "workgroups/WG1.html#data-processing",
    "title": "Work Group Meeting 1",
    "section": "Data processing",
    "text": "Data processing\nExamples:\n\nAre the character classes set correctly? Adjust with functions such as as_factor() or as_numeric()\nAre missing values correctly recognized?\nDo you need to calculate any sumscores or meanscores?\nDo you need to link multiple separate datasets?"
  },
  {
    "objectID": "workgroups/WG1.html#use-of-generative-ai",
    "href": "workgroups/WG1.html#use-of-generative-ai",
    "title": "Work Group Meeting 1",
    "section": "Use of Generative AI",
    "text": "Use of Generative AI\nYou are allowed to:\n\nGain inspiration\nAsk for help how to tackle an assignment\n\nYou are not allowed to:\n\nSubmit generated text or code as if it was written by you\nHave text or code revised"
  },
  {
    "objectID": "workgroups/WG1.html#before-the-end-of-the-meeting",
    "href": "workgroups/WG1.html#before-the-end-of-the-meeting",
    "title": "Work Group Meeting 1",
    "section": "Before the end of the meeting",
    "text": "Before the end of the meeting\nAs a group, send an email to your WG instructor with:\n\nNames of group members\nHow you agree to collaborate\nSelected data set (topic and link)\nResearch questions you want to answer in your assignments\n\nSend this email before the end of the meeting!"
  },
  {
    "objectID": "manual.html",
    "href": "manual.html",
    "title": "Course manual",
    "section": "",
    "text": "Regression techniques are widely used to quantify the relationship between two or more variables. In data science, linear and logistic regression are common and powerful techniques for evaluating such relations. These techniques are only useful, however, once you understand when and how to apply them. In this course, students will learn how to apply linear and logistic regression with the R statistical software package.\nThis course will introduce students to the principles of analytical data science, linear and logistic regression, and the basics of statistical learning. Students will develop fundamental R programming skills and will gain experience with tidyverse: visualize data with ggplot2 and performing basic data wrangling with dplyr. This course helps prepare students for an entry-level research career (e.g. junior researcher or research assistant) or further education in research (e.g., a [research] Master program or a PhD)."
  },
  {
    "objectID": "manual.html#course-content",
    "href": "manual.html#course-content",
    "title": "Course manual",
    "section": "",
    "text": "Regression techniques are widely used to quantify the relationship between two or more variables. In data science, linear and logistic regression are common and powerful techniques for evaluating such relations. These techniques are only useful, however, once you understand when and how to apply them. In this course, students will learn how to apply linear and logistic regression with the R statistical software package.\nThis course will introduce students to the principles of analytical data science, linear and logistic regression, and the basics of statistical learning. Students will develop fundamental R programming skills and will gain experience with tidyverse: visualize data with ggplot2 and performing basic data wrangling with dplyr. This course helps prepare students for an entry-level research career (e.g. junior researcher or research assistant) or further education in research (e.g., a [research] Master program or a PhD)."
  },
  {
    "objectID": "manual.html#course-structure",
    "href": "manual.html#course-structure",
    "title": "Course manual",
    "section": "Course structure",
    "text": "Course structure\nIn seven weeks, you will learn the basics of data handling and statistical programming with R and details about regression techniques in the context of statistical inference, prediction, and classification. Each week will comprise four activities:\n\nDuring the weekly in-person lectures, we will cover the theoretical content.\nDuring the weekly in-person workgroup meetings, you will work on real-world data analysis with a group of your peers.\nWeekly self-study practical exercises connect the statistical theory to practice by applying the lecture content in the R statistical programming language.\nThere is an online weekly Q&A session in case you get stuck or have questions."
  },
  {
    "objectID": "manual.html#how-to-pass-the-course",
    "href": "manual.html#how-to-pass-the-course",
    "title": "Course manual",
    "section": "How to pass the course",
    "text": "How to pass the course\nYour final grade consists of three elements:\n\n\n\nGrade component\nWeight\nRequired minimum grade\n\n\n\n\nGroup assignment 1\n25%\n5.5\n\n\nGroup assignment 2\n25%\n5.5\n\n\nWritten exam in Remindo\n50%\n5.5\n\n\n\n\nAttendance\nDuring this course, you will attend 7 lectures, 7 workgroup sessions and work on 6 practical assignments.\nAttendance for the lectures and the workgroup sessions is mandatory. You work on your group assignments during the workgroup meetings. For a collaborative spirit, attendance is crucial."
  },
  {
    "objectID": "manual.html#course-objectives-and-learning-outcomes",
    "href": "manual.html#course-objectives-and-learning-outcomes",
    "title": "Course manual",
    "section": "Course objectives and learning outcomes",
    "text": "Course objectives and learning outcomes\n\nCourse objectives\nAt the end of this course, students are able to:\n\nIdentify key statistical concepts such as:\n\n\n(Conditional) probability\nInference\nEstimation\nPrediction\nClassification\nSampling variability\nStatistical modeling\nResiduals Fitted values\n\n\nChoose an appropriate regression model for a given research scenario.\nExplain the differences/similarities between statistical inference and model-based prediction/classification; give examples of each type of problem.\nIdentify the assumptions of linear and logistic regression; describe the consequences of violating these assumptions.\nDescribe the three components of a generalized linear model and how these components are specified in logistic regression.\nInterpret the estimates from linear and logistic regression models, and use these estimates to answer research questions.\nUse the R statistical software platform to perform basic statistical programming, data manipulation, data visualization, and basic data wrangling.\nUse the R statistical software platform to perform, interpret, and evaluate linear and logistic regression analyses on real-world data.\nInterpret R output and use the results to answer research questions.\nUse Quarto to document the results of a statistical analysis.\n\n\n\nRationale for assessment strategy\nIn this course, skills and knowledge are evaluated with two types of assignment.\n\nThe exam evaluates knowledge and understanding of statistical concepts (Learning goal 1), the ability to critically evaluate research problems and statistical methods (Learning goals 2–5), and the ability to interpret statistical results and software output and apply these interpretations (Learning goals 6 & 9).\nThe group assignments evaluate the student’s ability to work with data, solve basic data analytic problems, execute quantitative data analyses on real-world data sets, and document the results (learning goals 6–10)."
  },
  {
    "objectID": "manual.html#important-notice-about-generative-ai-in-this-course",
    "href": "manual.html#important-notice-about-generative-ai-in-this-course",
    "title": "Course manual",
    "section": "Important notice about Generative AI in this course",
    "text": "Important notice about Generative AI in this course\n\nDefinition\nGenerative AI (GenAI) is a type of artificial intelligence that can create new content, such as images, videos, audio and text. An example is the chatbot ChatGPT. GenAI generates new and unique output based on a prompt from a user by analysing large amounts of existing data.\n\n\nGuidelines\nUtrecht University has these guidelines regarding GenAI.\nYou are always responsible for your own work and are never allowed to submit work developed entirely by GenAI as your own. If this does happen, it is considered fraud. See for more information the relevant parts of the EER. GenAI use includes, but is not limited to, generating text, audio, images, video and code. Regarding code, you may use AI tools to assist you in generating code that results in reproducible data sets. However, you may not generate data sets.\nWe would like to emphasize that taking this course is an investment in your own development. The material has been carefully put together to deepen your understanding of the field and strengthen your skills. By distributing or even selling this material, you violate the intellectual property of the UU. We therefore ask you to respect these guidelines and contribute to a fair and honest learning environment.\n\n\nScenario for this course\nYou may use GenAI to prepare the work you hand in. You can use GenAI to gain inspiration for research questions, or help with debugging your R code.\nYou CANNOT use genAI to generate text or code for the two assignments that you hand in.\nYou CANNOT generate data sets with GenAI.\n\n\n\n\n\n\nWarning\n\n\n\nThere is information in these materials that exceeds legal use of copyright materials in academic settings, or that cannot not be part of the public domain.\nYou CANNOT submit any content in this course as input to generative AI tools. This also includes staff names.\n\n\n\n\nTo which assignments does this scenario apply\nThe two assignments of this course."
  },
  {
    "objectID": "labs/lab6_answers.html",
    "href": "labs/lab6_answers.html",
    "title": "Lab 6",
    "section": "",
    "text": "We will use the following packages in this practical:\n\ndplyr for manipulation\nmagrittr for piping\nreadr for reading data\nggplot for plotting\nkableExtra for tables\nlibrary(pROC), library(regclass), and library(caret) for model diagnostics\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(readr)\nlibrary(pROC)\nlibrary(regclass)\nlibrary(caret)\n\n\n\nIn this practical, you will perform logistic regression analyses again using glm() and discuss model assumptions and diagnostics.\n1. Read in the data from the “titanic.csv” file, which we also used for the previous practical.\n\n# Load in the data\ntitanic &lt;- read_csv(here::here(\"data\", \"titanic.csv\")) %&gt;% \n  mutate(Survived = as.factor(Survived),\n         Sex = as.factor(Sex),\n         Pclass = as.factor(Pclass))\n\nRows: 887 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Sex\ndbl (6): Survived, Pclass, Age, Siblings/Spouses Aboard, Parents/Children Ab...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(titanic)\n\ntibble [887 × 8] (S3: tbl_df/tbl/data.frame)\n $ Survived               : Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass                 : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Name                   : chr [1:887] \"Mr. Owen Harris Braund\" \"Mrs. John Bradley (Florence Briggs Thayer) Cumings\" \"Miss. Laina Heikkinen\" \"Mrs. Jacques Heath (Lily May Peel) Futrelle\" ...\n $ Sex                    : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age                    : num [1:887] 22 38 26 35 35 27 54 2 27 14 ...\n $ Siblings/Spouses Aboard: num [1:887] 1 1 0 1 0 0 0 3 0 1 ...\n $ Parents/Children Aboard: num [1:887] 0 0 0 0 0 0 0 1 2 0 ...\n $ Fare                   : num [1:887] 7.25 71.28 7.92 53.1 8.05 ...\n\n#I use `readr::read_csv` to import the titanic training dataset, piping the input through `mutate_if` functions to correct the class. *\n\n#As a reminder, the variables are:*\n#`Survived` - passenger's survival status where 0 indicates did not survive, 1 indicats survived*\n#`Pclass` - 1st, 2nd, and 3rd class tickets*\n#`Age` - passenger age in years*\n#`Sex` - passenger sex as male or female*\n\n\n\n\n\nFit the following two logistic regression models and save them as fit1 and fit2:\n\n\nSurvived ~ Pclass\nSurvived ~ Age + Pclass*Sex\n\n\nfit1 &lt;- glm(Survived ~ Pclass, \n            family = binomial, \n            data = titanic)\n\nfit2 &lt;- glm(Survived ~ Age + Sex*Pclass, \n            family = binomial, \n            data = titanic)"
  },
  {
    "objectID": "labs/lab6_answers.html#loading-the-data",
    "href": "labs/lab6_answers.html#loading-the-data",
    "title": "Lab 6",
    "section": "",
    "text": "In this practical, you will perform logistic regression analyses again using glm() and discuss model assumptions and diagnostics.\n1. Read in the data from the “titanic.csv” file, which we also used for the previous practical.\n\n# Load in the data\ntitanic &lt;- read_csv(here::here(\"data\", \"titanic.csv\")) %&gt;% \n  mutate(Survived = as.factor(Survived),\n         Sex = as.factor(Sex),\n         Pclass = as.factor(Pclass))\n\nRows: 887 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Sex\ndbl (6): Survived, Pclass, Age, Siblings/Spouses Aboard, Parents/Children Ab...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nstr(titanic)\n\ntibble [887 × 8] (S3: tbl_df/tbl/data.frame)\n $ Survived               : Factor w/ 2 levels \"0\",\"1\": 1 2 2 2 1 1 1 1 2 2 ...\n $ Pclass                 : Factor w/ 3 levels \"1\",\"2\",\"3\": 3 1 3 1 3 3 1 3 3 2 ...\n $ Name                   : chr [1:887] \"Mr. Owen Harris Braund\" \"Mrs. John Bradley (Florence Briggs Thayer) Cumings\" \"Miss. Laina Heikkinen\" \"Mrs. Jacques Heath (Lily May Peel) Futrelle\" ...\n $ Sex                    : Factor w/ 2 levels \"female\",\"male\": 2 1 1 1 2 2 2 2 1 1 ...\n $ Age                    : num [1:887] 22 38 26 35 35 27 54 2 27 14 ...\n $ Siblings/Spouses Aboard: num [1:887] 1 1 0 1 0 0 0 3 0 1 ...\n $ Parents/Children Aboard: num [1:887] 0 0 0 0 0 0 0 1 2 0 ...\n $ Fare                   : num [1:887] 7.25 71.28 7.92 53.1 8.05 ...\n\n#I use `readr::read_csv` to import the titanic training dataset, piping the input through `mutate_if` functions to correct the class. *\n\n#As a reminder, the variables are:*\n#`Survived` - passenger's survival status where 0 indicates did not survive, 1 indicats survived*\n#`Pclass` - 1st, 2nd, and 3rd class tickets*\n#`Age` - passenger age in years*\n#`Sex` - passenger sex as male or female*"
  },
  {
    "objectID": "labs/lab6_answers.html#logistic-regression",
    "href": "labs/lab6_answers.html#logistic-regression",
    "title": "Lab 6",
    "section": "",
    "text": "Fit the following two logistic regression models and save them as fit1 and fit2:\n\n\nSurvived ~ Pclass\nSurvived ~ Age + Pclass*Sex\n\n\nfit1 &lt;- glm(Survived ~ Pclass, \n            family = binomial, \n            data = titanic)\n\nfit2 &lt;- glm(Survived ~ Age + Sex*Pclass, \n            family = binomial, \n            data = titanic)"
  },
  {
    "objectID": "labs/lab6_answers.html#binary-dependent-variable",
    "href": "labs/lab6_answers.html#binary-dependent-variable",
    "title": "Lab 6",
    "section": "Binary dependent variable",
    "text": "Binary dependent variable\nThe first outcome in a logistic regression is that the outcome should be binary and therefore follow a binomial distribution. This is easy to check: you just need to be sure that the outcome can only take one of two responses. You can plot the responses of the outcome variable to visually check this if you want. In our case, the possible outcomes are:\n\nSurvived (coded 1)\nDid not survive (coded 0)\n\n\nVisualise the responses of the outcome variable Survived using ggplot().\n\n\ntitanic %&gt;% \n  ggplot(aes(x = Survived, fill = Survived)) +\n  geom_bar() +\n  labs(x = \"Survival\",\n       y = \"Count\",\n       title = \"Distribution of the outcome variable\") +\n  theme_bw()\n\n\n\n\n\n\n\n# You can see that there are indeed only two outcomes for `Survived`, so our outcome follows a binomial distribution."
  },
  {
    "objectID": "labs/lab6_answers.html#balanced-outcomes",
    "href": "labs/lab6_answers.html#balanced-outcomes",
    "title": "Lab 6",
    "section": "Balanced outcomes",
    "text": "Balanced outcomes\nIf you are using logistic regression to make predictions/classifications then the accuracy will be affected by imbalance in the outcome classes. Notice that in the plot you just made there are more people who did not survive than who did survive. A possible consequence is reduced accuracy in classification of survivors.\nA certain amount of imbalance is expected and can be handled well by the model in most cases. The effects of this imbalance is context-dependent. Some solutions to serious class imbalance are down-sampling or weighting the outcomes to balance the importance placed on the outcomes by the model."
  },
  {
    "objectID": "labs/lab6_answers.html#sufficiently-large-sample-size",
    "href": "labs/lab6_answers.html#sufficiently-large-sample-size",
    "title": "Lab 6",
    "section": "Sufficiently large sample size",
    "text": "Sufficiently large sample size\nSample size in logistic regression is a complex issue, but some suggest that it is ideal to have 10 cases per candidate predictor in your model. The minimum number of cases to include is \\(N = \\frac{10*k} {p}\\), where \\(k\\) is the number of predictors and \\(p\\) is the smallest proportion of negative or positive cases in the population.\n\nCalculate the minimum number of positive cases needed in the model fit1.\n\n\n#First we need to get the proportion of people that survived in our sample, which is 0.38*\n\ntitanic %&gt;% \n  count(Survived) %&gt;% \n  mutate(prop = n / sum(n))\n\n# A tibble: 2 × 3\n  Survived     n  prop\n  &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;\n1 0          545 0.614\n2 1          342 0.386\n\n#Now we can plug this into our formula to get the minimum number of positive cases. \nssize_cal &lt;- function(k, p){\n  round((10*k)/p)\n}\n\nssize_cal(2, 0.38)\n\n[1] 53\n\n#We have many more than this in our data so the sample size is large enough.*"
  },
  {
    "objectID": "labs/lab6_answers.html#predictor-matrix-is-full-rank",
    "href": "labs/lab6_answers.html#predictor-matrix-is-full-rank",
    "title": "Lab 6",
    "section": "Predictor matrix is full-rank",
    "text": "Predictor matrix is full-rank\nYou learned about this assumption in the linear regression practicals, but to remind you:\n\nthere need to be more observations than predictors (n &gt; P)\nthere should be no multicollinearity among the linear predictors\n\n\nCheck that there is no multicollinearity in the logistic model.\n\n\n#VIF(fit1)\nVIF(fit2)\n\n               GVIF Df GVIF^(1/(2*Df))\nAge         1.38354  1        1.176240\nSex        10.70246  1        3.271462\nPclass     29.58719  2        2.332255\nSex:Pclass 41.98377  2        2.545484\n\n# Like with linear regression we can use the VIF. A VIV &gt; 10 indicates high multicollinearity. Remember that for continuous variables we should inspect the \"GVIF\" column. For categorical predictors we should check the \"GVIF^(1/(2*Df))\" column.\n\n# In model 1, the VIF cannot be determined since there is only a single predictor."
  },
  {
    "objectID": "labs/lab6_answers.html#continuous-predictors-are-linearly-related-to-the-logitpi",
    "href": "labs/lab6_answers.html#continuous-predictors-are-linearly-related-to-the-logitpi",
    "title": "Lab 6",
    "section": "Continuous predictors are linearly related to the \\(logit(\\pi)\\)",
    "text": "Continuous predictors are linearly related to the \\(logit(\\pi)\\)\nLogistic regression models assume a linear relationship between predictor variables and the logit of the outcome variable. This assumption is mainly concerned with continuous predictors. Since we only have one continuous predictor (Age) we can plot the relationship between Age and the logit of Survived.\n\nGet the predicted values of fit2 on the logit scale and bind them to the titanic data.\n\n\n# We do this using the `predict()` function, but specifying `type = \"link\"` this time. \n\ntitanic$logit &lt;- predict(fit2, type = \"link\")\n\n\nPlot the relationship between Age and logit and interpret it.\n\n\ntitanic %&gt;% \nggplot(aes(Age, logit))+\n  geom_point(size = 0.5, alpha = 0.5) +\n  geom_smooth(method = \"glm\") + \n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Age does appear to be negatively linearly related to the predicted logit values.\n\n\nHow should we deal with variables that are not linearly related to the logit?\n\n\n# This differs per situation. One solution can be to try to apply transformations to the variables."
  },
  {
    "objectID": "labs/lab6_answers.html#no-influential-values-or-outliers",
    "href": "labs/lab6_answers.html#no-influential-values-or-outliers",
    "title": "Lab 6",
    "section": "No influential values or outliers",
    "text": "No influential values or outliers\nInfluential values are extreme individual data points that can affect the fit of the logistic regression model. They can be visualised using Cook’s distance and the Residuals vs Leverage plot.\n\nUse the plot() function to visualise the outliers and influential points of fit2.\n\nHint: you need to specify the correct plot with the which argument. Check the lecture slides or search ??plot if you are unsure.\n\n# There are two relevant plots to check for influential values. The first is Cook's distance plot, which shows the top 3 largest values. Note that not all outliers are influential cases, so we should also inspect the Residuals vs Leverage plot, which shows actual influential cases.\n\n# You can get both of these plots by specifying `which = c(4, 5)` in the `plot()` function.\n\nplot(fit2, which = c(4, 5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAre there any influential cases in the Leverage vs Residuals plot? If so, what would you do?\n\n\n# Leverage is the extent that the model coefficients will change if a particular observation is removed from the dataset. Points that fall outside of the red dashed line (Cook's distance) are influential.*\n\n# There are no values which fall outside of the red dashed lines.* \n\n# If there are influential observations, you can remove them, replace them with a more realistic value, or keep them as they are and make note of it when reporting results.*"
  },
  {
    "objectID": "labs/lab6_answers.html#differences-to-linear-regressoin",
    "href": "labs/lab6_answers.html#differences-to-linear-regressoin",
    "title": "Lab 6",
    "section": "Differences to linear regressoin",
    "text": "Differences to linear regressoin\nLastly, it is important to note that the assumptions of a linear regression do not all map to logistic regression. In logistic regression, we do not need:\n\nconstant, finite error variance\nnormally distributed errors\n\nHowever, deviance residuals are useful for determining if the individual points are not fit well by the model.\nHint: you can use some of the code from the lecture for the next few questions.\n\nUse the resid() function to get the deviance residuals for fit2.\n\n\ndr &lt;- resid(fit2, type = \"deviance\")\n\n\nCompute the predicted logit values for the model.\n\n\neta &lt;- predict(fit2, type = \"link\")\n\n\nPlot the deviance residuals.\n\n\n# The first step is to bind the deviance residuals to the logit values.\n\ndr_data &lt;- data.frame(residuals = dr, \n                      eta = rep(eta, 3))\n\nWarning in data.frame(residuals = dr, eta = rep(eta, 3)): row names were found\nfrom a short variable and have been discarded\n\n# Now we can plot the data with the residuals on the y-axis, and the logit values on the x-axis.* \n\nggplot(dr_data, aes(x = eta, y = residuals)) + \n  geom_point(alpha = 0.35) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Deviance residuals\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nPearson residuals can also be useful in logistic regression. They measure deviations between the observed and fit1ted values. Pearson residuals are easier to plot than deviance residuals as the plot() function can be used.\n\nPlot the pearson residuals for the model.\n\n\nplot(fit2, which = 1)"
  },
  {
    "objectID": "labs/lab6_answers.html#confusion-matrix",
    "href": "labs/lab6_answers.html#confusion-matrix",
    "title": "Lab 6",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nYou can read about the confusion matrix on this Wikipedia page. This section tells you how to get some useful metrics from the confusion matrix to evaluate model performance.\n\nCreate two confusion matrices (one each for each model) using the classifications from the previous question. You can use the table() function, providing the modeled outcome as the true parameter and the classifications as the pred parameter.\n\n\n# The confusion matrix can be interpreted as follows:\n\n# True positives (bottom right) - correctly predicting people that survived\n# True negatives (top left) - correctly predicting people that did not survive\n# False positives (bottom left) - predicted people survived, when they did not\n# False negatives (top right) - predicting people did not survived, but they did\n\ncm1 &lt;- table(pred = pred1, true = titanic$Survived)\ncm2 &lt;- table(pred = pred2, true = titanic$Survived)\n\ncm1\n\n    true\npred   0   1\n   0 465 206\n   1  80 136\n\ncm2\n\n    true\npred   0   1\n   0 489 132\n   1  56 210\n\n\n\n# Note: the structure of the confusion matrix depends on the order in which you supply the outcome (i.e., if 0 and 1 were swapped, or if the predicted and true values are switched in rows and columns. \n\n# A third confusion matrix is given in which the order is changed. Always make sure that you read the confusion matrix carefully, so that you notice deviating structures. \n\ntable(true = titanic$Survived, pred = pred2)\n\n    pred\ntrue   0   1\n   0 489  56\n   1 132 210\n\n\n\nBased on the confusion matrices, which model do you think makes better predictions?\n\n\n# The second model makes more correct predictions according to the confusion matrix:\n\n# There are 505 correct true negative predictions, and 199 correct true positive predictions from model 2.\n# There are 469 correct true negative predictions and 136 true positive predictions from model 1 by comparison.\n\n# The first model makes more incorrect predictions too:*\n\n# There are 143 false negative predictions and 44 false positive predictions from model 2\n# There are 206 false negative predictions and 80 false positive predictions from model 1\n\n\nCalculate the accuracy, sensitivity, and specificity, false positive rate, positive and negative predictive values from the confusion matrix of the model that makes the best predictions.\n\n\n# First you need to extract the true negative, false negative, true positive, and false positive values from the confusion matrix.\nTN &lt;- cm2[1, 1]\nFN &lt;- cm2[1, 2]\nTP &lt;- cm2[2, 2]\nFP &lt;- cm2[2, 1]\n\n# Then you can use these values to calculate the metric asked for.\ntibble(\n  ACCURACY = (TP + TN) / sum(cm2),\n  SENSITIVITY = TP / (TP + FN),\n  SPECIFICITY = TN / (TN + FP),\n  FPR = FP / (TN + FP),\n  PPV = TP / (TP + FP),\n  NPV = TN / (TN + FN)\n  )\n\n# A tibble: 1 × 6\n  ACCURACY SENSITIVITY SPECIFICITY   FPR   PPV   NPV\n     &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.788       0.614       0.897 0.103 0.789 0.787\n\n\n\nExplain what the difference metrics mean in substantive terms?\n\n\n# Model accuracy is 0.79, meaning that 79% are correctly classified.\n\n# Model sensitivity is 0.58, meaning that if the passenger did survive, there is a 58% chance the model will detect this.\n\n# Model specificity is 0.92, meaning that if the passenger did not survive is an 92% chance the model will detect this.\n\n# Model FPR is 0.08, meaning that if a passenger in reality did not survive, there is a 8% chance that the model predicts this passenger as surviving.\n\n# Model PPV is 0.82, meaning that if a passenger is predicted as surviving, there is a 82% chance that this passenger indeed survived.\n\n# Model NPV is 0.78, meaning that if the passenger predicted as not surviving, there is a 78% chance that this passenger indeed did not survive.\n\n\nWhat does it mean for a model to have such low specificity, but high sensitivity?\n\n\n# Sensitivity and specificity are inversely proportioned. This means that as one increases, the other will decrease.\n\n# In our example we have moderate sensitivity, and very high specificity. High specificity means that there are few passengers predicted as not surviving when in reality they did survive. A moderate sensitivity means that relatively more passengers were predicted as surviving when in reality they did not survive. \n\nThe confusionMatrix() function from the caret package can do a lot of this for us. The function takes three arguments:\n\ndata - a vector of predicted classes (in factor form)\nreference - a vector of true classes (in factor from)\npositive - a character string indicating the ‘positive’ outcome. If not specified, the confusion matrix assumes that the first specified category is the positive outcome.\n\nYou can type ??confusionMatrix into the console to learn more.\n\nconfusionMatrix(as.factor(pred2), \n                reference = titanic$Survived, \n                positive = \"1\")\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction   0   1\n         0 489 132\n         1  56 210\n                                          \n               Accuracy : 0.788           \n                 95% CI : (0.7597, 0.8145)\n    No Information Rate : 0.6144          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.5334          \n                                          \n Mcnemar's Test P-Value : 4.502e-08       \n                                          \n            Sensitivity : 0.6140          \n            Specificity : 0.8972          \n         Pos Pred Value : 0.7895          \n         Neg Pred Value : 0.7874          \n             Prevalence : 0.3856          \n         Detection Rate : 0.2368          \n   Detection Prevalence : 0.2999          \n      Balanced Accuracy : 0.7556          \n                                          \n       'Positive' Class : 1"
  },
  {
    "objectID": "labs/lab5_answers.html",
    "href": "labs/lab5_answers.html",
    "title": "Lab 5",
    "section": "",
    "text": "We will use the following packages in this practical:\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(foreign)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(readr)\nIn this practical, you will perform regression analyses using glm() and inspect variables by plotting these variables, using ggplot()."
  },
  {
    "objectID": "labs/lab5_answers.html#working-with-odds-and-log-odds",
    "href": "labs/lab5_answers.html#working-with-odds-and-log-odds",
    "title": "Lab 5",
    "section": "Working with odds and log-odds",
    "text": "Working with odds and log-odds\nBefore we get started with logistic modelling it helps to understand how odds, log-odds, and probability are related. Essentially, they are all just different expressions of the same thing and converting between them involve simple formulas.\nCoefficients calculated using the glm() function returns log-odds by default. Most of us find it difficult to think in terms of log-odds, so instead we convert them to odds (or odds-ratios) using the exp() function. If we want to go from odds to log-odds, we just take the logarithm using log().\nAn odds-ratio is the probability of success and is defined as \\(Odds = \\frac{P}{1-P}\\), where \\(P\\) is the probability of an event happening and \\(1-P\\) is the probability that it does not happen. For example, if we have an 80% chance of a sunny day, then we have a 20% chance of a rainy day. The odds would then equal \\(\\frac{.80}{.20} = 4\\), meaning the odds of a sunny day are 4 to 1. Let’s consider this further with an example.\nThe code below creates a data frame called data with a column called conc showing the number of trials wherein different concentrations of the peptide-C protein inhibited the flow of current across a membrane. The yes column contains counts of trials where this occured.\n\ndata &lt;- data.frame(conc = c(0.1, 0.5, 1, 10, 20, 30, 50, 70, 80, 100, 150),\n                   no = c(7, 1, 10, 9, 2, 9, 13, 1, 1, 4, 3),\n                   yes = c(0, 0, 3, 4, 0, 6, 7, 0, 0, 1 ,7)\n                   ) \ndata\n\n    conc no yes\n1    0.1  7   0\n2    0.5  1   0\n3    1.0 10   3\n4   10.0  9   4\n5   20.0  2   0\n6   30.0  9   6\n7   50.0 13   7\n8   70.0  1   0\n9   80.0  1   0\n10 100.0  4   1\n11 150.0  3   7\n\n\n\nAdd the following variables to the dataset:\n\n\nthe total number of trials for each observation (i.e., the sum of the no and yes trials for each row)\nthe proportion of yes trials in each row (i.e. yes divided by the total)\nthe log-odds of inhibition for each row (i.e. the log-odds of yes vs no)\n\n\ndata &lt;- \n  data %&gt;% \n  mutate(total = no + yes,\n         prop = yes / total,\n         logit = qlogis(prop)\n         )\n\n# The `qlogis()` function is equivalent to the log-odds (i.e, logit) function.\n\n\nInspect the new columns. Do you notice anything unusual?\n\n\nhead(data)\n\n  conc no yes total      prop      logit\n1  0.1  7   0     7 0.0000000       -Inf\n2  0.5  1   0     1 0.0000000       -Inf\n3  1.0 10   3    13 0.2307692 -1.2039728\n4 10.0  9   4    13 0.3076923 -0.8109302\n5 20.0  2   0     2 0.0000000       -Inf\n6 30.0  9   6    15 0.4000000 -0.4054651\n\n#There are many zero proportions which produce logit values of infinity. We can work around this issue by adding a constant (usually 0.5) to all cells before calculating the log-odds. We add the same value to the numerator and denominator of our odds formula, so we don't change the relative interpretations of the odds. We could also add a 1 to each cell. This option is conceptually interesting because the log of 1 equals 0. It's almost like we're adding zero to the odds and still correcting the issue.\n\n\nAdd a new column to your dataset containing the corrected odds.\n\nYou can compute the value of this column using the following formulation of the log-odds:\n\\[ log(odds) = log(\\frac{yes + 0.5} {no + 0.5}) \\]\n\nrobustLogit &lt;- function(x, y) log((x + 0.5) / (y + 0.5))\n\ndata &lt;- data %&gt;% \n  mutate(logit2 = robustLogit(yes, no))\n\ndata\n\n    conc no yes total      prop      logit     logit2\n1    0.1  7   0     7 0.0000000       -Inf -2.7080502\n2    0.5  1   0     1 0.0000000       -Inf -1.0986123\n3    1.0 10   3    13 0.2307692 -1.2039728 -1.0986123\n4   10.0  9   4    13 0.3076923 -0.8109302 -0.7472144\n5   20.0  2   0     2 0.0000000       -Inf -1.6094379\n6   30.0  9   6    15 0.4000000 -0.4054651 -0.3794896\n7   50.0 13   7    20 0.3500000 -0.6190392 -0.5877867\n8   70.0  1   0     1 0.0000000       -Inf -1.0986123\n9   80.0  1   0     1 0.0000000       -Inf -1.0986123\n10 100.0  4   1     5 0.2000000 -1.3862944 -1.0986123\n11 150.0  3   7    10 0.7000000  0.8472979  0.7621401\n\n\n\nFit a logistic regression model where:\n\n\nprop is the outcome\nconc is the only predictor\nthe number of total trials per row are used as weights (we need this because a different number of trials can go into defining each observation of prop)\n\nInterpret the slope estimate.\n\nsummary(glm(prop ~ conc, \n            family = binomial, \n            weights = total, \n            data = data))\n\n\nCall:\nglm(formula = prop ~ conc, family = binomial, data = data, weights = total)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -1.32701    0.33837  -3.922 8.79e-05 ***\nconc         0.01215    0.00496   2.450   0.0143 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16.683  on 10  degrees of freedom\nResidual deviance: 10.389  on  9  degrees of freedom\nAIC: 30.988\n\nNumber of Fisher Scoring iterations: 4\n\n# A unit increase in conc increases the log-odds of inhibition by 0.0121 units, and this increase is statistically significant.\n\n# If we exponentiate the slope estimate, we can get an interpretation in odds units, but the effect becomes multiplicative instead of additive. So for every unit increase in conc, the odds of inhibition are 1.01215 times higher. Note then that odds above 1 indicate inhibition is x-times higher, while odds below 1 indicate inhibition is x-times less."
  },
  {
    "objectID": "labs/lab5_answers.html#titanic-data",
    "href": "labs/lab5_answers.html#titanic-data",
    "title": "Lab 5",
    "section": "Titanic data",
    "text": "Titanic data\nYou will work with the titanic data set which you can download here, containing information on the fate of passengers on the infamous voyage.\n\nSurvived: this is the outcome variable that you are trying to predict, with 1 meaning a passenger survived and 0 meaning they did not\nPclass: this is the ticket class the passenger was travelling on, with 1, 2, and 3 representing 1st, 2nd and 3rd class respectively\nAge: this is the age of the passenger in years\nSex: this is the sex of the passenger, either male or female\n\n\nRead in the data from the “titanic.csv” file, selecting only the variables Survived, Pclass, Sex and Age. If necessary, correct the class of the variables.\n\n\ntitanic &lt;- read_csv(here::here(\"data\", \"titanic.csv\")) %&gt;% \n  mutate(Survived = as.factor(Survived),\n         Sex = as.factor(Sex),\n         Pclass = as.factor(Pclass))\n\nRows: 887 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Sex\ndbl (6): Survived, Pclass, Age, Siblings/Spouses Aboard, Parents/Children Ab...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nWhat relationships do you expect to find between the predictor variables and the outcome?\n\n\n# We could say that:\n# class is related to the outcome as passengers travelling on a higher class ticket have a higher probability of survival\n# sex is related to the outcome as women have a higher probability of survival\n# age is related to the outcome as younger passengers have a higher probability of survival\n\n\nInvestigate how many passengers survived in each class. You can do this visually by creating a bar plot, or by using the table() function. Search ??table for more information.\n\n\ntitanic %&gt;% \n  ggplot(aes(Pclass, fill = Survived)) +\n  geom_bar(position = \"dodge\") +\n  labs(x = \"Passenger Class\",\n       y = \"Count\") +\n  theme_bw()\n\n\n\n\n\n\n\n# The bar plot clearly shows that people in lower class were less likely to survive.\n# We can also use the `prop.table()` function to investigate this. The argument `margin = 1` turns the counts to marginal proportions.\n\ntitanic %$% \n  table(Pclass, Survived) %&gt;% \n  prop.table(margin = 1) %&gt;% \n  round(2)\n\n      Survived\nPclass    0    1\n     1 0.37 0.63\n     2 0.53 0.47\n     3 0.76 0.24\n\n\n\nSimilarly, investigate the relationship between survival and sex by creating a bar plot and a table.\n\n\ntitanic %$% \n  table(Sex, Survived) %&gt;% \n  prop.table(margin = 1) %&gt;% \n  round(2)\n\n        Survived\nSex         0    1\n  female 0.26 0.74\n  male   0.81 0.19\n\n# The table shows the proportion of males and females that survived versus those who did not survive. Females are much more likely to have survived than males.\n\ntitanic %&gt;% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(position = \"dodge\") +\n  labs(x = \"Sex\",\n       y = \"Count\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nInvestigate the relationship between age and survival by creating a histogram of the age of survivors versus non-survivors.\n\n\ntitanic %&gt;% \n  ggplot(aes(Age, fill = Survived)) +\n  geom_histogram(colour = \"white\") +\n  labs(x = \"Age\",\n       y = \"Count\") +\n  facet_wrap(~Survived) +\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n# The distribution of age is different for survivors and non-survivors. Younger passengers have higher chances of survival compared to older passengers."
  },
  {
    "objectID": "labs/lab5_answers.html#no-predictors",
    "href": "labs/lab5_answers.html#no-predictors",
    "title": "Lab 5",
    "section": "No predictors",
    "text": "No predictors\n\nSpecify a logistic regression model where “Survived” is the outcome and there are no predictors.\n\n\nglm(Survived ~ 1, \n    family = binomial, \n    data = titanic) %&gt;% \n  summary()\n\n\nCall:\nglm(formula = Survived ~ 1, family = binomial, data = titanic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.46598    0.06898  -6.755 1.43e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1182.8  on 886  degrees of freedom\nResidual deviance: 1182.8  on 886  degrees of freedom\nAIC: 1184.8\n\nNumber of Fisher Scoring iterations: 4\n\n# A logistic regression without any predictors is simply modelling the log-odds of survival for the entire population (the intercept, beta0).\n# The log-odds are -0.473, and the odds are $exp(-0.473) = 0.623$.\n\n# We can also get the odds from a frequency table: the probability of survival is $342/549 = 0.623$. The log-odds equals exp(beta0) = -0.473.\n\ntitanic %&gt;% \n  count(Survived) %&gt;% \n  mutate(prop = prop.table(n)) %&gt;% \n  kbl(digits = 2) %&gt;% \n  kable_paper(bootstrap_options = \"striped\", full_width = FALSE)\n\n\n\n\nSurvived\nn\nprop\n\n\n\n\n0\n545\n0.61\n\n\n1\n342\n0.39"
  },
  {
    "objectID": "labs/lab5_answers.html#binary-predictor",
    "href": "labs/lab5_answers.html#binary-predictor",
    "title": "Lab 5",
    "section": "Binary predictor",
    "text": "Binary predictor\n\nSpecify a logistic regression model where “Survived” is the outcome and “Sex” is the only predictor.\n\n\nglm(Survived ~ Sex, \n    family = binomial, \n    data = titanic) %&gt;% \n  summary()\n\n\nCall:\nglm(formula = Survived ~ Sex, family = binomial, data = titanic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.0566     0.1290   8.191 2.58e-16 ***\nSexmale      -2.5051     0.1672 -14.980  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1182.77  on 886  degrees of freedom\nResidual deviance:  916.12  on 885  degrees of freedom\nAIC: 920.12\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nWhat does the intercept mean? What are the odds and what are the log-odds of survival for males?\n\n\n# In the model with one dichotomous predictor we are modelling logit(p) = beta0 + beta1*male.\n\n# The intercept is the log-odds of survival for women (1.0566), since the reference group is female.\n\n# The log-odds of survival for men is -2.5137 lower than for women. The odds of survival for men is 0.081, or 92% lower than females."
  },
  {
    "objectID": "labs/lab5_answers.html#categorical-predictor-more-than-2-categories",
    "href": "labs/lab5_answers.html#categorical-predictor-more-than-2-categories",
    "title": "Lab 5",
    "section": "Categorical predictor (more than 2 categories)",
    "text": "Categorical predictor (more than 2 categories)\n\nSpecify a logistic regression model where “Survived” is the outcome and “Pclass” is the only predictor.\n\n\nglm(Survived ~ Pclass, \n    family = binomial, \n    data = titanic) %&gt;% \n  summary()\n\n\nCall:\nglm(formula = Survived ~ Pclass, family = binomial, data = titanic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   0.5306     0.1409   3.766 0.000166 ***\nPclass2      -0.6394     0.2041  -3.133 0.001731 ** \nPclass3      -1.6596     0.1760  -9.430  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1182.8  on 886  degrees of freedom\nResidual deviance: 1080.9  on 884  degrees of freedom\nAIC: 1086.9\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nWhich category is the reference group? What are their odds of survival?\n\n\n# The reference group are 1st class passengers, represented by the intercept.\n# The log-odds of survival for 1st class passengers is 0.5306.\n# The odds are 1.70, meaning 1st class passengers are 70% more likely to survive.\n\n\nWhat are the chances of survival for 2nd and 3rd class passengers?\n\n\n# For 2nd class passengers, the log-odds of survival is -0.6394.\n# The odds are  0.527, meaning 2nd class passengers are 47% less likely to survive than 1st class passengers.\n\n# For 3rd class passengers, the log-odds of survival is -1.646.\n# The odds are 0.188, meaning 3nd class passengers are 81% less likely to survive than 1st class passengers."
  },
  {
    "objectID": "labs/lab5_answers.html#continuous-predictor",
    "href": "labs/lab5_answers.html#continuous-predictor",
    "title": "Lab 5",
    "section": "Continuous predictor",
    "text": "Continuous predictor\n\nSpecify a logistic regression model where “Survived” is the outcome and “Age” is the only predictor.\n\nSave this model as you will come back to it later.\n\nfit1 &lt;- glm(Survived ~ Age, \n            family = binomial, \n            data = titanic)\nsummary(fit1)\n\n\nCall:\nglm(formula = Survived ~ Age, family = binomial, data = titanic)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -0.209189   0.159494  -1.312   0.1897  \nAge         -0.008774   0.004947  -1.774   0.0761 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1182.8  on 886  degrees of freedom\nResidual deviance: 1179.6  on 885  degrees of freedom\nAIC: 1183.6\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nWhat does the intercept mean when there is a continuous predictor?\n\n\n# In the case of a continuous predictor there is no real reference group. Instead, the intercept is the log-odds of survival when age = 0. In this model, the log-odds of survival for passengers of age 0 is -0.143, corresponding with the odds of survival at 0.867 (= exp(log odds)).\n\n\nHow are the odds and log-odds interpreted for a continuous predictor?\n\n\n# For continuous predictors, the log-odds either increase or decrease with every unit increase in the continuous predictor. So, in our model:\n# For every increase in age of one year, the log-odds of survival decrease by -0.011, meaning that as age increases the chances of survival decrease.\n# For every increase in age of one year, the odds of survival are 0.99 (= exp(-0.0112)) times the odds of those with one age unit less, or -1.09%."
  },
  {
    "objectID": "labs/lab5_answers.html#multinomial-model-with-an-interaction-term",
    "href": "labs/lab5_answers.html#multinomial-model-with-an-interaction-term",
    "title": "Lab 5",
    "section": "Multinomial model with an interaction term",
    "text": "Multinomial model with an interaction term\n\nSpecify a logistic regression model Survived is the outcome and Pclass plus an interaction between Sex and Age as the predictor.\n\nSave this model as we will return to it later.\n\nfit2 &lt;- glm(Survived ~ Pclass + Sex*Age, family = binomial, data = titanic)\nsummary(fit2)\n\n\nCall:\nglm(formula = Survived ~ Pclass + Sex * Age, family = binomial, \n    data = titanic)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.958264   0.407568   7.258 3.92e-13 ***\nPclass2     -1.431734   0.286920  -4.990 6.04e-07 ***\nPclass3     -2.609380   0.270236  -9.656  &lt; 2e-16 ***\nSexmale     -1.124477   0.401734  -2.799  0.00513 ** \nAge         -0.002537   0.010807  -0.235  0.81437    \nSexmale:Age -0.054463   0.013856  -3.931 8.48e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1182.8  on 886  degrees of freedom\nResidual deviance:  785.3  on 881  degrees of freedom\nAIC: 797.3\n\nNumber of Fisher Scoring iterations: 5\n\n\n\nHow is the significant interaction term interpreted in this model?\n\n\n# The interaction between age and sex is significant, suggesting the slopes for age on survival are different for males and females."
  },
  {
    "objectID": "labs/lab5_answers.html#deviance",
    "href": "labs/lab5_answers.html#deviance",
    "title": "Lab 5",
    "section": "Deviance",
    "text": "Deviance\nDeviance is measure of the goodness-of-fit in a GLM where lower deviance indicates a better fitting model. R reports two types of deviance:\n\nnull deviance: how well the outcome is predicted by the intercept-only model\nresidual deviance: how well the outcome is predicted by the model with the predictors added\n\n\nGet the model summaries and indicate what the null and residual deviance are.\n\n\n# You can use the `summary()` command to get the deviance statistics for each model. The null and residual deviance are below the model coefficients. \n\nsummary(fit1)\n\n\nCall:\nglm(formula = Survived ~ Age, family = binomial, data = titanic)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -0.209189   0.159494  -1.312   0.1897  \nAge         -0.008774   0.004947  -1.774   0.0761 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1182.8  on 886  degrees of freedom\nResidual deviance: 1179.6  on 885  degrees of freedom\nAIC: 1183.6\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(fit2)\n\n\nCall:\nglm(formula = Survived ~ Pclass + Sex * Age, family = binomial, \n    data = titanic)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.958264   0.407568   7.258 3.92e-13 ***\nPclass2     -1.431734   0.286920  -4.990 6.04e-07 ***\nPclass3     -2.609380   0.270236  -9.656  &lt; 2e-16 ***\nSexmale     -1.124477   0.401734  -2.799  0.00513 ** \nAge         -0.002537   0.010807  -0.235  0.81437    \nSexmale:Age -0.054463   0.013856  -3.931 8.48e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1182.8  on 886  degrees of freedom\nResidual deviance:  785.3  on 881  degrees of freedom\nAIC: 797.3\n\nNumber of Fisher Scoring iterations: 5\n\n# For model 1, the null deviance is 1186.7 and the residual deviance is 1182.3 For model 2, the null deviance is 1186.66 and the residual deviance is 793.82\n\nWe can use the anova() function to perform an analysis of deviance that compares the difference in deviances between competing models.\n\nCompare the fit of model 1 with the fit of model 2 using anova() andtest = “Chisq”`.\n\n\nanova(fit1, fit2, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: Survived ~ Age\nModel 2: Survived ~ Pclass + Sex * Age\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       885     1179.6                          \n2       881      785.3  4    394.3 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# The analysis of deviance indicates that there is a reduction in residual deviance of 388.46 that is statistically significant. Model 2 is a better model. For a binomial model, the statistical test should be the chi-square difference test."
  },
  {
    "objectID": "labs/lab5_answers.html#information-criteria",
    "href": "labs/lab5_answers.html#information-criteria",
    "title": "Lab 5",
    "section": "Information criteria",
    "text": "Information criteria\nAIC is the Akaike’s Information Criterion, a method for assessing model quality through comparison of related models. AIC is based on the deviance but introduces a penalty for more complex models. The number itself is not meaninful, and it is only useful when comparing models against one another. Like deviance, the model with the lowest AIC is best.\n\nUse the AIC() function to get the AIC value for model 1 and model 2.\n\n\nAIC(fit1, fit2)\n\n     df      AIC\nfit1  2 1183.593\nfit2  6  797.297\n\n# The AIC for model 2 is lower than the AIC for model 1, indicating that model 2 has a better fit\n\nBIC is the Bayesian Information Criterion and is very similar to AIC, but penalises a complex model more than the AIC would. Complex models will have a larger score indicating worse fit. One difference to the AIC is that the probability of selecting the correct model with the BIC increases as the sample size of the training set increases.\n\nUse the BIC() function to get the BIC value for model 1 and model 2. ::: {.cell}\n\nBIC(fit1, fit2)\n\n     df      BIC\nfit1  2 1193.169\nfit2  6  826.024\n\n# The BIC for model 2 is lower than the BIC for model 1, indicating that model 2 has a better fit\n:::\n\nWhich model should we proceed with?\n\n\n# Model 2, as it has lower residual deviance, AIC and BIC."
  },
  {
    "objectID": "labs/lab4.html",
    "href": "labs/lab4.html",
    "title": "Lab 4",
    "section": "",
    "text": "In this practical, the assumptions of the linear regression model will be discussed. You will practice with checking the different assumptions, and practice with accounting for some of the assumptions with additional steps. Please note that these assumptions can only be checked once you have selected a (final) model, as these assumptions ‘need’ a model that they apply to.\nWe will use the following packages in this practical:\n\nlibrary(magrittr)\nlibrary(ggplot2) \nlibrary(regclass)\nlibrary(MASS)"
  },
  {
    "objectID": "labs/lab4.html#linearity",
    "href": "labs/lab4.html#linearity",
    "title": "Lab 4",
    "section": "Linearity",
    "text": "Linearity\nWith the assumption of linearity, it is assumed that the relation between the dependent and independent variables is (more or less) linear. You can check this by generating a scatterplot using a predictor variable and outcome variable of the regression model.\n\nCheck whether there is a linear relation between the variables vertical length and the cross length.\nNext check the relation between weight and height.\nDescribe both plots. What differences do you see?\n\nWhen a non-linear relation is present, you can either choose another model to use, or transform the predictor before adding it to the model, for example using a log-transformation. Applying a transformation, however, will not always solve the problem, and makes interpretation of the model less intuitive.\n\nApply a log-transformation to the weight variable.\nPlot the relation between length and weight again, but now including the transformed variable.\nDescribe if the transformation improved the linear relation."
  },
  {
    "objectID": "labs/lab4.html#predictor-matrix-full-rank",
    "href": "labs/lab4.html#predictor-matrix-full-rank",
    "title": "Lab 4",
    "section": "Predictor matrix full rank",
    "text": "Predictor matrix full rank\nThis assumption states that:\n\nthere need to be more observations than predictors (n &gt; P).\n\nno predictor can be a linear combination of other predictors; predictors cannot have a very high correlation (multicollinearity).\n\n\nThe first part of this assumption is easy to check: see if the number of observations minus the number of predictors is a positive number. The second part can be checked by either obtaining correlations between the predictors, or by determining the VIF (variance inflation factor). When the VIF is above 10, this indicate high multicollinearity. To account for this, predictors can be excluded from the model, or a new variable can be constructed based on predictors with a high correlation.\nTo examine VIF scores, the function VIF() from the regclass can be used on a prespecified model. If this model includes a categorical variable with multiple categories, such as ‘species’ in the example data, the generalized VIF is used, and we have to look at the third column (GVIF^*(1/(2*Df))), these values can be compared to normal VIF values.\n\nSpecify a linear model with weight as outcome variable using all other variables in the dataset as predictors. Save this model as model_fish1. Calculate VIF values for this model.\nCheck the VIF scores. If VIF scores exceed a score of 10, give substantial explanation why the VIF scores are this high.\nWhat adjustments can be made to the model to account for multicollinearity in this case?\nRun a new model which only includes one of the three length variables and call it model_fish2. Describe if there is an improvement.\nWhat happens with the regression model when there are more predictors than observations?"
  },
  {
    "objectID": "labs/lab4.html#exogenous-predictors",
    "href": "labs/lab4.html#exogenous-predictors",
    "title": "Lab 4",
    "section": "Exogenous predictors",
    "text": "Exogenous predictors\nFor this assumption, the expected value of the errors (mean of the errors) must be 0. Furthermore, The errors must be independent of the predictors.\n\nWhat is the possible consequence of not meeting this assumption?"
  },
  {
    "objectID": "labs/lab4.html#constant-finite-error-variance",
    "href": "labs/lab4.html#constant-finite-error-variance",
    "title": "Lab 4",
    "section": "Constant, finite error variance",
    "text": "Constant, finite error variance\nThis assumptions is also called ‘the assumption of homoscedasticity’. It states that the variance of the error terms should be constant over all levels of the predictors. This can be checked by plotting the residuals against the fitted values. These plots can be obtained by simply taking the first plot of a specified model, plot(model_x).\n\nCreate a residual vs fitted values plot for model_fish1, which is the first plot generated by the plot() function.\nLoad in the iris data, and specify a model where sepal length is predicted by all other variables and save this as model_iris1.\nCreate a residual vs fitted plot for this model as well.\nDiscuss both plots and indicate whether the assumption is met.\nDiscuss what the consequence would be if this assumption is violated."
  },
  {
    "objectID": "labs/lab4.html#independent-errors",
    "href": "labs/lab4.html#independent-errors",
    "title": "Lab 4",
    "section": "Independent errors",
    "text": "Independent errors\nThis assumption states that error terms should have no correlation. Dependence of the errors can result from multiple things. First, there is a possible dependence in the error terms when there is serial dependence, for example because the data contains variables that are measured over time. Another reason can be when there is a cluster structure in the data, for example students in classes in schools.\n\nHow can both causes of correlated error terms be detected, and what can be done to solve the problem?"
  },
  {
    "objectID": "labs/lab4.html#normally-distributed-errors",
    "href": "labs/lab4.html#normally-distributed-errors",
    "title": "Lab 4",
    "section": "Normally distributed errors",
    "text": "Normally distributed errors\nThis assumption states that errors should be roughly normally distributed. Like the assumption of homoscedasticity, this can be checked by model plots, provided by R.\n\nCreate a QQ plot for model_iris1, which is the second plot generated by the plot() function. Indicate whether the assumption is met.\nCreate a new model using the fish data, where diagonal_width is predicted by cross_length, and store the model as model_fish3.\nCreate a QQ plot for model_fish3.\nInterpret the two plots. Is the assumption met in both cases?\nIn what cases is it problematic that the assumption is not met? And in what cases is it no problem?"
  },
  {
    "objectID": "labs/lab4.html#outliers",
    "href": "labs/lab4.html#outliers",
    "title": "Lab 4",
    "section": "Outliers",
    "text": "Outliers\nOutliers are observations that show extreme outcomes compared to the other data, or observations with outcome values that fit the model very badly. Outliers can be detected by inspecting the externally studentized residuals.\n\nMake a plot of studentized residuals by using the functions rstudent and plot for `model_fish1. What do you conclude?\nMake a plot of studentized residuals for model_iris1.\nStore the dataset Animals from the MASS package. Define a regression model where animals’ body weight is predicted by brain weight and store it as model_animals1.\nMake a plot of the studentized residuals for model_animals1."
  },
  {
    "objectID": "labs/lab4.html#high-leverage-observations",
    "href": "labs/lab4.html#high-leverage-observations",
    "title": "Lab 4",
    "section": "High-leverage observations",
    "text": "High-leverage observations\nHigh-leverage observations are observations with extreme predictor values. To detect these observations, we look at their leverage values. These values can be summarized in a leverage plot.\n\nFor the model specified under model_animals1, create a leverage plot by plotting the hatvalues() of the model."
  },
  {
    "objectID": "labs/lab4.html#influence-on-the-model",
    "href": "labs/lab4.html#influence-on-the-model",
    "title": "Lab 4",
    "section": "Influence on the model",
    "text": "Influence on the model\nBoth outliers and observations with high leverage are not necessarily a problem. Cases that are both, however, seem to form more of a problem. These cases can influence the model heavily and can therefore be problematic.\nInfluence measures come in two sorts: Cook’s distance checks for influential observations, while DFBETAS check for influential, and possible problematic, observations per regression coefficients.\n\nFor model_animals1, check Cooks distance by plotting the cooks.distance of the model.\nFor model_animals1, check the DFBETAS by using the function dfbetas.\nDescribe what you see in the plots for Cook’s distance and DFBETAS. What do you conclude?\nDelete the problematic observation that you found in Question 12 and store the dataset under a new name.\nFit the regression model where animals’ body weight is predicted by brain weight using the adjusted dataset and store it as model_animals2.\nCompare the output to model_animals1 and describe the changes.\nRun the plots for influential observations again on this new model and see if anything changes."
  },
  {
    "objectID": "labs/lab3.html",
    "href": "labs/lab3.html",
    "title": "Lab 3",
    "section": "",
    "text": "We will use the following packages in this practical:\n\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nWe will use the same data that we used last week to perform a simple linear regression, the Iris dataset. But now, we will extend on this simple model with multiple variables.\nIn order to do this, we first need to load the data again and run the simple model where Sepal length is predicted by Petal width.\n\ndata &lt;- iris # load the data\nhead(iris)   # inspect the data\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n# specify model\nmodel1 &lt;- lm(Sepal.Length ~ Petal.Width, \n             data = data)\n\n# ask for summary\nsummary(model1)\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Width, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.38822 -0.29358 -0.04393  0.26429  1.34521 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.77763    0.07293   65.51   &lt;2e-16 ***\nPetal.Width  0.88858    0.05137   17.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.478 on 148 degrees of freedom\nMultiple R-squared:  0.669, Adjusted R-squared:  0.6668 \nF-statistic: 299.2 on 1 and 148 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "labs/lab3.html#categorical-predictors",
    "href": "labs/lab3.html#categorical-predictors",
    "title": "Lab 3",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nUp to here, we only included continuous predictors in our models. We will now include a categorical predictor in the model as well.\nWhen a categorical predictor is added, this predictor is split in several contrasts (or dummies), where each group is compared to a reference group. In our example Iris data, the variable ‘Species’ is a categorical variable that indicate the species of flower. This variable can be added as example for a categorical predictor. Contrasts, and thus the dummy coding, can be inspected through contrasts().\n\nAdd species as a predictor to the model specified as model2, store it under the name model3 and interpret the categorical coefficients of this new model."
  },
  {
    "objectID": "labs/lab3.html#calculating-new-predicted-values-with-a-regression-equation",
    "href": "labs/lab3.html#calculating-new-predicted-values-with-a-regression-equation",
    "title": "Lab 3",
    "section": "Calculating new predicted values with a regression equation",
    "text": "Calculating new predicted values with a regression equation\nA regression model can be used to predict values for new cases that were not used to built the model. The regression equation always consists of coefficients (\\(\\beta\\)s) and observed variables (\\(X\\)):\n\\[\\hat{y} = \\beta_0 + \\beta_1 * X_{a}* + \\beta_2 * X_b +  \\ldots  + \\beta_n * X_n\\]\nAll terms can be made specific for the regression equation of the created model. For example, if we have a model where ‘happiness’ is predicted by age and income (scored from 1-50), the equation could look like:\n\\[\\hat{y}_{happiness} = \\beta_{intercept} + \\beta_{age} * X_{age} + \\beta_{income} * X_{income}\\]\nThen, we could impute the coefficients obtained through the model. Given \\(\\beta_{intercept} = 10.2\\), \\(\\beta_{age} = 0.7\\), and \\(\\beta_{income} = 1.3\\), the equation would become:\n\\[\\hat{y}_{happiness} = 10.2 + 0.7 * X_{age} + 1.3 * X_{income}\\]\nIf we now want to predict the happiness score for someone of age 28 and with an income score of 35, the prediction would become:\n\\[\\hat{y}_{happiness} = 10.2 + 0.7 * 28 + 1.3 * 35 = 75.3\\]\n\nGiven this regression equation, calculate the predicted value for someone of age 44 and an income score of 27."
  },
  {
    "objectID": "labs/lab3.html#prediction-with-a-categorical-variable",
    "href": "labs/lab3.html#prediction-with-a-categorical-variable",
    "title": "Lab 3",
    "section": "Prediction with a categorical variable",
    "text": "Prediction with a categorical variable\nAdding a categorical predictor to the regression equation gives the number of contrasts as coefficient terms added. The previous regression equation for predicting happiness could be adjusted by adding ‘living density’ as a categorical predictor with levels ‘big city’, ‘smaller city’, ‘rural’, where ‘big city’ would be the reference category. The equation could then be:\n\\[\\hat{y}_{happiness} = 10.2 + 0.7 * X_{age} + 1.3 * X_{income} + 8.4 * X_{smaller city} + 17.9 * X_{rural}\\]\nWhen predicting a score for an equation with a categorical predictor, you just assign a 1 to the category that the observation belongs to, and 0s for all other categories.\n\nGiven this equation, calculate the predicted score for someone of age 29, an income score of 21, and living in a smaller city. And what would this score be if the person would live in a big city instead?"
  },
  {
    "objectID": "labs/lab3.html#prediction-with-an-interaction",
    "href": "labs/lab3.html#prediction-with-an-interaction",
    "title": "Lab 3",
    "section": "Prediction with an interaction",
    "text": "Prediction with an interaction\nIn regression equations with an interaction, an extra coefficient is added to the equation. For example, the happiness equation with age and income as predictors could have an added interaction term. The equation could then look like:\n\\[\\hat{y}_{happiness} = 10.2 + 0.7 * X_{age} + 1.3 * X_{income} + 0.01 * X_{age} * X_{income}\\]\nFor a person of age 36 and income score 30, the predicted score would be:\n\\[\\hat{y}_{happiness} = 10.2 + 0.7 * 36 + 1.3 * 30 + 0.01 * 36 * 30 = 85.2\\]\n\nGiven this regression equation with interaction term, what would be the predicted happiness score for someone of age 52 and income score 26?"
  },
  {
    "objectID": "labs/lab2.html#variation",
    "href": "labs/lab2.html#variation",
    "title": "Lab 2",
    "section": "Variation",
    "text": "Variation\nVariation describes how the values of a variable change each time you measure it. A continuous variable that is measured on one occasion will have different values to that same variable measured on another occasion. This variation between measurement occasions are what we call “error”. Categorical variables also vary across measurement occasions or across individuals. Visualising variation is a very intuitive way to understand trends or patterns within a variable."
  },
  {
    "objectID": "labs/lab2.html#covariation",
    "href": "labs/lab2.html#covariation",
    "title": "Lab 2",
    "section": "Covariation",
    "text": "Covariation\nCovariation describes how the values of at least two variables vary together or are related. Again, visualising covariation is also the most intuitive way to understand patterns between variables."
  },
  {
    "objectID": "labs/lab2.html#non-graphical-eda",
    "href": "labs/lab2.html#non-graphical-eda",
    "title": "Lab 2",
    "section": "Non-Graphical EDA",
    "text": "Non-Graphical EDA\nIf you are interested in a particular aspect of a variable you can summarise this information in a table. We have covered how to create nicely formatted tables in the previous practical, so the example below is just to refresh your memory."
  },
  {
    "objectID": "labs/lab2.html#summary-tables",
    "href": "labs/lab2.html#summary-tables",
    "title": "Lab 2",
    "section": "Summary tables",
    "text": "Summary tables\nBelow we summarise hwy by vehicle class, by asking for commonly used summary statistics.\n\nmpg %&gt;% \n  group_by(class) %&gt;%         # by class\n  summarise(min = min(hwy),   # minimum of hwy\n            mean = mean(hwy), # mean\n            sd = sd(hwy),     # standard deviation\n            max = max(hwy),   # maximum\n            n = n())  %&gt;%     # nr of observations\n  kbl() %&gt;%  \n  kable_styling(latex_options = c(\"striped\", \"hover\"), full_width = F)  \n\n\n\n\nclass\nmin\nmean\nsd\nmax\nn\n\n\n\n\n2seater\n23\n24.80000\n1.303840\n26\n5\n\n\ncompact\n23\n28.29787\n3.781620\n44\n47\n\n\nmidsize\n23\n27.29268\n2.135930\n32\n41\n\n\nminivan\n17\n22.36364\n2.062655\n24\n11\n\n\npickup\n12\n16.87879\n2.274280\n22\n33\n\n\nsubcompact\n20\n28.14286\n5.375012\n44\n35\n\n\nsuv\n12\n18.12903\n2.977973\n27\n62\n\n\n\n\n\n\n\n\nCreate a summary of engine displacement by year, including the minimum, maximum, median, and inter quantile range."
  },
  {
    "objectID": "labs/lab2.html#graphical-eda-base-r-vs-ggplot",
    "href": "labs/lab2.html#graphical-eda-base-r-vs-ggplot",
    "title": "Lab 2",
    "section": "Graphical EDA: base R vs ggplot",
    "text": "Graphical EDA: base R vs ggplot\nPlots can be made using base R functions like plot(), hist(), or barplot(). Here are some examples of how this works on the mpg data.\n\nhist(mpg$displ)\n\n\n\n\n\n\n\n\n\nbarplot(table(mpg$cyl))\n\n\n\n\n\n\n\n\n\nplot(x = mpg$displ, y = mpg$hwy,\n     xlab = \"Highway mpg\",\n     ylab = \"Engine displacement (L)\")\n\n\n\n\n\n\n\n\nThese plots are quick to produce and are useful for an initial understanding of the data, but the syntax used to create them is specific to the type of plot. In contrast, ggplot has a streamlined approach to plotting involving largely the same steps regardless of plot type. Specifically, in ggplot we build up visualisations layer by layer using the + operator (which is similar to the %&gt;% operator we are already familiar with).\nThe main, important, layers in ggplot are:\n\nPass the data to ggplot()\nChoose aesthetic mappings with aes()\nChoose geometric components with geom()\nChoose additional labels, themes, and visuals\nChoose faceting if it is applicable with facet()\n\nWhat does this layering system look like? In Chapter 3: Data Visualisation of Hadley Wickham’s R4DS book, there is a general ggplot template to give you an idea of how plots are layered.\n```{r}\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(\n     mapping = aes(&lt;MAPPINGS&gt;)) +\n  &lt;COORDINATE_FUNCTION&gt; +\n  &lt;FACET_FUNCTION&gt; +\n  &lt;THEME&gt; \n```\nBelow is a real example of how this looks. Remember that you do not have to / need to use all of the ggplot options (and there are many many more than presented here). However, this gives you an idea of how the layers of these plots are built up.\n\nggplot(iris) + # Data\n  geom_point(mapping = aes(x = Sepal.Length, # Variable on the x-axis\n                           y = Sepal.Width, # Variable on the y-axis\n                           colour = Species)) + # Legend \n  labs(x = \"Sepal Length\",\n       y = \"Sepal Width\", \n       title = \"Relationship between Sepal Length and Width by Species\") +\n  coord_cartesian() + # Default standard for mapping x and y\n  facet_wrap(~Species) + # Splits plot by the species variable\n  theme_bw() # Sets the background theme\n\n\n\n\n\n\n\n\nYou can read about different ggplot2 options here. The reference guides by RPubs, STHDA and Data Novia will be useful once you grasp the basics and want to make more complicated visualisations."
  },
  {
    "objectID": "labs/lab2.html#visualising-distributions-of-single-variables",
    "href": "labs/lab2.html#visualising-distributions-of-single-variables",
    "title": "Lab 2",
    "section": "Visualising distributions of single variables",
    "text": "Visualising distributions of single variables\n\nCategorical variables\nBar charts are often used to visualise categorical variables with a finite set of values. Below is an example how to visuailse the distribution of drv - the type of drive train in the mpg dataset.\ngeom_bar transforms each value of drv into a count to be plotted on the y-axis. The x-axis presents the category associated with each count.\n\nggplot(mpg) +\n  geom_bar(aes(x = drv))\n\n\n\n\n\n\n\n\n\nCreate a bar chart that shows a count for the different vehicle classes in mpg.\nLook up different ggplot themes, and apply one to the bar chart you just created.\n\n\n\nContinuous variables\nHistograms are often used to visualise the distribution of a continuous variable. Below is an example of how to visualise the distribution of cty - the number of city miles per gallon in the mpg dataset.\ngeom_hist transforms the x-axis into equally spaced “bins” and the height of each bar on the y-axis is the number of observations falling in each bin.\n\nggplot(mpg) +\n  geom_histogram(aes(x = cty), \n                 binwidth = 3)\n\n\n\n\n\n\n\n\nYou can change the value of binwidth to adjust the width of the intervals. Different binwidths can reveal different patterns in the data.\n\nWhat happens when you change the value of binwidth to 1? Does the distribution change?\n\nDensity plots are an alternative to histograms for continuous data. Below is an example of the density of cty from the mpg data. The argument fill = \"darkseagreen\" just adds colour. You can also specify colour = \"&lt;COLOUR NAME&gt;\" to change the density line.\ngeom_density transforms the data into smoothed kernel density estimates (basically a smooth histogram). It is useful for continuous data that come from an underlying smooth distribution.\n\nggplot(mpg, aes(x = cty)) +\n  geom_density(fill = \"darkseagreen\") \n\n\n\n\n\n\n\n\nYou can also include raw data in a histogram in the form of “rug” marks.\n\nAdd rug marks to plot above by adding the argument + geom_rug(size = 1, colour = \"darkorange\")."
  },
  {
    "objectID": "labs/lab2.html#visualing-the-distributions-of-multiple-variables",
    "href": "labs/lab2.html#visualing-the-distributions-of-multiple-variables",
    "title": "Lab 2",
    "section": "Visualing the distributions of multiple variables",
    "text": "Visualing the distributions of multiple variables\n\nContinuous - Continuous\nScatterplots are often used to explore how two continuous variables covary together. Below is an example of how to visualise covariation between displ (engine displacement) and hwy (highway miles per gallon) in the mpg data. If there is a clear pattern in how the points fall on the x- and y-axes then we can say these variables covary.\ngeom_point plots values of displ on the y-axis and values of hwy on the x-axis.\n\nggplot(mpg) +\n  geom_point(aes(x = displ, \n                 y = hwy))\n\n\n\n\n\n\n\n\nSome of these values might be clustered together based on another characteristic. You can explore this by adding colour to the aesthetic mappings, for example.\n\nggplot(mpg) +\n  geom_point(aes(x = displ, \n                 y = hwy, \n                 colour = class))\n\n\n\n\n\n\n\n\n\nRepeat the plot above, but mapping the type of transmission to the colour aesthetic. Add a theme and change the titles of the x- and y-axes.\n\nDifferent aesthetic mappings can be combined in one plot. For example, you may want to add a line of best fit to a scatterplot plot, which you can do using geom_smooth().\n\nCreate a scatter plot showing the relationship between engine displacement and city miles, including a line of best fit.\n\nYou can fit more than one line in the same plot. This is often useful in multilevel modelling, where different hierarchies (e.g., groups, locations, individuals) have different slopes.\n\nRecreate the previous plot, but adding colour = class to the aesthetic mappings. Use method = \"lm\" and set se = FALSE in geom_smooth(). What difference does this make?\n\n\n\nContinuous - Categorical\nBoxplots are often used to explore the distribution of a continuous variable broken down by a categorical variable. Below is an example of hwy broken down by drv.\ngeom_boxplot transforms the data to show summary statistics that are represented by the different visual features of the boxplot.\n\nggplot(mpg) +\n  geom_boxplot(aes(x = drv, \n                   y = hwy))\n\n\n\n\n\n\n\n\nThese features of the boxplot are:\n\nLower horizontal line\nThicker horizontal middle line\nUpper horizontal line\nVertical whiskers\nPoints beyond the whiskers\n\n\nWhat do each of these features tell us?\nAdd + coord_flip() to the plot above. What does this do?\nWhat can you conclude about the highway miles per gallon of each type of drive train from the boxplot above?\n\nThe variable trans contains 10 different transmissions types. These 10 categories can be assumed under 2 broader categories: manual and auto.\n\nUse mutate() and fct_collapse() to collapse the 10 categories of trans to just these 2 categories.\nUse the previous example to create a boxplot mapping cty on the y-axis and drv on the x-axis, this time adding the argument colour = trans to aes(). What has changed?"
  },
  {
    "objectID": "labs/lab2.html#facets",
    "href": "labs/lab2.html#facets",
    "title": "Lab 2",
    "section": "Facets",
    "text": "Facets\nFacets are a way to split your plot into many subplots according to some categorical variable.\nTo do this, you use the command facet_wrap() to facet by a single variable. The first argument to facet_wrap() is a formula initiated by ~ and a variable name. Below is an example of how this can work on the mpg data.\n\nggplot(mpg) + \n  geom_point(aes(x = displ, \n                 y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n\nIt is possible to facet on a combination of two variables using facet_grid(). Like before, the first argument is a formula, but containing two variable names separated by ~. Below is an example of how this can work.\n\nggplot(mpg) + \n  geom_point(aes(x = displ, \n                 y = hwy)) + \n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n\n\nNotice that there are empty cells in the bottom left facets above. What do you think this means?\nCreate a scatter plot of displ and hwy, faceting by the of manufacturer name.\nChange the names on the axes to be more informative."
  },
  {
    "objectID": "labs/lab2.html#arranging-multiple-plots",
    "href": "labs/lab2.html#arranging-multiple-plots",
    "title": "Lab 2",
    "section": "Arranging multiple plots",
    "text": "Arranging multiple plots\nIf you have multiple plots that you want to arrange on the same page, you can use ggarrange() from the ggpubr package.\nThe first step is creating some plots. For this exercise we will only use the variables hwy, cty, and class.\n\nCreate the following three plots:\n\nA. A bar plot showing counts of class, assigning class to the fill aesthetic.\nB. A box plot of class by hwy per gallon, assigning class to the colour aesthetic.\nC. A jittered scatterplot of hwy and cty miles per gallon, assigning class to the colour aesthetic.\nRemember to save each plot as an object to be called upon later.\nD. Use ggarrange() to arrange the three plots in one space. ggarrange() takes the plots as arguments as well as ncol or nrow to customise the arrangement\nE. Repeat the code from the previous question, adding common.legend = TRUE and legend = \"bottom\" to ggarrange()."
  },
  {
    "objectID": "labs/lab2.html#loading-the-dataset",
    "href": "labs/lab2.html#loading-the-dataset",
    "title": "Lab 2",
    "section": "Loading the dataset",
    "text": "Loading the dataset\nIn the this practical, we will use the built-in data set iris. This data set contains the measurement of different iris species (flowers), you can find more information here.\n\nLoad the dataset and explain what variables are measured in the first three columns of your data set."
  },
  {
    "objectID": "labs/lab2.html#inspecting-the-dataset",
    "href": "labs/lab2.html#inspecting-the-dataset",
    "title": "Lab 2",
    "section": "Inspecting the dataset",
    "text": "Inspecting the dataset\nA good way of eyeballing on a relation between two continuous variables is by creating a scatterplot.\n\nPlot the sepal length and the petal width variables in a ggplot scatter plot (geom_points)\n\nA loess curve can be added to the plot to get a general idea of the relation between the two variables. You can add a loess curve to a ggplot with stat_smooth(method = \"loess\").\n\nAdd a loess curve to the plot under question 2, for further inspection.\n\nTo get a clearer idea of the general trend in the data (or of the relation), a regression line can be added to the plot. A regression line can be added in the same way as a loess curve, the method argument in the function needs to be altered to lm to do so.\n\nChange the loess curve of the previous plot to a regression line. Describe the relation that the line indicates."
  },
  {
    "objectID": "labs/lab2.html#simple-linear-regression",
    "href": "labs/lab2.html#simple-linear-regression",
    "title": "Lab 2",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nWith the lm() function, you can specify a linear regression model. You can save a model in an object and request summary statistics with the summary() command. The model is always specified with the code outcome_variable ~ predictor.\nWhen a model is stored in an object, you can ask for the coefficients with coefficients(). The next code block shows how you would specify a model where petal width is predicted by sepal width, and how summary statistics for this model would look like\n\n# Specify model: outcome = petal width, predictor = sepal width\niris_model1 &lt;- lm(Petal.Width ~ Sepal.Width,\n                  data = iris)\n\nsummary(iris_model1)\n\n\nCall:\nlm(formula = Petal.Width ~ Sepal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.38424 -0.60889 -0.03208  0.52691  1.64812 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.1569     0.4131   7.642 2.47e-12 ***\nSepal.Width  -0.6403     0.1338  -4.786 4.07e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7117 on 148 degrees of freedom\nMultiple R-squared:  0.134, Adjusted R-squared:  0.1282 \nF-statistic: 22.91 on 1 and 148 DF,  p-value: 4.073e-06\n\n\nThe summary of the model provides:\n\nThe model formula;\nEstimated coefficients (with standard errors and their significance tests);\nInformation on the residuals;\nA general test for the significance of the model (F-test);\nThe (adjusted) R squared as a metric for model performance.\n\nIndividual elements can be extracted by calling specific model elements (e.g. iris_model1$coefficients).\n\nSpecify a regression model where Sepal length is predicted by Petal width. Store this model as `model1. Supply summary statistics for this model.\nBased on the summary of the model, give a substantive interpretation of the regression coefficient.\nRelate the summary statistics and coefficients to the plots you made in questions 2 - 4."
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Lab 1",
    "section": "",
    "text": "From the Open Statistical Programming modules, complete the following:"
  },
  {
    "objectID": "labs/lab1.html#filtering-data",
    "href": "labs/lab1.html#filtering-data",
    "title": "Lab 1",
    "section": "Filtering data",
    "text": "Filtering data\nThe filter() function allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame.\nR provides the following options for filtering: &gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal). You can also combine these with the following logical operators: & meaning “and”, | meaning “or”, and ! meaning “not”.\n\nUse the filter() function to display only married people in the gss_cat data set.\nUse the filter() function to display divorced AND widowed people in the gss_cat data set."
  },
  {
    "objectID": "labs/lab1.html#arranging-data",
    "href": "labs/lab1.html#arranging-data",
    "title": "Lab 1",
    "section": "Arranging data",
    "text": "Arranging data\n\nUse the arrange() function to reorder the information in the data frame by the number of tv hours.\nYou can combine arrange() with functions like desc() to re-order a column in descending order. Try doing this.\nHow would you filter only married people and arrange them by how much tv they watch?\n\nHint: You need to combine filter and arrange using the %&gt;%\n\nHow would you use arrange() and count() to find what the most common religion is?"
  },
  {
    "objectID": "labs/lab1.html#summarizing-data",
    "href": "labs/lab1.html#summarizing-data",
    "title": "Lab 1",
    "section": "Summarizing data",
    "text": "Summarizing data\n\nHow many hours of tv on average are watched by people of different religions?\n\nHint: select(), group_by(), and summarize() are useful functions for this"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ADS fundamentals",
    "section": "",
    "text": "Welcome to the course “Fundamental Techniques in Data Science with R”."
  },
  {
    "objectID": "index.html#important-links",
    "href": "index.html#important-links",
    "title": "ADS fundamentals",
    "section": "Important links",
    "text": "Important links\n\nCoordinator: Dr L. Boeschoten\nLecturer: Dr K.M. Lang\nInstructors: Dr A. Giachanou and T.C. Carrière, MSc\nRooms: See MyTimetable\nCourse manual\nInstructions group assignments\nLink to upload group assignments\nExam information"
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "ADS fundamentals",
    "section": "Weekly schedule",
    "text": "Weekly schedule\n\n\n\nWeek\nDate\nContent\nPrepare\n\n\n1\n10-11 (Mon)\nLecture\n\nRead course manual\nCheck preparation\nRead R4DS Ch. 3, 4, 7, 13, 16\n\n\n\n1\n11-11 (Tue)\nQ&A - online\n\n\n\n1\n13-11 (Thu)\nWG\nBe present to pass the course!!\n\nSlides\n\n\n\n2\n17-11 (Mon)\nDeadline lab 1: 15:00\n\nLab 1 (solutions)\n\n\n\n2\n17-11 (Mon)\nLecture\n\nRead R4DS Ch. 1, 9, 10\nRead SDAM Ch. 4\n\n\n\n2\n18-11 (Tue)\nQ&A - online\n\n\n\n2\n20-11 (Thu)\nWG\n\n\n\n3\n24-11 (Mon)\nDeadline lab 2: 15:00\n\nLab 2 (solutions)\n\n\n\n3\n24-11 (Mon)\nLecture\n\nRead SDAM Ch. 5.1 - 5.6, 6.1\n\n\n\n3\n25-11 (Tue)\nQ&A - online\n\n\n\n3\n27-11 (Thu)\nWG\n\n\n\n4\n1-12 (Mon)\nDeadline lab 3: 15:00\n\nLab 3 (solutions)\ndemo\n\n\n\n4\n1-12 (Mon)\nLecture\n\nRead SDAM Ch. 5.7 - 5.9\nRead this and this\n\n\n\n4\n2-12 (Tue)\nQ&A - online\n\n\n\n4\n4-12 (Thu)\nWG\n\n\n\n5\n8-12 (Mon)\nDeadline lab 4: 15:00\n\nLab 4 (solutions)\n\n\n\n5\n8-12 (Mon)\nDeadline assignment 1: 15:00\n\n\n\n5\n8-12 (Mon)\nLecture\n\nRead SDAM Ch. 12.1 - 12.5, 12.8\nRead this\n\n\n\n5\n9-12 (Tue)\nQ&A - online\n\n\n\n5\n11-12 (Thu)\nWG\n\n\n\n6\n15-12 (Mon)\nDeadline lab 5: 15:00\n\nLab 5 (solutions)\n\n\n\n6\n15-12 (Mon)\nLecture\n\nRead this\nRead Book420 CH. 17.4\n\n\n\n6\n16-12 (Tue)\nQ&A - online\n\n\n\n6\n18-12 (Thu)\nWG\n\n\n\n7\n12-1 (Mon)\nDeadline lab 6: 15:00\n\nLab 6 (solutions)\n\n\n\n7\n12-1 (Mon)\nRecap\n\n\n\n7\n15-1 (Thu)\nPresentations assignment 2\nBe present from 09:00 to 12:45 to pass the course!!\n\n\n8\n22-1 (Thu)\nExam"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Both assignments are group assignments. We will form groups of 4 at the first workgroup meeting, and you work on both assignments with the same group throughout the rest of the course.\nIn order to be assigned to a group, presence at the first meeting is crucial!\nYou can use the same dataset for both assigments. The minimum requirement for the dataset is that it contains:"
  },
  {
    "objectID": "assignments.html#assignment-1-linear-regression",
    "href": "assignments.html#assignment-1-linear-regression",
    "title": "Assignments",
    "section": "Assignment 1: Linear regression",
    "text": "Assignment 1: Linear regression\n\nType of assignment: Written report in Quarto.\nGrading: 25% of your final grade\nDeadline: Monday December 8, 15:00\nWhat to submit: A ZIP archive containing the complete R project (dataset, .qmd, HTML)\nWhere to submit: Here\nWhat is graded: Only your .html file is graded, so make sure all code, comments and output are visible! Carefully check if the compiled .html file in your ZIP archive is correct and complete before submitting!\nDescription: For this assignment, you perform and report a multiple linear regression analysis in an Quarto text document. The assignment will be graded on the following six dimensions:\n\n\nPreliminaries: Introduction of your research questions, description and potential processing of your data.\nModel estimation: Description of the model estimates, model fit, and model comparison procedure (including moderation)\nAssumptions: Testing of model assumptions, checking for influential cases. Act upon and/or reflect on violations when needed.\nInterpretation: Substantive interpretation of the final model. Answering your research question.\nLayout: Structure of the document, efficiency of output presentation, use of custom functions (when applicable). Presentation of suitable visualizations.\nCoding: Use code from the labs and annotate what each piece of code does."
  },
  {
    "objectID": "assignments.html#assignment-2-logistic-regression",
    "href": "assignments.html#assignment-2-logistic-regression",
    "title": "Assignments",
    "section": "Assignment 2: Logistic regression",
    "text": "Assignment 2: Logistic regression\nGroup assignment 2: Logistic regression\n\nType of assignment: Presentation and discussion with slides in Quarto\nGrading: 25% of your final grade\nPresentation: WG meeting Thursday January 15, from 09:00 to 12:45. Prepare a presentation of 15 min. and expect 5 min. of questions from students and teachers.\nDeadline: Wednesday January 14, 15:00. Hand in your ZIP archive including your slides. Your WG instructor will set the slides up for the presentations on Thursday. Carefully check if the compiled .html file in your ZIP archive is correct and complete before submitting!\nWhat to submit: A ZIP archive containing the complete R project (dataset, .qmd, slides)\nWhere to submit: Here\nDescription: For this assignment, you perform and report a multiple logistic regression analysis in a Quarto presentation. The presentation will be graded on the following six dimensions:\n\n\nPreliminaries: Introduction of your research questions, description and potential processing of your data.\nModel estimation: Description of the model estimates, model fit, and model comparison procedure (including moderation).\nAssumptions: Testing of model assumptions, checking for influential cases. Act upon and/or reflect on violations when needed.\nInterpretation: Substantive interpretation of the final model (including the confusion matrix). Answering your research question.\nLayout: Structure of the document, efficiency of output presentation, use of custom functions (when applicable). Presentation of suitable visualizations.\nCoding: Use code from the labs and annotate what each piece of code does."
  },
  {
    "objectID": "exam.html",
    "href": "exam.html",
    "title": "Exam",
    "section": "",
    "text": "Anything mentioned in the lectures may appear on the exam.\nThis includes both information printed in the lecture slides and information delivered verbally during a lecture itself.Anything covered in the required readings may appear on the exam.\nObviously, this is a lot of material. To prioritize, keep in mind that topics mentioned in the lectures and topics directly related to ideas covered in the lectures are most likely to appear on the exam.\nThe materials presented on the “preparation” page can also be part of the exam since this is required knowledge.\nSummary of the required readings:\n\nR4DS: Ch. 1, 3, 4, 7, 9, 10, 13, 16.\nSDAM: Ch. 1, 2, 3, 4, 5, 6.1, 12.1-12.5, 12.8.\nBook420: Ch.17.4.\nhttps://stats.oarc.ucla.edu/r/dae/robust-regression/\nhttps://stats.oarc.ucla.edu/r/dae/robust-regression/\nhttps://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/\nhttps://stats.oarc.ucla.edu/r/dae/logit-regression/"
  },
  {
    "objectID": "exam.html#what-will-be-tested",
    "href": "exam.html#what-will-be-tested",
    "title": "Exam",
    "section": "",
    "text": "Anything mentioned in the lectures may appear on the exam.\nThis includes both information printed in the lecture slides and information delivered verbally during a lecture itself.Anything covered in the required readings may appear on the exam.\nObviously, this is a lot of material. To prioritize, keep in mind that topics mentioned in the lectures and topics directly related to ideas covered in the lectures are most likely to appear on the exam.\nThe materials presented on the “preparation” page can also be part of the exam since this is required knowledge.\nSummary of the required readings:\n\nR4DS: Ch. 1, 3, 4, 7, 9, 10, 13, 16.\nSDAM: Ch. 1, 2, 3, 4, 5, 6.1, 12.1-12.5, 12.8.\nBook420: Ch.17.4.\nhttps://stats.oarc.ucla.edu/r/dae/robust-regression/\nhttps://stats.oarc.ucla.edu/r/dae/robust-regression/\nhttps://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/\nhttps://stats.oarc.ucla.edu/r/dae/logit-regression/"
  },
  {
    "objectID": "exam.html#what-about-equations",
    "href": "exam.html#what-about-equations",
    "title": "Exam",
    "section": "What about equations?",
    "text": "What about equations?\nThis is not a math class; we are not trying to test your ability to do calculations or manipulate equations. That being said, a certain degree of mathematical literacy is crucial to statistics and data science, so you will have to do some simple calculations on the exam. For example, you should be comfortable with the following.\n\nWorking with the linear regression equation, \\(Y_i = \\beta_0 + \\beta_1 X_i +\\varepsilon_i\\), to:\n\nCalculate predicted values give certain inputs\nInterpret parameter estimates\nEvaluate hypotheses/research questions\n\nThe differences between:\n\nThe full regression model: \\(Y_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i +\\hat{\\varepsilon}_i\\)\nThe equation for the best-fit line: \\(\\hat{Y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_i\\)\n\nThe definition of a residual:\n\n\\(\\hat{\\varepsilon}_i = Y_i - \\hat{Y}_i\\)\n\nThe relationship between probabilities (\\(p\\)) and odds (for binary outcomes):\n\n\\(\\text{odds} = \\frac{p}{1-p}\\)\n\nThe definition of the logit function:\n\n\\(\\ln(\\text{odds}) = \\ln\\left(\\frac{p}{1-p}\\right) = \\text{logit}(p)\\)\n\nThe definition of the logistic function, its relation to the logit function, and its role in logistic regression:\n\n\\(\\text{logistic}(\\eta_i) = \\text{logit}^{-1}(p_i) = \\frac{\\exp(\\eta_i)}{1+\\exp(\\eta_i)} = \\frac{\\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i)}{1 + \\exp(\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i)} = \\hat{p}_i\\)\n\n\nOf course, you should also be able to do basic arithmetic operations that are too trivial to detail here (e.g., calculating the difference between the \\(R^2\\) statistics from two models that you are trying to compare).\nNote: Although all examples above are shown in terms of simple linear regression models, you should also be able to do these calculations/interpretations using multiple linear regression models and models that include dummy codes and interactions."
  },
  {
    "objectID": "exam.html#qa-and-practice-exam",
    "href": "exam.html#qa-and-practice-exam",
    "title": "Exam",
    "section": "Q&A and practice exam",
    "text": "Q&A and practice exam\n\nThe lecture on Monday January 12 will be completely devoted to a recap of the material and there will be plenty of time to answer all your questions. Send your questions in advance to Dr. Lang if you want to make sure you get an extensive answer!\nYou can find the practice exam here."
  },
  {
    "objectID": "exam.html#what-can-i-expect-regarding-the-exam",
    "href": "exam.html#what-can-i-expect-regarding-the-exam",
    "title": "Exam",
    "section": "What can I expect regarding the exam?",
    "text": "What can I expect regarding the exam?\nThe exam takes place on January 22. Keep an eye out on MyTimetable for the timing, location. These can change very last-minute!\nThe exam takes place on laptops that are provided in the room, and we use the software program Remindo. Please read the instructions on how to use Remindo carefully. You can also practice making an exam in Remindo. Remindo has a small built-in calculator, so you don’t need to bring one.\nRegulations during the exam:\n\nTurn of your phone and smartwatch and stow them away. You cannot use them in any way during the exam.\nYou are not allowed to use any teaching materials or dictionaries during the exam.\nClose your bag and do not store it in the walkway.\nYou cannot leave in the first 30 minutes.\nYou are not allowed to visit the bathroom during the exam.\nMake sure your ID is visible on your table during the exam.\nIf you are more than 30 minutes late to the exam, you are not allowed to participate.\n\nWhat to bring:\n\nIdentification card.\nSolid ID and password for logging in to Remindo."
  },
  {
    "objectID": "exam.html#testing-procedure",
    "href": "exam.html#testing-procedure",
    "title": "Exam",
    "section": "Testing procedure",
    "text": "Testing procedure\nThere are two opportunities per year to take a test.\n\nShould circumstances prevent you from taking the first opportunity, you will be invited to the second. It is not necessary to deregister.\nIf you do not obtain a satisfactory final grade on the second opportunity, you will have to retake the course in the 2026-2027 academic year. See detailed information on testing on the student website.\nbut you have obtained a pass for one part (e.g. an assignment or the exam), then you do not have to redo this part next year."
  },
  {
    "objectID": "labs/lab4_answers.html",
    "href": "labs/lab4_answers.html",
    "title": "Lab 4",
    "section": "",
    "text": "In this practical, the assumptions of the linear regression model will be discussed. You will practice with checking the different assumptions, and practice with accounting for some of the assumptions with additional steps. Please note that these assumptions can only be checked once you have selected a (final) model, as these assumptions ‘need’ a model that they apply to.\nWe will use the following packages in this practical:\n\nlibrary(magrittr)\nlibrary(ggplot2) \nlibrary(regclass)\nlibrary(MASS)"
  },
  {
    "objectID": "labs/lab4_answers.html#linearity",
    "href": "labs/lab4_answers.html#linearity",
    "title": "Lab 4",
    "section": "Linearity",
    "text": "Linearity\nWith the assumption of linearity, it is assumed that the relation between the dependent and independent variables is (more or less) linear. You can check this by generating a scatterplot using a predictor variable and outcome variable of the regression model.\n\nCheck whether there is a linear relation between the variables vertical length and the cross length.\n\n\nggplot(data_fish, \n       aes(x = vertical_length, y = cross_length)) +\n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ggtitle(\"linear relation is present\") +\n  xlab(\"vertical length in cm\") +\n  ylab(\"cross length in cm\")\n\n\n\n\n\n\n\n\n\nNext check the relation between weight and height.\n\n\nggplot(data_fish, aes(x = weigth, y = height)) + \n  geom_point() +\n  geom_smooth(se = FALSE) +\n  ggtitle(\"linear relation is missing\") +\n  xlab(\"Weigth in gram\") +\n  ylab(\"heigth in cm\")\n\n\n\n\n\n\n\n\n\nDescribe both plots. What differences do you see?\n\n\n# The first plot shows a case where there is a more or less linear relation (Vertical length of the fish and cross length of the fish). In the second plot, the relation is clearly not linear.\n\nWhen a non-linear relation is present, you can either choose another model to use, or transform the predictor before adding it to the model, for example using a log-transformation. Applying a transformation, however, will not always solve the problem, and makes interpretation of the model less intuitive.\n\nApply a log-transformation to the weight variable.\n\n\ndata_fish$weigth_trans &lt;- data_fish$weigth %&gt;% \n  log()\n\n\nPlot the relation between length and weight again, but now including the transformed variable.\n\n\nggplot(data_fish, aes(x = weigth_trans, y = height)) + \n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  ggtitle(\"linear relation improved\") +\n  xlab(\"Weigth in gram\") +\n  ylab(\"heigth in cm\")\n\n\n\n\n\n\n\n\n\nDescribe if the transformation improved the linear relation.\n\n\n# You see that the relation is still not completely linear, but a lot more linear than before the transformation (plot 2)."
  },
  {
    "objectID": "labs/lab4_answers.html#predictor-matrix-full-rank",
    "href": "labs/lab4_answers.html#predictor-matrix-full-rank",
    "title": "Lab 4",
    "section": "Predictor matrix full rank",
    "text": "Predictor matrix full rank\nThis assumption states that:\n\nthere need to be more observations than predictors (n &gt; P).\n\nno predictor can be a linear combination of other predictors; predictors cannot have a very high correlation (multicollinearity).\n\n\nThe first part of this assumption is easy to check: see if the number of observations minus the number of predictors is a positive number. The second part can be checked by either obtaining correlations between the predictors, or by determining the VIF (variance inflation factor). When the VIF is above 10, this indicate high multicollinearity. To account for this, predictors can be excluded from the model, or a new variable can be constructed based on predictors with a high correlation.\nTo examine VIF scores, the function VIF() from the regclass can be used on a prespecified model. If this model includes a categorical variable with multiple categories, such as ‘species’ in the example data, the generalized VIF is used, and we have to look at the third column (GVIF^*(1/(2*Df))), these values can be compared to normal VIF values.\n\nSpecify a linear model with weight as outcome variable using all other variables in the dataset as predictors. Save this model as model_fish1. Calculate VIF values for this model.\n\n\nmodel_fish1 &lt;- lm(weigth ~., \n                  data = data_fish[,1:7])\n\nmodel_fish1 %&gt;%\n  VIF()\n\n                      GVIF Df GVIF^(1/(2*Df))\nspecies         1509.77571  6        1.840388\nvertical_length 2360.42508  1       48.584206\ndiagonal_length 4307.91811  1       65.634732\ncross_length    2076.93715  1       45.573426\nheight            56.20370  1        7.496913\ndiagonal_width    29.16651  1        5.400602\n\n\n\nCheck the VIF scores. If VIF scores exceed a score of 10, give substantial explanation why the VIF scores are this high.\n\n\n# The VIF values in this first model are extreme in some cases, more specifically for the three variables that all measure some aspect of length, it makes sense that these values can be highly correlated. One way to solve this, is excluding predictors that almost measure the same thing as another variable.\n\n\nWhat adjustments can be made to the model to account for multicollinearity in this case?\n\n\n# A straightforward solution is to include only one of the variables measuring an aspect of length. More elaborate solutions exist but are not covered in this course.\n\n\nRun a new model which only includes one of the three length variables and call it model_fish2. Describe if there is an improvement.\n\n\nmodel_fish2 &lt;- lm(weigth ~ species + diagonal_length + height + diagonal_width, \n             data = data_fish)\n\nmodel_fish2 %&gt;%\n  VIF()\n\n                     GVIF Df GVIF^(1/(2*Df))\nspecies         218.36805  6        1.566507\ndiagonal_length  28.17950  1        5.308437\nheight           54.91084  1        7.410185\ndiagonal_width   28.52222  1        5.340620\n\n# We chose to go with a model which only includes `diagonal_length`, as this variable had the highest VIF value and therefore is able to best grasp the variance that is also measured by the other two length variables. However which strategy is most appropriate can differ per situation. We see now that the VIF values have returned to 'normal' values (although still a bit high).\n\n\nWhat happens with the regression model when there are more predictors than observations?\n\n\n# If there are more predictors than observations, the model can not be identified and the parameters cannot be estimated"
  },
  {
    "objectID": "labs/lab4_answers.html#exogenous-predictors",
    "href": "labs/lab4_answers.html#exogenous-predictors",
    "title": "Lab 4",
    "section": "Exogenous predictors",
    "text": "Exogenous predictors\nFor this assumption, the expected value of the errors (mean of the errors) must be 0. Furthermore, The errors must be independent of the predictors.\n\nWhat is the possible consequence of not meeting this assumption?\n\n\n# Not meeting this assumption results in biased estimates for the regression coefficients."
  },
  {
    "objectID": "labs/lab4_answers.html#constant-finite-error-variance",
    "href": "labs/lab4_answers.html#constant-finite-error-variance",
    "title": "Lab 4",
    "section": "Constant, finite error variance",
    "text": "Constant, finite error variance\nThis assumptions is also called ‘the assumption of homoscedasticity’. It states that the variance of the error terms should be constant over all levels of the predictors. This can be checked by plotting the residuals against the fitted values. These plots can be obtained by simply taking the first plot of a specified model, plot(model_x).\n\nCreate a residual vs fitted values plot for model_fish1, which is the first plot generated by the plot() function.\n\n\nmodel_fish1 %&gt;%\n  plot(1)\n\n\n\n\n\n\n\n\n\nLoad in the iris data, and specify a model where sepal length is predicted by all other variables and save this as model_iris1.\n\n\ndata_iris   &lt;- iris\nmodel_iris1 &lt;- lm(Sepal.Length ~ ., \n                  data = data_iris)\n\n\nCreate a residual vs fitted plot for this model as well.\n\n\nmodel_iris1 %&gt;%\n  plot(1)\n\n\n\n\n\n\n\n\n\nDiscuss both plots and indicate whether the assumption is met.\n\n\n# In the `iris_data` plot, it can be seen that the red line is quite constant. Also, the dots seem to have a rather constant variance. In the `fish_data` plot, however, the variance in error terms seems smaller for the lower values than for the higher values. This second plot indicates heteroscedasticity and indicates that the assumption is violated.\n\n\nDiscuss what the consequence would be if this assumption is violated.\n\n\n# If this assumption is violated, estimated standard errors are biased."
  },
  {
    "objectID": "labs/lab4_answers.html#independent-errors",
    "href": "labs/lab4_answers.html#independent-errors",
    "title": "Lab 4",
    "section": "Independent errors",
    "text": "Independent errors\nThis assumption states that error terms should have no correlation. Dependence of the errors can result from multiple things. First, there is a possible dependence in the error terms when there is serial dependence, for example because the data contains variables that are measured over time. Another reason can be when there is a cluster structure in the data, for example students in classes in schools.\n\nHow can both causes of correlated error terms be detected, and what can be done to solve the problem?\n\n\n# Temporal dependence can be checked by investigating the autocorrelation, while clustered data can be found by investigating the intra class correlation (ICC).\n\n# More important: dealing with these dependencies requires another model (multilevel for clustered data, or a model that account for the time aspect). Those models are out of the scope of this course, but always be aware of a theoretical dependency between your errors."
  },
  {
    "objectID": "labs/lab4_answers.html#normally-distributed-errors",
    "href": "labs/lab4_answers.html#normally-distributed-errors",
    "title": "Lab 4",
    "section": "Normally distributed errors",
    "text": "Normally distributed errors\nThis assumption states that errors should be roughly normally distributed. Like the assumption of homoscedasticity, this can be checked by model plots, provided by R.\n\nCreate a QQ plot for model_iris1, which is the second plot generated by the plot() function. Indicate whether the assumption is met.\n\n\nmodel_iris1 %&gt;%\n  plot(2)\n\n\n\n\n\n\n\n\n\nCreate a new model using the fish data, where diagonal_width is predicted by cross_length, and store the model as model_fish3.\n\n\nmodel_fish3 &lt;- lm(diagonal_width ~ cross_length, \n                  data = data_fish)\n\n\nCreate a QQ plot for model_fish3.\n\n\nmodel_fish3 %&gt;%\n  plot(2)\n\n\n\n\n\n\n\n\n\nInterpret the two plots. Is the assumption met in both cases?\n\n\n# In the two plots above, QQ plots are provided for the 2 models. For the first model, the error terms follow the ideal line pretty well, and the assumption holds. In the second plot, the tails deviate quite a lot from the intended line, and it can be debated that the assumption is violated.\n\n\nIn what cases is it problematic that the assumption is not met? And in what cases is it no problem?\n\n\n# The assumption is important in smaller samples (n &lt; 30). In bigger samples, violating the assumption is less of a big problem. For prediction intervals however, normality of errors is always wanted."
  },
  {
    "objectID": "labs/lab4_answers.html#outliers",
    "href": "labs/lab4_answers.html#outliers",
    "title": "Lab 4",
    "section": "Outliers",
    "text": "Outliers\nOutliers are observations that show extreme outcomes compared to the other data, or observations with outcome values that fit the model very badly. Outliers can be detected by inspecting the externally studentized residuals.\n\nMake a plot of studentized residuals by using the functions rstudent and plot for `model_fish1. What do you conclude?\n\n\nmodel_fish1 %&gt;%\n  rstudent() %&gt;%\n  plot()\n\n\n\n\n\n\n\n# There is at least one clear outlier around observation number 70.\n\n\nMake a plot of studentized residuals for model_iris1.\n\n\nmodel_iris1 %&gt;%\n  rstudent() %&gt;%\n  plot()\n\n\n\n\n\n\n\n\n\nStore the dataset Animals from the MASS package. Define a regression model where animals’ body weight is predicted by brain weight and store it as model_animals1.\n\n\ndata_animals   &lt;- Animals\nmodel_animals1 &lt;- lm(body ~ brain,\n                     data = data_animals)\n\n# There are not really any clear outliers to worry about.\n\n\nMake a plot of the studentized residuals for model_animals1.\n\n\nmodel_animals1 %&gt;%\n  rstudent() %&gt;%\n  plot()\n\n\n\n\n\n\n\n# Observation number 26 is an extreme outlier."
  },
  {
    "objectID": "labs/lab4_answers.html#high-leverage-observations",
    "href": "labs/lab4_answers.html#high-leverage-observations",
    "title": "Lab 4",
    "section": "High-leverage observations",
    "text": "High-leverage observations\nHigh-leverage observations are observations with extreme predictor values. To detect these observations, we look at their leverage values. These values can be summarized in a leverage plot.\n\nFor the model specified under model_animals1, create a leverage plot by plotting the hatvalues() of the model.\n\n\nmodel_animals1 %&gt;%\n  hatvalues() %&gt;%\n  plot()\n\n\n\n\n\n\n\n# In the leverage plot, observation 7 and 15 stand out from the other observations. When you look at the data set, you can notice that both of these observations are elephant species.\n\n# A case with high leverage is not necessarily bad: the influence on the model is more important."
  },
  {
    "objectID": "labs/lab4_answers.html#influence-on-the-model",
    "href": "labs/lab4_answers.html#influence-on-the-model",
    "title": "Lab 4",
    "section": "Influence on the model",
    "text": "Influence on the model\nBoth outliers and observations with high leverage are not necessarily a problem. Cases that are both, however, seem to form more of a problem. These cases can influence the model heavily and can therefore be problematic.\nInfluence measures come in two sorts: Cook’s distance checks for influential observations, while DFBETAS check for influential, and possible problematic, observations per regression coefficients.\n\nFor model_animals1, check Cooks distance by plotting the cooks.distance of the model.\n\n\nmodel_animals1 %&gt;%\n  cooks.distance() %&gt;%\n  plot()\n\n\n\n\n\n\n\n\n\nFor model_animals1, check the DFBETAS by using the function dfbetas.\n\n\nplot(dfbetas(model_animals1)[,1],\n     main = \"intercept\")\n\n\n\n\n\n\n\nplot(dfbetas(model_animals1)[,2],\n     main = \"slope\")\n\n\n\n\n\n\n\n# Note that because of the structure of the output of `dfbetas` it is not very convenient to process it using a pipe structure.\n\n\nDescribe what you see in the plots for Cook’s distance and DFBETAS. What do you conclude?\n\n\n# Case 26, the earlier spotted outlier, has in all three plots an outstanding value. There is reason to assume that this observation is problematic.\n\n\nDelete the problematic observation that you found in Question 12 and store the dataset under a new name.\n\n\ndata_animals2 &lt;- data_animals[-26,]\n\n\nFit the regression model where animals’ body weight is predicted by brain weight using the adjusted dataset and store it as model_animals2.\n\n\nmodel_animals2 &lt;- lm(body ~ brain, \n                     data = data_animals2)\n\n\nCompare the output to model_animals1 and describe the changes.\n\n\nsummary(model_animals1)\n\n\nCall:\nlm(formula = body ~ brain, data = data_animals)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -4316  -4312  -4242  -3806  82694 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 4316.32258 3465.24131   1.246    0.224\nbrain         -0.06594    2.42114  -0.027    0.978\n\nResidual standard error: 16790 on 26 degrees of freedom\nMultiple R-squared:  2.853e-05, Adjusted R-squared:  -0.03843 \nF-statistic: 0.0007417 on 1 and 26 DF,  p-value: 0.9785\n\nsummary(model_animals2)\n\n\nCall:\nlm(formula = body ~ brain, data = data_animals2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1653.1  -876.8  -814.4  -778.9 10855.6 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept) 810.1594   616.6065   1.314    0.201\nbrain         0.6855     0.4231   1.620    0.118\n\nResidual standard error: 2930 on 25 degrees of freedom\nMultiple R-squared:  0.09501,   Adjusted R-squared:  0.05881 \nF-statistic: 2.625 on 1 and 25 DF,  p-value: 0.1178\n\n# We see that the model changes quite a bit: the intercept becomes much lower, and the slope even changes direction (negative to positive).\n\n\nRun the plots for influential observations again on this new model and see if anything changes.\n\n\nmodel_animals2 %&gt;%\n  cooks.distance() %&gt;%\n  plot()\n\n\n\n\n\n\n\nplot(dfbetas(model_animals2)[,1],\n     main = \"intercept\")\n\n\n\n\n\n\n\nplot(dfbetas(model_animals2)[,2],\n     main = \"slope\")\n\n\n\n\n\n\n\n# We see that new influential observations arise. These were earlier overshadowed by observation 26. If you look at these cases, you see these are the cases with very heavy animals. In this case the solution should be to transform the data and take the log of the weights, instead of these values. This means that the assumption of linearity was probably not met for this data set.*"
  },
  {
    "objectID": "labs/lab1_answers.html",
    "href": "labs/lab1_answers.html",
    "title": "Lab 1",
    "section": "",
    "text": "From the Open Statistical Programming modules, complete the following:"
  },
  {
    "objectID": "labs/lab1_answers.html#filtering-data",
    "href": "labs/lab1_answers.html#filtering-data",
    "title": "Lab 1",
    "section": "Filtering data",
    "text": "Filtering data\nThe filter() function allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame.\nR provides the following options for filtering: &gt;, &gt;=, &lt;, &lt;=, != (not equal), and == (equal). You can also combine these with the following logical operators: & meaning “and”, | meaning “or”, and ! meaning “not”.\n\nUse the filter() function to display only married people in the gss_cat data set.\n\n\ngss_cat %&gt;% \n  filter(marital == \"Married\")\n\n# A tibble: 10,117 × 9\n    year marital   age race  rincome        partyid          relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;            &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Married    25 White $20000 - 24999 Strong democrat  Prot… Sout…      NA\n 2  2000 Married    44 White $25000 or more Not str democrat Prot… Other       0\n 3  2000 Married    47 White $25000 or more Strong republic… Prot… Sout…       3\n 4  2000 Married    53 White $25000 or more Not str democrat Prot… Other       2\n 5  2000 Married    52 White $25000 or more Ind,near rep     None  Not …      NA\n 6  2000 Married    52 White $25000 or more Strong democrat  Prot… Sout…       1\n 7  2000 Married    51 White $25000 or more Strong republic… Prot… Unit…      NA\n 8  2000 Married    40 Black $25000 or more Strong democrat  Prot… Bapt…       7\n 9  2000 Married    40 White $10000 - 14999 Not str democrat Cath… Not …       3\n10  2000 Married    45 Black Not applicable Independent      Prot… Unit…      NA\n# ℹ 10,107 more rows\n\n# Since we only want to find married people we will use the equal operator, ==, and encase the observations we want in quotes.*\n\n\nUse the filter() function to display divorced AND widowed people in the gss_cat data set. ::: {.cell}\n\ngss_cat %&gt;% \n  filter(marital == \"Divorced\" | marital == \"Widowed\")\n\n# A tibble: 5,190 × 9\n    year marital    age race  rincome        partyid         relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;    &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;           &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Divorced    48 White $8000 to 9999  Not str republ… Prot… Bapt…      NA\n 2  2000 Widowed     67 White Not applicable Independent     Prot… No d…       2\n 3  2000 Divorced    25 White Not applicable Not str democr… None  Not …       1\n 4  2000 Divorced    44 White $7000 to 7999  Ind,near dem    Prot… Luth…      NA\n 5  2000 Divorced    52 White $25000 or more Ind,near dem    None  Not …       1\n 6  2000 Widowed     77 White Not applicable Strong republi… Jewi… Not …      NA\n 7  2000 Widowed     54 White $25000 or more Ind,near rep    Chri… Not …       1\n 8  2000 Widowed     82 White Not applicable Not str democr… Prot… Other       3\n 9  2000 Widowed     83 White Not applicable Strong democrat Prot… Epis…      NA\n10  2000 Widowed     89 White Not applicable Not str democr… Prot… Othe…       4\n# ℹ 5,180 more rows\n\n# In this case we need to combine logical operators. The or operator is appropriate here since we are looking for two kinds of matches in the same variable (& would not work since people cannot be both divorced and widowed). We combine this with the equal operator.\n:::"
  },
  {
    "objectID": "labs/lab1_answers.html#arranging-data",
    "href": "labs/lab1_answers.html#arranging-data",
    "title": "Lab 1",
    "section": "Arranging data",
    "text": "Arranging data\n\nUse the arrange() function to reorder the information in the data frame by the number of tv hours.\n\n\ngss_cat %&gt;% \n  arrange(tvhours)\n\n# A tibble: 21,483 × 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Married          44 White $25000 or more Not str d… Prot… Other       0\n 2  2000 Never married    23 White $1000 to 2999  Strong de… Prot… Other       0\n 3  2000 Never married    32 White $25000 or more Not str r… Cath… Not …       0\n 4  2000 Divorced         39 White Refused        Independe… None  Not …       0\n 5  2000 Separated        57 Black Not applicable Ind,near … None  Not …       0\n 6  2000 Widowed          78 White Not applicable Not str r… Prot… Unit…       0\n 7  2000 Married          45 Other Refused        Independe… None  Not …       0\n 8  2000 Never married    28 White $25000 or more Other par… Prot… No d…       0\n 9  2000 Widowed          65 White $6000 to 6999  Strong de… Cath… Not …       0\n10  2000 Never married    34 Black $10000 - 14999 Not str d… None  Not …       0\n# ℹ 21,473 more rows\n\n# Arrange only needs one argument, the variable you wish to reorder. Arrange orders the rows of a gss_cat by the tvhours column. The default here is to arrange in ascending order.\n\n\nYou can combine arrange() with functions like desc() to re-order a column in descending order. Try doing this.\n\n\ngss_cat %&gt;%\n  arrange(desc(tvhours))\n\n# A tibble: 21,483 × 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Never married    30 Black Not applicable Independe… Prot… Bapt…      24\n 2  2000 Separated        45 Black Not applicable Ind,near … Prot… Other      24\n 3  2002 Never married    33 White $6000 to 6999  Independe… Cath… Not …      24\n 4  2006 Divorced         53 Black Not applicable Strong de… Prot… Sout…      24\n 5  2008 Divorced         50 Black No answer      Ind,near … Prot… Bapt…      24\n 6  2008 Never married    44 White Not applicable Independe… Prot… Other      24\n 7  2008 Never married    21 White Don't know     Independe… Cath… Not …      24\n 8  2008 Widowed          71 White Not applicable Strong de… Prot… Don'…      24\n 9  2010 Widowed          62 Black Not applicable Strong de… Prot… Am b…      24\n10  2010 Widowed          52 Black Refused        Strong de… Prot… Bapt…      24\n# ℹ 21,473 more rows\n\n# When combining these functions you need to wrap arrange around the operation you want to do.\n\n\nHow would you filter only married people and arrange them by how much tv they watch?\n\nHint: You need to combine filter and arrange using the %&gt;%\n\ngss_cat %&gt;% \n  filter(marital == \"Married\") %&gt;% \n  arrange(tvhours)\n\n# A tibble: 10,117 × 9\n    year marital   age race  rincome        partyid          relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;   &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;            &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Married    44 White $25000 or more Not str democrat Prot… Other       0\n 2  2000 Married    45 Other Refused        Independent      None  Not …       0\n 3  2000 Married    35 White Refused        Strong republic… Prot… Other       0\n 4  2000 Married    38 White $20000 - 24999 Ind,near dem     Cath… Not …       0\n 5  2000 Married    25 White $15000 - 19999 Strong republic… Prot… No d…       0\n 6  2000 Married    48 White $25000 or more Ind,near dem     Prot… Other       0\n 7  2000 Married    37 White $25000 or more Not str democrat Prot… Unit…       0\n 8  2000 Married    39 White Lt $1000       Not str democrat Prot… Othe…       0\n 9  2000 Married    49 White $20000 - 24999 Independent      Prot… No d…       0\n10  2000 Married    33 White $1000 to 2999  Not str democrat Prot… Other       0\n# ℹ 10,107 more rows\n\n# Using the pipe we can perform multiple operations at once without needing to save each interim step as an object.\n\n\nHow would you use arrange() and count() to find what the most common religion is?\n\n\ngss_cat %&gt;%\n  count(relig) %&gt;%\n  arrange(desc(n))\n\n# A tibble: 15 × 2\n   relig                       n\n   &lt;fct&gt;                   &lt;int&gt;\n 1 Protestant              10846\n 2 Catholic                 5124\n 3 None                     3523\n 4 Christian                 689\n 5 Jewish                    388\n 6 Other                     224\n 7 Buddhism                  147\n 8 Inter-nondenominational   109\n 9 Moslem/islam              104\n10 Orthodox-christian         95\n11 No answer                  93\n12 Hinduism                   71\n13 Other eastern              32\n14 Native american            23\n15 Don't know                 15\n\n# In this case, we are not passing a variable to arrange but a logical, n. This tells R to count all the categories in relig and subsequently order these categories in descending order.*"
  },
  {
    "objectID": "labs/lab1_answers.html#summarizing-data",
    "href": "labs/lab1_answers.html#summarizing-data",
    "title": "Lab 1",
    "section": "Summarizing data",
    "text": "Summarizing data\n\nHow many hours of tv on average are watched by people of different religions?\n\nHint: select(), group_by(), and summarize() are useful functions for this\n\ngss_cat %&gt;%\n  select(relig, tvhours) %&gt;% # also works without using select\n  group_by(relig) %&gt;% \n  summarise(tvhours = mean(tvhours, na.rm = TRUE))\n\n# A tibble: 15 × 2\n   relig                   tvhours\n   &lt;fct&gt;                     &lt;dbl&gt;\n 1 No answer                  2.72\n 2 Don't know                 4.62\n 3 Inter-nondenominational    2.87\n 4 Native american            3.46\n 5 Christian                  2.79\n 6 Orthodox-christian         2.42\n 7 Moslem/islam               2.44\n 8 Other eastern              1.67\n 9 Hinduism                   1.89\n10 Buddhism                   2.38\n11 Other                      2.73\n12 None                       2.71\n13 Jewish                     2.52\n14 Catholic                   2.96\n15 Protestant                 3.15\n\n# Combining several operations can seem complex, but once you understand what is happening you can apply to most other cases. In this example we tell R to take the `gss_data`, select only `relig` and `tvhours`, group the different categories of `relig` and perform a summarising function on these groups in respect to tvhours. Inside the `summarise()` function we tell it to take an average and to remove missing values.*"
  },
  {
    "objectID": "labs/lab2_answers.html#variation",
    "href": "labs/lab2_answers.html#variation",
    "title": "Lab 2",
    "section": "Variation",
    "text": "Variation\nVariation describes how the values of a variable change each time you measure it. A continuous variable that is measured on one occasion will have different values to that same variable measured on another occasion. This variation between measurement occasions are what we call “error”. Categorical variables also vary across measurement occasions or across individuals. Visualising variation is a very intuitive way to understand trends or patterns within a variable."
  },
  {
    "objectID": "labs/lab2_answers.html#covariation",
    "href": "labs/lab2_answers.html#covariation",
    "title": "Lab 2",
    "section": "Covariation",
    "text": "Covariation\nCovariation describes how the values of at least two variables vary together or are related. Again, visualising covariation is also the most intuitive way to understand patterns between variables."
  },
  {
    "objectID": "labs/lab2_answers.html#non-graphical-eda",
    "href": "labs/lab2_answers.html#non-graphical-eda",
    "title": "Lab 2",
    "section": "Non-Graphical EDA",
    "text": "Non-Graphical EDA\nIf you are interested in a particular aspect of a variable you can summarise this information in a table. We have covered how to create nicely formatted tables in the previous practical, so the example below is just to refresh your memory."
  },
  {
    "objectID": "labs/lab2_answers.html#summary-tables",
    "href": "labs/lab2_answers.html#summary-tables",
    "title": "Lab 2",
    "section": "Summary tables",
    "text": "Summary tables\nBelow we summarise hwy by vehicle class, by asking for commonly used summary statistics.\n\nmpg %&gt;% \n  group_by(class) %&gt;%         # by class\n  summarise(min = min(hwy),   # minimum of hwy\n            mean = mean(hwy), # mean\n            sd = sd(hwy),     # standard deviation\n            max = max(hwy),   # maximum\n            n = n())  %&gt;%     # nr of observations\n  kbl() %&gt;%  \n  kable_styling(latex_options = c(\"striped\", \"hover\"), full_width = F)  \n\n\n\n\nclass\nmin\nmean\nsd\nmax\nn\n\n\n\n\n2seater\n23\n24.80000\n1.303840\n26\n5\n\n\ncompact\n23\n28.29787\n3.781620\n44\n47\n\n\nmidsize\n23\n27.29268\n2.135930\n32\n41\n\n\nminivan\n17\n22.36364\n2.062655\n24\n11\n\n\npickup\n12\n16.87879\n2.274280\n22\n33\n\n\nsubcompact\n20\n28.14286\n5.375012\n44\n35\n\n\nsuv\n12\n18.12903\n2.977973\n27\n62\n\n\n\n\n\n\n\n\nCreate a summary of engine displacement by year, including the minimum, maximum, median, and inter quantile range.\n\n\nmpg %&gt;% \n  group_by(year) %&gt;%                    # by year\n  summarise(min = min(displ),           # minimum of engine displacement\n            Q1 = quantile(displ, 0.25), # first quartile\n            median = median(displ),     # median\n            Q3 = quantile(displ, 0.75), # third quartile\n            max = max(displ),           # maximum\n            IQR = Q3 - Q1) %&gt;%          # inter quartile range\n  kbl() %&gt;%  \n  kable_styling(latex_options = c(\"striped\", \"hover\"), full_width = F)          \n\n\n\n\nyear\nmin\nQ1\nmedian\nQ3\nmax\nIQR\n\n\n\n\n1999\n1.6\n2.2\n3.0\n4.0\n6.5\n1.8\n\n\n2008\n1.8\n2.5\n3.6\n4.7\n7.0\n2.2"
  },
  {
    "objectID": "labs/lab2_answers.html#graphical-eda-base-r-vs-ggplot",
    "href": "labs/lab2_answers.html#graphical-eda-base-r-vs-ggplot",
    "title": "Lab 2",
    "section": "Graphical EDA: base R vs ggplot",
    "text": "Graphical EDA: base R vs ggplot\nPlots can be made using base R functions like plot(), hist(), or barplot(). Here are some examples of how this works on the mpg data.\n\nhist(mpg$displ)\n\n\n\n\n\n\n\n\n\nbarplot(table(mpg$cyl))\n\n\n\n\n\n\n\n\n\nplot(x = mpg$displ, y = mpg$hwy,\n     xlab = \"Highway mpg\",\n     ylab = \"Engine displacement (L)\")\n\n\n\n\n\n\n\n\nThese plots are quick to produce and are useful for an initial understanding of the data, but the syntax used to create them is specific to the type of plot. In contrast, ggplot has a streamlined approach to plotting involving largely the same steps regardless of plot type. Specifically, in ggplot we build up visualisations layer by layer using the + operator (which is similar to the %&gt;% operator we are already familiar with).\nThe main, important, layers in ggplot are:\n\nPass the data to ggplot()\nChoose aesthetic mappings with aes()\nChoose geometric components with geom()\nChoose additional labels, themes, and visuals\nChoose faceting if it is applicable with facet()\n\nWhat does this layering system look like? In Chapter 3: Data Visualisation of Hadley Wickham’s R4DS book, there is a general ggplot template to give you an idea of how plots are layered.\n```{r}\nggplot(data = &lt;DATA&gt;) + \n  &lt;GEOM_FUNCTION&gt;(\n     mapping = aes(&lt;MAPPINGS&gt;)) +\n  &lt;COORDINATE_FUNCTION&gt; +\n  &lt;FACET_FUNCTION&gt; +\n  &lt;THEME&gt; \n```\nBelow is a real example of how this looks. Remember that you do not have to / need to use all of the ggplot options (and there are many many more than presented here). However, this gives you an idea of how the layers of these plots are built up.\n\nggplot(iris) + # Data\n  geom_point(mapping = aes(x = Sepal.Length, # Variable on the x-axis\n                           y = Sepal.Width, # Variable on the y-axis\n                           colour = Species)) + # Legend \n  labs(x = \"Sepal Length\",\n       y = \"Sepal Width\", \n       title = \"Relationship between Sepal Length and Width by Species\") +\n  coord_cartesian() + # Default standard for mapping x and y\n  facet_wrap(~Species) + # Splits plot by the species variable\n  theme_bw() # Sets the background theme\n\n\n\n\n\n\n\n\nYou can read about different ggplot2 options here. The reference guides by RPubs, STHDA and Data Novia will be useful once you grasp the basics and want to make more complicated visualisations."
  },
  {
    "objectID": "labs/lab2_answers.html#visualising-distributions-of-single-variables",
    "href": "labs/lab2_answers.html#visualising-distributions-of-single-variables",
    "title": "Lab 2",
    "section": "Visualising distributions of single variables",
    "text": "Visualising distributions of single variables\n\nCategorical variables\nBar charts are often used to visualise categorical variables with a finite set of values. Below is an example how to visuailse the distribution of drv - the type of drive train in the mpg dataset.\ngeom_bar transforms each value of drv into a count to be plotted on the y-axis. The x-axis presents the category associated with each count.\n\nggplot(mpg) +\n  geom_bar(aes(x = drv))\n\n\n\n\n\n\n\n\n\nCreate a bar chart that shows a count for the different vehicle classes in mpg.\n\n\nggplot(mpg) +              # list the dataset\n  geom_bar(aes(x = class)) # geom_bar for a barplot, and the 'class' variable on the x-axis. \n\n\n\n\n\n\n\n# geom_bar adds counts on the y-axis by default\n\n\nLook up different ggplot themes, and apply one to the bar chart you just created.\n\n\nggplot(mpg) +\n  geom_bar(aes(x = class)) +\n  theme_bw() # Here we add the theme\n\n\n\n\n\n\n\n# There are different `ggplot` themes and the one you choose is largely down to personal preference, but you also want one that enhances your data visualisation. Here I choose `+ theme_bw()` but other examples include `+ theme_minimal()` and `+ theme_light()`.\n\n\n\nContinuous variables\nHistograms are often used to visualise the distribution of a continuous variable. Below is an example of how to visualise the distribution of cty - the number of city miles per gallon in the mpg dataset.\ngeom_hist transforms the x-axis into equally spaced “bins” and the height of each bar on the y-axis is the number of observations falling in each bin.\n\nggplot(mpg) +\n  geom_histogram(aes(x = cty), \n                 binwidth = 3)\n\n\n\n\n\n\n\n\nYou can change the value of binwidth to adjust the width of the intervals. Different binwidths can reveal different patterns in the data.\n\nWhat happens when you change the value of binwidth to 1? Does the distribution change?\n\n\nggplot(mpg) +\n  geom_histogram(aes(x = cty), \n                 binwidth = 1)\n\n\n\n\n\n\n\n# The general distribution does not drastically change, but it is now much easier to spot outliers. Specifically, there are two extreme values to the right of the distribution where the city miles per gallon exceed 30 in a small number of cases.\n\nDensity plots are an alternative to histograms for continuous data. Below is an example of the density of cty from the mpg data. The argument fill = \"darkseagreen\" just adds colour. You can also specify colour = \"&lt;COLOUR NAME&gt;\" to change the density line.\ngeom_density transforms the data into smoothed kernel density estimates (basically a smooth histogram). It is useful for continuous data that come from an underlying smooth distribution.\n\nggplot(mpg, aes(x = cty)) +\n  geom_density(fill = \"darkseagreen\") \n\n\n\n\n\n\n\n\nYou can also include raw data in a histogram in the form of “rug” marks.\n\nAdd rug marks to plot above by adding the argument + geom_rug(size = 1, colour = \"darkorange\").\n\n\nggplot(mpg, aes(x = cty)) +\n  geom_density(fill = \"darkseagreen\") +\n  geom_rug(size = 1, \n           colour = \"darkorange\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "labs/lab2_answers.html#visualing-the-distributions-of-multiple-variables",
    "href": "labs/lab2_answers.html#visualing-the-distributions-of-multiple-variables",
    "title": "Lab 2",
    "section": "Visualing the distributions of multiple variables",
    "text": "Visualing the distributions of multiple variables\n\nContinuous - Continuous\nScatterplots are often used to explore how two continuous variables covary together. Below is an example of how to visualise covariation between displ (engine displacement) and hwy (highway miles per gallon) in the mpg data. If there is a clear pattern in how the points fall on the x- and y-axes then we can say these variables covary.\ngeom_point plots values of displ on the y-axis and values of hwy on the x-axis.\n\nggplot(mpg) +\n  geom_point(aes(x = displ, \n                 y = hwy))\n\n\n\n\n\n\n\n\nSome of these values might be clustered together based on another characteristic. You can explore this by adding colour to the aesthetic mappings, for example.\n\nggplot(mpg) +\n  geom_point(aes(x = displ, \n                 y = hwy, \n                 colour = class))\n\n\n\n\n\n\n\n\n\nRepeat the plot above, but mapping the type of transmission to the colour aesthetic. Add a theme and change the titles of the x- and y-axes.\n\n\nggplot(mpg) +\n  geom_point(aes(x = displ, \n                 y = hwy, \n                 colour = trans)) +\n  labs(x = \"Engine Displacement (L)\",\n       y = \"Highway miles per gallon\",) +\n  theme_bw()\n\n\n\n\n\n\n\n\nDifferent aesthetic mappings can be combined in one plot. For example, you may want to add a line of best fit to a scatterplot plot, which you can do using geom_smooth().\n\nCreate a scatter plot showing the relationship between engine displacement and city miles, including a line of best fit.\n\n\nggplot(mpg, aes(x = displ, \n                y = hwy)) +\n  geom_point() +\n  geom_smooth() +\n  labs(x = \"Engine Displacement (L)\",\n       y = \"Highway miles per gallon\",) +\n  theme_bw()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# `geom_smooth()` is used to add a line of best fit. The default method is \"loess\" with the formula x ~ y, but you can change this to any other method. For example, you can choose `method = \"lm\"` for a linear best fit line.\n\nYou can fit more than one line in the same plot. This is often useful in multilevel modelling, where different hierarchies (e.g., groups, locations, individuals) have different slopes.\n\nRecreate the previous plot, but adding colour = class to the aesthetic mappings. Use method = \"lm\" and set se = FALSE in geom_smooth(). What difference does this make?\n\n\nggplot(mpg, aes(x = displ, \n                y = hwy, \n                colour = class)) +\n  geom_point() +\n  geom_smooth(method = lm, \n              se = FALSE) +\n  labs(x = \"Engine Displacement (L)\",\n       y = \"Highway miles per gallon\",) +\n  theme_bw()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n# Adding `colour = class` to the aesthetic mappings causes `geom_smooth()` to fit a line for each class of vehicle. Using `method = \"lm\"` with `se = FALSE` fits a linear line without the standard error. Dropping the standard error makes the plot more readable, since they are otherwise overlapping.\n\n\n\nContinuous - Categorical\nBoxplots are often used to explore the distribution of a continuous variable broken down by a categorical variable. Below is an example of hwy broken down by drv.\ngeom_boxplot transforms the data to show summary statistics that are represented by the different visual features of the boxplot.\n\nggplot(mpg) +\n  geom_boxplot(aes(x = drv, \n                   y = hwy))\n\n\n\n\n\n\n\n\nThese features of the boxplot are:\n\nLower horizontal line\nThicker horizontal middle line\nUpper horizontal line\nVertical whiskers\nPoints beyond the whiskers\n\n\nWhat do each of these features tell us?\n\n\n# Lower horizontal line -&gt; 25th percentile\n# Thicker horizontal middle line -&gt; median\n# Upper horizontal line -&gt; 75th percentile\n# Vertical whiskers -&gt; IQR \n# 1.5 Points beyond the whiskers -&gt;  OUTLIERS\n\n\nAdd + coord_flip() to the plot above. What does this do?\n\n\nggplot(mpg) +\n  geom_boxplot(aes(x = drv, y = hwy)) +\n  coord_flip()\n\n\n\n\n\n\n\n# coord_flip()` swaps the x- and y-axes.\n\n\nWhat can you conclude about the highway miles per gallon of each type of drive train from the boxplot above?\n\n\n# There are many outliers (points beyond the whiskers) for drive train type \"f\", which represents a front-wheel drive. There are no outliers beyond the IQR for the other types of drive train, 4-wheel drive and rear-wheel drive.\n\nThe variable trans contains 10 different transmissions types. These 10 categories can be assumed under 2 broader categories: manual and auto.\n\nUse mutate() and fct_collapse() to collapse the 10 categories of trans to just these 2 categories. ::: {.cell}\n\nmpg &lt;- mpg %&gt;% \n  mutate(trans = fct_collapse(trans,\n                             \"auto\" = c(\"auto(av)\", \n                                        \"auto(l3)\", \n                                        \"auto(l4)\", \n                                        \"auto(l5)\", \n                                        \"auto(l6)\", \n                                        \"auto(s4)\", \n                                        \"auto(s5)\", \n                                        \"auto(s6)\"),\n                             \"manual\" = c(\"manual(m5)\", \n                                          \"manual(m6)\"))) \n:::\n\nUse the previous example to create a boxplot mapping cty on the y-axis and drv on the x-axis, this time adding the argument colour = trans to aes(). What has changed?\n\n\nggplot(mpg) +\n  geom_boxplot(aes(x = drv, \n                   y = cty, \n                   colour = trans))\n\n\n\n\n\n\n\n# Adding the argument `colour = trans` to the aesthetic mappings further splits the drive train into auto and manual.\n\n# Note that you can also perform the above operations in one piped step.\n\nmpg %&gt;% \n    mutate(trans = fct_collapse(trans,\n                             \"auto\" = c(\"auto(av)\", \n                                        \"auto(l3)\", \n                                        \"auto(l4)\", \n                                        \"auto(l5)\", \n                                        \"auto(l6)\", \n                                        \"auto(s4)\", \n                                        \"auto(s5)\", \n                                        \"auto(s6)\"),\n                             \"manual\" = c(\"manual(m5)\", \n                                          \"manual(m6)\"))) %&gt;% \n  ggplot() +\n  geom_boxplot(aes(x = drv, \n                   y = cty, \n                   colour = trans))"
  },
  {
    "objectID": "labs/lab2_answers.html#facets",
    "href": "labs/lab2_answers.html#facets",
    "title": "Lab 2",
    "section": "Facets",
    "text": "Facets\nFacets are a way to split your plot into many subplots according to some categorical variable.\nTo do this, you use the command facet_wrap() to facet by a single variable. The first argument to facet_wrap() is a formula initiated by ~ and a variable name. Below is an example of how this can work on the mpg data.\n\nggplot(mpg) + \n  geom_point(aes(x = displ, \n                 y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\n\n\n\n\n\n\n\n\nIt is possible to facet on a combination of two variables using facet_grid(). Like before, the first argument is a formula, but containing two variable names separated by ~. Below is an example of how this can work.\n\nggplot(mpg) + \n  geom_point(aes(x = displ, \n                 y = hwy)) + \n  facet_grid(drv ~ cyl)\n\n\n\n\n\n\n\n\n\nNotice that there are empty cells in the bottom left facets above. What do you think this means?\n\n\n# There are no rear-wheel drives that have 4 cyclinders, so there is nothing to plot here.\n\n\nCreate a scatter plot of displ and hwy, faceting by the of manufacturer name.\n\n\nggplot(mpg) + \n  geom_point(aes(x = displ, \n                 y = hwy)) + \n  facet_wrap(~manufacturer, nrow = 3)\n\n\n\n\n\n\n\n\n\nChange the names on the axes to be more informative.\n\n\nggplot(mpg) + \n  geom_point(aes(x = displ, \n                 y = hwy)) + \n  labs(x = \"Engine displacement (litres)\", \n       y = \"Highway miles per gallon\") +\n  facet_wrap(~manufacturer, nrow = 3)"
  },
  {
    "objectID": "labs/lab2_answers.html#arranging-multiple-plots",
    "href": "labs/lab2_answers.html#arranging-multiple-plots",
    "title": "Lab 2",
    "section": "Arranging multiple plots",
    "text": "Arranging multiple plots\nIf you have multiple plots that you want to arrange on the same page, you can use ggarrange() from the ggpubr package.\nThe first step is creating some plots. For this exercise we will only use the variables hwy, cty, and class.\n\nCreate the following three plots:\n\nA. A bar plot showing counts of class, assigning class to the fill aesthetic.\nB. A box plot of class by hwy per gallon, assigning class to the colour aesthetic.\nC. A jittered scatterplot of hwy and cty miles per gallon, assigning class to the colour aesthetic.\nRemember to save each plot as an object to be called upon later.\n\n# plot A\np1 &lt;- ggplot(data = mpg, aes(class, fill = class)) +\n        geom_bar() +\n        labs(x = \"Vehicle class\",\n              y = \"Count\") +\n        coord_flip() +  \n        theme_bw()\n\n# plot B\np2 &lt;- ggplot(data = mpg) + \n        geom_boxplot(mapping = aes(x = class, y = hwy, colour = class)) +\n        labs(x = \"Vehicle class\",\n             y = \"Highway mpg\") +\n        coord_flip() +\n        theme_bw()\n# plot C\np3 &lt;- ggplot(data = mpg) + \n        geom_jitter(mapping = aes(x = cty, y = hwy, colour = class)) +\n        labs(x = \"Vehicle class\",\n             y = \"Highway mpg\") +\n        theme_bw()\n\n# I create three plots names \"p1\", \"p2\", and \"p3\", using previous examples to guide me if necessary. For p1 and p2 I add `coord_flip()` to make the categories easier to read.\n\nD. Use ggarrange() to arrange the three plots in one space. ggarrange() takes the plots as arguments as well as ncol or nrow to customise the arrangement\n\nggarrange(p1, p2, p3, \n          ncol = 2, \n          nrow = 2)\n\n\n\n\n\n\n\n# The only necessary arguments are the plots, but specifying `ncol` and `nrow` can improve the appearance. the repetition of the legend is usually something we want to avoid.\n\nE. Repeat the code from the previous question, adding common.legend = TRUE and legend = \"bottom\" to ggarrange().\n\nggarrange(p1, p2, p3, \n          nrow = 2, \n          ncol = 2, \n          vjust = 0.5, \n          common.legend = TRUE,\n          legend = \"bottom\")\n\n\n\n\n\n\n\n# Setting `common.legend = TRUE` gets rid of the repetitive legend and slightly improves the look. In this example I move the legend to the bottom position, but \"top\", \"left\", and \"right\" are other options."
  },
  {
    "objectID": "labs/lab2_answers.html#loading-the-dataset",
    "href": "labs/lab2_answers.html#loading-the-dataset",
    "title": "Lab 2",
    "section": "Loading the dataset",
    "text": "Loading the dataset\nIn the this practical, we will use the built-in data set iris. This data set contains the measurement of different iris species (flowers), you can find more information here.\n\nLoad the dataset and explain what variables are measured in the first three columns of your data set.\n\n\ndata &lt;- iris # load the data\nhead(iris)   # inspect the data\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n# The data set contains three different kinds of flowers. The petal leaves and sepal leaves are measured in length and width. All measurements are in centimeters."
  },
  {
    "objectID": "labs/lab2_answers.html#inspecting-the-dataset",
    "href": "labs/lab2_answers.html#inspecting-the-dataset",
    "title": "Lab 2",
    "section": "Inspecting the dataset",
    "text": "Inspecting the dataset\nA good way of eyeballing on a relation between two continuous variables is by creating a scatterplot.\n\nPlot the sepal length and the petal width variables in a ggplot scatter plot (geom_points)\n\n\nggplot(data) +\n  geom_point(aes(Sepal.Length, Petal.Width)) +\n  xlab(\"Sepal length (in cm)\") +\n  ylab(\"Petal width (in cm)\") +\n  labs(col = \"Species\") +\n  theme_minimal() +\n  ggtitle(\"Plot of 2 continous variables\")  + \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nA loess curve can be added to the plot to get a general idea of the relation between the two variables. You can add a loess curve to a ggplot with stat_smooth(method = \"loess\").\n\nAdd a loess curve to the plot under question 2, for further inspection.\n\n\nggplot(data, aes(x = Sepal.Length, y = Petal.Width)) +\n  geom_point() +\n  stat_smooth(method = \"loess\", se=F, col = \"blue\") +\n  xlab(\"Sepal length (in cm)\") +\n  ylab(\"Petal width (in cm)\") +\n  labs(col = \"Species\") +\n  theme_minimal() +\n  ggtitle(\"Plot of 2 continous variables\")  + \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n# The curve is added to the previous plot by the line `stat_smooth(method = \"loess, se = F, col = \"blue\")`.\n\nTo get a clearer idea of the general trend in the data (or of the relation), a regression line can be added to the plot. A regression line can be added in the same way as a loess curve, the method argument in the function needs to be altered to lm to do so.\n\nChange the loess curve of the previous plot to a regression line. Describe the relation that the line indicates.\n\n\n# In comparison to the previous plot, we now adjust \"method = \"loess\"\" to \"method = \"lm\"\".\nggplot(data, aes(x = Sepal.Length, y = Petal.Width)) +\n  geom_point() +\n  stat_smooth(method = \"lm\", se=F, col = \"blue\") +\n  xlab(\"Sepal length (in cm)\") +\n  ylab(\"Petal width (in cm)\") +\n  labs(col = \"Species\") +\n  theme_minimal() +\n  ggtitle(\"Plot of 2 continous variables\")  + \n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n# The line indicates that there seems to be a more or less linear positive relation between the two plotted variables. This means that an increase in sepal length probably indicates an increase in petal width as well."
  },
  {
    "objectID": "labs/lab2_answers.html#simple-linear-regression",
    "href": "labs/lab2_answers.html#simple-linear-regression",
    "title": "Lab 2",
    "section": "Simple linear regression",
    "text": "Simple linear regression\nWith the lm() function, you can specify a linear regression model. You can save a model in an object and request summary statistics with the summary() command. The model is always specified with the code outcome_variable ~ predictor.\nWhen a model is stored in an object, you can ask for the coefficients with coefficients(). The next code block shows how you would specify a model where petal width is predicted by sepal width, and how summary statistics for this model would look like\n\n# Specify model: outcome = petal width, predictor = sepal width\niris_model1 &lt;- lm(Petal.Width ~ Sepal.Width,\n                  data = iris)\n\nsummary(iris_model1)\n\n\nCall:\nlm(formula = Petal.Width ~ Sepal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.38424 -0.60889 -0.03208  0.52691  1.64812 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   3.1569     0.4131   7.642 2.47e-12 ***\nSepal.Width  -0.6403     0.1338  -4.786 4.07e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7117 on 148 degrees of freedom\nMultiple R-squared:  0.134, Adjusted R-squared:  0.1282 \nF-statistic: 22.91 on 1 and 148 DF,  p-value: 4.073e-06\n\n\nThe summary of the model provides:\n\nThe model formula;\nEstimated coefficients (with standard errors and their significance tests);\nInformation on the residuals;\nA general test for the significance of the model (F-test);\nThe (adjusted) R squared as a metric for model performance.\n\nIndividual elements can be extracted by calling specific model elements (e.g. iris_model1$coefficients).\n\nSpecify a regression model where Sepal length is predicted by Petal width. Store this model as `model1. Supply summary statistics for this model.\n\n\n# specify model\nmodel1 &lt;- lm(Sepal.Length ~ Petal.Width, \n             data = data)\n\n# ask for summary\nsummary(model1)\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Width, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.38822 -0.29358 -0.04393  0.26429  1.34521 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.77763    0.07293   65.51   &lt;2e-16 ***\nPetal.Width  0.88858    0.05137   17.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.478 on 148 degrees of freedom\nMultiple R-squared:  0.669, Adjusted R-squared:  0.6668 \nF-statistic: 299.2 on 1 and 148 DF,  p-value: &lt; 2.2e-16\n\n\n\nBased on the summary of the model, give a substantive interpretation of the regression coefficient.\n\n\n# The regression coefficient indicates the amount of change in the predicted variable when the predictor variable is changed with one unit. In case of the example model, this means that for every centimeter increase in the width of a petal leaf, the predicted length of a sepal leaf increases by 0.89 cm.\n\n\nRelate the summary statistics and coefficients to the plots you made in questions 2 - 4.\n\n\n# The coefficients seem to be in accordance with the earlier plots. They are not exactly the same, but indicate a similar positive relationship."
  },
  {
    "objectID": "labs/lab3_answers.html",
    "href": "labs/lab3_answers.html",
    "title": "Lab 3",
    "section": "",
    "text": "We will use the following packages in this practical:\n\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(gridExtra)\n\nWe will use the same data that we used last week to perform a simple linear regression, the Iris dataset. But now, we will extend on this simple model with multiple variables.\nIn order to do this, we first need to load the data again and run the simple model where Sepal length is predicted by Petal width.\n\ndata &lt;- iris # load the data\nhead(iris)   # inspect the data\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n\n\n# specify model\nmodel1 &lt;- lm(Sepal.Length ~ Petal.Width, \n             data = data)\n\n# ask for summary\nsummary(model1)\n\n\nCall:\nlm(formula = Sepal.Length ~ Petal.Width, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.38822 -0.29358 -0.04393  0.26429  1.34521 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.77763    0.07293   65.51   &lt;2e-16 ***\nPetal.Width  0.88858    0.05137   17.30   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.478 on 148 degrees of freedom\nMultiple R-squared:  0.669, Adjusted R-squared:  0.6668 \nF-statistic: 299.2 on 1 and 148 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "labs/lab3_answers.html#categorical-predictors",
    "href": "labs/lab3_answers.html#categorical-predictors",
    "title": "Lab 3",
    "section": "Categorical predictors",
    "text": "Categorical predictors\nUp to here, we only included continuous predictors in our models. We will now include a categorical predictor in the model as well.\nWhen a categorical predictor is added, this predictor is split in several contrasts (or dummies), where each group is compared to a reference group. In our example Iris data, the variable ‘Species’ is a categorical variable that indicate the species of flower. This variable can be added as example for a categorical predictor. Contrasts, and thus the dummy coding, can be inspected through contrasts().\n\nAdd species as a predictor to the model specified as model2, store it under the name model3 and interpret the categorical coefficients of this new model.\n\n\n# Create 3rd model with categorical predictor\nmodel3 &lt;- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Species,\n             data = data)\n\n# Ask for summary data\nsummary(model3)\n\n\nCall:\nlm(formula = Sepal.Length ~ Sepal.Width + Petal.Length + Species, \n    data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82156 -0.20530  0.00638  0.22645  0.74999 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        2.39039    0.26227   9.114 5.94e-16 ***\nSepal.Width        0.43222    0.08139   5.310 4.03e-07 ***\nPetal.Length       0.77563    0.06425  12.073  &lt; 2e-16 ***\nSpeciesversicolor -0.95581    0.21520  -4.442 1.76e-05 ***\nSpeciesvirginica  -1.39410    0.28566  -4.880 2.76e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3103 on 145 degrees of freedom\nMultiple R-squared:  0.8633,    Adjusted R-squared:  0.8595 \nF-statistic: 228.9 on 4 and 145 DF,  p-value: &lt; 2.2e-16\n\n# In the output, we see that 'Species' has multiple rows of output, and that one species (Setosa) does not seem to show. Setosa is the reference group. The other two lines are those respecitve groups compared to the setosa group. This means that that the predicted sepal length of a versicolor would be .9558 lower than the predicted value of a setosas with the same values on the other variables."
  },
  {
    "objectID": "labs/lab3_answers.html#calculating-new-predicted-values-with-a-regression-equation",
    "href": "labs/lab3_answers.html#calculating-new-predicted-values-with-a-regression-equation",
    "title": "Lab 3",
    "section": "Calculating new predicted values with a regression equation",
    "text": "Calculating new predicted values with a regression equation\nA regression model can be used to predict values for new cases that were not used to built the model. The regression equation always consists of coefficients (\\(\\beta\\)s) and observed variables (\\(X\\)):\n\\[\\hat{y} = \\beta_0 + \\beta_1 * X_{a}* + \\beta_2 * X_b +  \\ldots  + \\beta_n * X_n\\]\nAll terms can be made specific for the regression equation of the created model. For example, if we have a model where ‘happiness’ is predicted by age and income (scored from 1-50), the equation could look like:\n\\[\\hat{y}_{happiness} = \\beta_{intercept} + \\beta_{age} * X_{age} + \\beta_{income} * X_{income}\\]\nThen, we could impute the coefficients obtained through the model. Given \\(\\beta_{intercept} = 10.2\\), \\(\\beta_{age} = 0.7\\), and \\(\\beta_{income} = 1.3\\), the equation would become:\n\\[\\hat{y}_{happiness} = 10.2 + 0.7 * X_{age} + 1.3 * X_{income}\\]\nIf we now want to predict the happiness score for someone of age 28 and with an income score of 35, the prediction would become:\n\\[\\hat{y}_{happiness} = 10.2 + 0.7 * 28 + 1.3 * 35 = 75.3\\]\n\nGiven this regression equation, calculate the predicted value for someone of age 44 and an income score of 27.\n\n\n# Calculate score\n10.2 + .7*44 + 1.3*27 # 76.1\n\n[1] 76.1"
  },
  {
    "objectID": "labs/lab3_answers.html#prediction-with-a-categorical-variable",
    "href": "labs/lab3_answers.html#prediction-with-a-categorical-variable",
    "title": "Lab 3",
    "section": "Prediction with a categorical variable",
    "text": "Prediction with a categorical variable\nAdding a categorical predictor to the regression equation gives the number of contrasts as coefficient terms added. The previous regression equation for predicting happiness could be adjusted by adding ‘living density’ as a categorical predictor with levels ‘big city’, ‘smaller city’, ‘rural’, where ‘big city’ would be the reference category. The equation could then be:\n\\[\\hat{y}_{happiness} = 10.2 + 0.7 * X_{age} + 1.3 * X_{income} + 8.4 * X_{smaller city} + 17.9 * X_{rural}\\]\nWhen predicting a score for an equation with a categorical predictor, you just assign a 1 to the category that the observation belongs to, and 0s for all other categories.\n\nGiven this equation, calculate the predicted score for someone of age 29, an income score of 21, and living in a smaller city. And what would this score be if the person would live in a big city instead?\n\n\n# impute X_age with 29, X_income with 21, X_smallercity with 1, and X_rural with 0\n10.2 + .7*29 + 1.3*21 + 8.4*1 + 17.9*0 # = 66.2\n\n# If this person would live in a big city, the equation would become\n10.2 + .7*29 + 1.3*21 + 8.4*0 + 17.9*0 # = 57.8"
  },
  {
    "objectID": "labs/lab3_answers.html#prediction-with-an-interaction",
    "href": "labs/lab3_answers.html#prediction-with-an-interaction",
    "title": "Lab 3",
    "section": "Prediction with an interaction",
    "text": "Prediction with an interaction\nIn regression equations with an interaction, an extra coefficient is added to the equation. For example, the happiness equation with age and income as predictors could have an added interaction term. The equation could then look like:\n\\[\\hat{y}_{happiness} = 10.2 + 0.7 * X_{age} + 1.3 * X_{income} + 0.01 * X_{age} * X_{income}\\]\nFor a person of age 36 and income score 30, the predicted score would be:\n\\[\\hat{y}_{happiness} = 10.2 + 0.7 * 36 + 1.3 * 30 + 0.01 * 36 * 30 = 85.2\\]\n\nGiven this regression equation with interaction term, what would be the predicted happiness score for someone of age 52 and income score 26?\n\n\n# Imputed equation X_age = 52, X_income = 26\n10.2 + 0.7 * 52 + 1.3 * 26 + 0.01 * 52 * 26 # 93.92\n\n[1] 93.92"
  },
  {
    "objectID": "labs/lab5.html",
    "href": "labs/lab5.html",
    "title": "Lab 5",
    "section": "",
    "text": "We will use the following packages in this practical:\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(foreign)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(readr)\nIn this practical, you will perform regression analyses using glm() and inspect variables by plotting these variables, using ggplot()."
  },
  {
    "objectID": "labs/lab5.html#working-with-odds-and-log-odds",
    "href": "labs/lab5.html#working-with-odds-and-log-odds",
    "title": "Lab 5",
    "section": "Working with odds and log-odds",
    "text": "Working with odds and log-odds\nBefore we get started with logistic modelling it helps to understand how odds, log-odds, and probability are related. Essentially, they are all just different expressions of the same thing and converting between them involve simple formulas.\nCoefficients calculated using the glm() function returns log-odds by default. Most of us find it difficult to think in terms of log-odds, so instead we convert them to odds (or odds-ratios) using the exp() function. If we want to go from odds to log-odds, we just take the logarithm using log().\nAn odds-ratio is the probability of success and is defined as \\(Odds = \\frac{P}{1-P}\\), where \\(P\\) is the probability of an event happening and \\(1-P\\) is the probability that it does not happen. For example, if we have an 80% chance of a sunny day, then we have a 20% chance of a rainy day. The odds would then equal \\(\\frac{.80}{.20} = 4\\), meaning the odds of a sunny day are 4 to 1. Let’s consider this further with an example.\nThe code below creates a data frame called data with a column called conc showing the number of trials wherein different concentrations of the peptide-C protein inhibited the flow of current across a membrane. The yes column contains counts of trials where this occured.\n\ndata &lt;- data.frame(conc = c(0.1, 0.5, 1, 10, 20, 30, 50, 70, 80, 100, 150),\n                   no = c(7, 1, 10, 9, 2, 9, 13, 1, 1, 4, 3),\n                   yes = c(0, 0, 3, 4, 0, 6, 7, 0, 0, 1 ,7)\n                   ) \ndata\n\n    conc no yes\n1    0.1  7   0\n2    0.5  1   0\n3    1.0 10   3\n4   10.0  9   4\n5   20.0  2   0\n6   30.0  9   6\n7   50.0 13   7\n8   70.0  1   0\n9   80.0  1   0\n10 100.0  4   1\n11 150.0  3   7\n\n\n\nAdd the following variables to the dataset:\n\n\nthe total number of trials for each observation (i.e., the sum of the no and yes trials for each row)\nthe proportion of yes trials in each row (i.e. yes divided by the total)\nthe log-odds of inhibition for each row (i.e. the log-odds of yes vs no)\n\n\nInspect the new columns. Do you notice anything unusual?\nAdd a new column to your dataset containing the corrected odds.\n\nYou can compute the value of this column using the following formulation of the log-odds:\n\\[ log(odds) = log(\\frac{yes + 0.5} {no + 0.5}) \\]\n\nFit a logistic regression model where:\n\n\nprop is the outcome\nconc is the only predictor\nthe number of total trials per row are used as weights (we need this because a different number of trials can go into defining each observation of prop)\n\nInterpret the slope estimate."
  },
  {
    "objectID": "labs/lab5.html#titanic-data",
    "href": "labs/lab5.html#titanic-data",
    "title": "Lab 5",
    "section": "Titanic data",
    "text": "Titanic data\nYou will work with the titanic data set which you can download here, containing information on the fate of passengers on the infamous voyage.\n\nSurvived: this is the outcome variable that you are trying to predict, with 1 meaning a passenger survived and 0 meaning they did not\nPclass: this is the ticket class the passenger was travelling on, with 1, 2, and 3 representing 1st, 2nd and 3rd class respectively\nAge: this is the age of the passenger in years\nSex: this is the sex of the passenger, either male or female\n\n\nRead in the data from the “titanic.csv” file, selecting only the variables Survived, Pclass, Sex and Age. If necessary, correct the class of the variables.\nWhat relationships do you expect to find between the predictor variables and the outcome?\nInvestigate how many passengers survived in each class. You can do this visually by creating a bar plot, or by using the table() function. Search ??table for more information.\nSimilarly, investigate the relationship between survival and sex by creating a bar plot and a table.\nInvestigate the relationship between age and survival by creating a histogram of the age of survivors versus non-survivors."
  },
  {
    "objectID": "labs/lab5.html#no-predictors",
    "href": "labs/lab5.html#no-predictors",
    "title": "Lab 5",
    "section": "No predictors",
    "text": "No predictors\n\nSpecify a logistic regression model where “Survived” is the outcome and there are no predictors."
  },
  {
    "objectID": "labs/lab5.html#binary-predictor",
    "href": "labs/lab5.html#binary-predictor",
    "title": "Lab 5",
    "section": "Binary predictor",
    "text": "Binary predictor\n\nSpecify a logistic regression model where “Survived” is the outcome and “Sex” is the only predictor.\nWhat does the intercept mean? What are the odds and what are the log-odds of survival for males?"
  },
  {
    "objectID": "labs/lab5.html#categorical-predictor-more-than-2-categories",
    "href": "labs/lab5.html#categorical-predictor-more-than-2-categories",
    "title": "Lab 5",
    "section": "Categorical predictor (more than 2 categories)",
    "text": "Categorical predictor (more than 2 categories)\n\nSpecify a logistic regression model where “Survived” is the outcome and “Pclass” is the only predictor.\nWhich category is the reference group? What are their odds of survival?\nWhat are the chances of survival for 2nd and 3rd class passengers?"
  },
  {
    "objectID": "labs/lab5.html#continuous-predictor",
    "href": "labs/lab5.html#continuous-predictor",
    "title": "Lab 5",
    "section": "Continuous predictor",
    "text": "Continuous predictor\n\nSpecify a logistic regression model where “Survived” is the outcome and “Age” is the only predictor.\n\nSave this model as you will come back to it later.\n\nWhat does the intercept mean when there is a continuous predictor?\nHow are the odds and log-odds interpreted for a continuous predictor?"
  },
  {
    "objectID": "labs/lab5.html#multinomial-model-with-an-interaction-term",
    "href": "labs/lab5.html#multinomial-model-with-an-interaction-term",
    "title": "Lab 5",
    "section": "Multinomial model with an interaction term",
    "text": "Multinomial model with an interaction term\n\nSpecify a logistic regression model Survived is the outcome and Pclass plus an interaction between Sex and Age as the predictor.\n\nSave this model as we will return to it later.\n\nHow is the significant interaction term interpreted in this model?"
  },
  {
    "objectID": "labs/lab5.html#deviance",
    "href": "labs/lab5.html#deviance",
    "title": "Lab 5",
    "section": "Deviance",
    "text": "Deviance\nDeviance is measure of the goodness-of-fit in a GLM where lower deviance indicates a better fitting model. R reports two types of deviance:\n\nnull deviance: how well the outcome is predicted by the intercept-only model\nresidual deviance: how well the outcome is predicted by the model with the predictors added\n\n\nGet the model summaries and indicate what the null and residual deviance are.\n\nWe can use the anova() function to perform an analysis of deviance that compares the difference in deviances between competing models.\n\nCompare the fit of model 1 with the fit of model 2 using anova() andtest = “Chisq”`."
  },
  {
    "objectID": "labs/lab5.html#information-criteria",
    "href": "labs/lab5.html#information-criteria",
    "title": "Lab 5",
    "section": "Information criteria",
    "text": "Information criteria\nAIC is the Akaike’s Information Criterion, a method for assessing model quality through comparison of related models. AIC is based on the deviance but introduces a penalty for more complex models. The number itself is not meaninful, and it is only useful when comparing models against one another. Like deviance, the model with the lowest AIC is best.\n\nUse the AIC() function to get the AIC value for model 1 and model 2.\n\nBIC is the Bayesian Information Criterion and is very similar to AIC, but penalises a complex model more than the AIC would. Complex models will have a larger score indicating worse fit. One difference to the AIC is that the probability of selecting the correct model with the BIC increases as the sample size of the training set increases.\n\nUse the BIC() function to get the BIC value for model 1 and model 2.\nWhich model should we proceed with?"
  },
  {
    "objectID": "labs/lab6.html",
    "href": "labs/lab6.html",
    "title": "Lab 6",
    "section": "",
    "text": "We will use the following packages in this practical:\n\ndplyr for manipulation\nmagrittr for piping\nreadr for reading data\nggplot for plotting\nkableExtra for tables\nlibrary(pROC), library(regclass), and library(caret) for model diagnostics\n\n\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(readr)\nlibrary(pROC)\nlibrary(regclass)\nlibrary(caret)\n\n\n\nIn this practical, you will perform logistic regression analyses again using glm() and discuss model assumptions and diagnostics.\n1. Read in the data from the “titanic.csv” file, which we also used for the previous practical.\n\n\n\n\nFit the following two logistic regression models and save them as fit1 and fit2:\n\n\nSurvived ~ Pclass\nSurvived ~ Age + Pclass*Sex"
  },
  {
    "objectID": "labs/lab6.html#loading-the-data",
    "href": "labs/lab6.html#loading-the-data",
    "title": "Lab 6",
    "section": "",
    "text": "In this practical, you will perform logistic regression analyses again using glm() and discuss model assumptions and diagnostics.\n1. Read in the data from the “titanic.csv” file, which we also used for the previous practical."
  },
  {
    "objectID": "labs/lab6.html#logistic-regression",
    "href": "labs/lab6.html#logistic-regression",
    "title": "Lab 6",
    "section": "",
    "text": "Fit the following two logistic regression models and save them as fit1 and fit2:\n\n\nSurvived ~ Pclass\nSurvived ~ Age + Pclass*Sex"
  },
  {
    "objectID": "labs/lab6.html#binary-dependent-variable",
    "href": "labs/lab6.html#binary-dependent-variable",
    "title": "Lab 6",
    "section": "Binary dependent variable",
    "text": "Binary dependent variable\nThe first outcome in a logistic regression is that the outcome should be binary and therefore follow a binomial distribution. This is easy to check: you just need to be sure that the outcome can only take one of two responses. You can plot the responses of the outcome variable to visually check this if you want. In our case, the possible outcomes are:\n\nSurvived (coded 1)\nDid not survive (coded 0)\n\n\nVisualise the responses of the outcome variable Survived using ggplot()."
  },
  {
    "objectID": "labs/lab6.html#balanced-outcomes",
    "href": "labs/lab6.html#balanced-outcomes",
    "title": "Lab 6",
    "section": "Balanced outcomes",
    "text": "Balanced outcomes\nIf you are using logistic regression to make predictions/classifications then the accuracy will be affected by imbalance in the outcome classes. Notice that in the plot you just made there are more people who did not survive than who did survive. A possible consequence is reduced accuracy in classification of survivors.\nA certain amount of imbalance is expected and can be handled well by the model in most cases. The effects of this imbalance is context-dependent. Some solutions to serious class imbalance are down-sampling or weighting the outcomes to balance the importance placed on the outcomes by the model."
  },
  {
    "objectID": "labs/lab6.html#sufficiently-large-sample-size",
    "href": "labs/lab6.html#sufficiently-large-sample-size",
    "title": "Lab 6",
    "section": "Sufficiently large sample size",
    "text": "Sufficiently large sample size\nSample size in logistic regression is a complex issue, but some suggest that it is ideal to have 10 cases per candidate predictor in your model. The minimum number of cases to include is \\(N = \\frac{10*k} {p}\\), where \\(k\\) is the number of predictors and \\(p\\) is the smallest proportion of negative or positive cases in the population.\n\nCalculate the minimum number of positive cases needed in the model fit1."
  },
  {
    "objectID": "labs/lab6.html#predictor-matrix-is-full-rank",
    "href": "labs/lab6.html#predictor-matrix-is-full-rank",
    "title": "Lab 6",
    "section": "Predictor matrix is full-rank",
    "text": "Predictor matrix is full-rank\nYou learned about this assumption in the linear regression practicals, but to remind you:\n\nthere need to be more observations than predictors (n &gt; P)\nthere should be no multicollinearity among the linear predictors\n\n\nCheck that there is no multicollinearity in the logistic model."
  },
  {
    "objectID": "labs/lab6.html#continuous-predictors-are-linearly-related-to-the-logitpi",
    "href": "labs/lab6.html#continuous-predictors-are-linearly-related-to-the-logitpi",
    "title": "Lab 6",
    "section": "Continuous predictors are linearly related to the \\(logit(\\pi)\\)",
    "text": "Continuous predictors are linearly related to the \\(logit(\\pi)\\)\nLogistic regression models assume a linear relationship between predictor variables and the logit of the outcome variable. This assumption is mainly concerned with continuous predictors. Since we only have one continuous predictor (Age) we can plot the relationship between Age and the logit of Survived.\n\nGet the predicted values of fit2 on the logit scale and bind them to the titanic data.\nPlot the relationship between Age and logit and interpret it.\nHow should we deal with variables that are not linearly related to the logit?"
  },
  {
    "objectID": "labs/lab6.html#no-influential-values-or-outliers",
    "href": "labs/lab6.html#no-influential-values-or-outliers",
    "title": "Lab 6",
    "section": "No influential values or outliers",
    "text": "No influential values or outliers\nInfluential values are extreme individual data points that can affect the fit of the logistic regression model. They can be visualised using Cook’s distance and the Residuals vs Leverage plot.\n\nUse the plot() function to visualise the outliers and influential points of fit2.\n\nHint: you need to specify the correct plot with the which argument. Check the lecture slides or search ??plot if you are unsure.\n\nAre there any influential cases in the Leverage vs Residuals plot? If so, what would you do?"
  },
  {
    "objectID": "labs/lab6.html#differences-to-linear-regressoin",
    "href": "labs/lab6.html#differences-to-linear-regressoin",
    "title": "Lab 6",
    "section": "Differences to linear regressoin",
    "text": "Differences to linear regressoin\nLastly, it is important to note that the assumptions of a linear regression do not all map to logistic regression. In logistic regression, we do not need:\n\nconstant, finite error variance\nnormally distributed errors\n\nHowever, deviance residuals are useful for determining if the individual points are not fit well by the model.\nHint: you can use some of the code from the lecture for the next few questions.\n\nUse the resid() function to get the deviance residuals for fit2.\nCompute the predicted logit values for the model.\nPlot the deviance residuals.\n\nPearson residuals can also be useful in logistic regression. They measure deviations between the observed and fit1ted values. Pearson residuals are easier to plot than deviance residuals as the plot() function can be used.\n\nPlot the pearson residuals for the model."
  },
  {
    "objectID": "labs/lab6.html#confusion-matrix",
    "href": "labs/lab6.html#confusion-matrix",
    "title": "Lab 6",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nYou can read about the confusion matrix on this Wikipedia page. This section tells you how to get some useful metrics from the confusion matrix to evaluate model performance.\n\nCreate two confusion matrices (one each for each model) using the classifications from the previous question. You can use the table() function, providing the modeled outcome as the true parameter and the classifications as the pred parameter.\nBased on the confusion matrices, which model do you think makes better predictions?\nCalculate the accuracy, sensitivity, and specificity, false positive rate, positive and negative predictive values from the confusion matrix of the model that makes the best predictions.\nExplain what the difference metrics mean in substantive terms?\nWhat does it mean for a model to have such low specificity, but high sensitivity?\n\nThe confusionMatrix() function from the caret package can do a lot of this for us. The function takes three arguments:\n\ndata - a vector of predicted classes (in factor form)\nreference - a vector of true classes (in factor from)\npositive - a character string indicating the ‘positive’ outcome. If not specified, the confusion matrix assumes that the first specified category is the positive outcome.\n\nYou can type ??confusionMatrix into the console to learn more."
  },
  {
    "objectID": "lectures.html",
    "href": "lectures.html",
    "title": "Lectures",
    "section": "",
    "text": "Below, you can access the lecture slides and associated materials for each week."
  },
  {
    "objectID": "lectures.html#w1",
    "href": "lectures.html#w1",
    "title": "Lectures",
    "section": "Week 1",
    "text": "Week 1\nR Programming Fundamentals\n\nLecture Slides\nProject Organization Demo\n\nThis link will download a ZIP archive that you must unpack on your computer."
  },
  {
    "objectID": "lectures.html#w2",
    "href": "lectures.html#w2",
    "title": "Lectures",
    "section": "Week 2",
    "text": "Week 2\nData Visualization & Simple Linear Regression\n\nVisualization Lecture Slides\nSLR Lecture Slides"
  },
  {
    "objectID": "lectures.html#w3",
    "href": "lectures.html#w3",
    "title": "Lectures",
    "section": "Week 3",
    "text": "Week 3\nMultiple Linear Regression\n\nLecture Slides"
  },
  {
    "objectID": "lectures.html#w4",
    "href": "lectures.html#w4",
    "title": "Lectures",
    "section": "Week 4",
    "text": "Week 4\nMLR Assumptions & Diagnostics\n\nAssumptions Lecture Slides\nInfluential Observations Lecture Slides"
  },
  {
    "objectID": "lectures.html#w5",
    "href": "lectures.html#w5",
    "title": "Lectures",
    "section": "Week 5",
    "text": "Week 5\nGeneralized Linear Models & Logistic Regression\n\nGLM & Logistic Regression Lecture Slides\nExtra: Extended Example Slides"
  },
  {
    "objectID": "lectures.html#w6",
    "href": "lectures.html#w6",
    "title": "Lectures",
    "section": "Week 6",
    "text": "Week 6\nLogistic Regression Assumptions & Assessing Classification Performance\n\nAssumption & Diagnostics Slides\nClassification Performance Slides"
  },
  {
    "objectID": "lectures.html#w7",
    "href": "lectures.html#w7",
    "title": "Lectures",
    "section": "Week 7",
    "text": "Week 7\nWrap Up & Review\n\nLecture Slides"
  },
  {
    "objectID": "preparation.html",
    "href": "preparation.html",
    "title": "Preparation",
    "section": "",
    "text": "In this course, you will use both R and RStudio.\nIf you have never worked with R before, use the tutorials from Open Statistical Programming to be ready and prepared before the course starts. The materials discussed in the tutorials\n\nSoftware setup\nFirst steps\nData types\nBasic visualizations\n\nare required knowledge before you start the course!"
  },
  {
    "objectID": "preparation.html#required-r-skills",
    "href": "preparation.html#required-r-skills",
    "title": "Preparation",
    "section": "",
    "text": "In this course, you will use both R and RStudio.\nIf you have never worked with R before, use the tutorials from Open Statistical Programming to be ready and prepared before the course starts. The materials discussed in the tutorials\n\nSoftware setup\nFirst steps\nData types\nBasic visualizations\n\nare required knowledge before you start the course!"
  },
  {
    "objectID": "preparation.html#required-statistical-knowledge",
    "href": "preparation.html#required-statistical-knowledge",
    "title": "Preparation",
    "section": "Required statistical knowledge",
    "text": "Required statistical knowledge\nWe expect you to be familiar with some basic statistical concepts such as:\n\nDescriptive statistics\nSampling\nCorrelation\nT-test\nP-values\n\nTo refresh your memory, you can read SDAM Ch. 1, 2 and 3.\nThe course material builds on this knowledge, and it can also be part of the exam.\nNote: if you followed the course ADS-BOS, you have covered these topics and you should be good to go!"
  }
]