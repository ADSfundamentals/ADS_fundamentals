{"title":"Lab 3","markdown":{"yaml":{"title":"Lab 3","output-file":"lab3.html","params":{"answers":false}},"headingText":"Part 1: Repeat from last week","containsRefs":false,"markdown":"\n\n\nWe will use the following packages in this practical:\n\n```{r, message=FALSE, warning=FALSE}\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(gridExtra)\n```\n\nWe will use the same data that we used last week to perform a simple linear regression, the Iris dataset. But now, we will extend on this simple model with multiple variables.\n\nIn order to do this, we first need to load the data again and run the simple model where Sepal length is predicted by Petal width.\n\n```{r}\ndata <- iris # load the data\nhead(iris)   # inspect the data\n\n```\n\n```{r}\n# specify model\nmodel1 <- lm(Sepal.Length ~ Petal.Width, \n             data = data)\n\n# ask for summary\nsummary(model1)\n```\n\n# Part 2: Multiple linear regression\n\nYou can add additional predictors to a model. This can improve the fit and the predictions. When multiple predictors are used in a regression model, it's called a Multiple linear regression. You specify this model as `outcome_variable ~ predictor_1 + predictor_2 + ... + predictor_n`.\n\n1.  **Add Petal length as a second predictor to the model specified as `model1` and store this under the name `model2`, and supply summary statistics. Again, give a substantive interpretation of the coefficients and the model.**\n\n```{r, include = params$answers}\n# Specify additional predictors\nmodel2 <- lm(Sepal.Length ~ Petal.Width + Petal.Length, \n             data = data)\n\n# Ask for summary statistics again\nsummary(model2)\n\n# When comparing the coefficients of model 2 with the coefficients of model 1, we can see that adding a predictor can change the coefficients of other predictors as well (it is a new model after all). In this example, it is notable that the coefficient for petal width has become a negative number, while it was positive in model 1.\n```\n\n## Categorical predictors\n\nUp to here, we only included continuous predictors in our models. We will now include a categorical predictor in the model as well.\n\nWhen a categorical predictor is added, this predictor is split in several contrasts (or dummies), where each group is compared to a reference group. In our example Iris data, the variable 'Species' is a categorical variable that indicate the species of flower. This variable can be added as example for a categorical predictor. Contrasts, and thus the dummy coding, can be inspected through `contrasts()`.\n\n2.  **Add species as a predictor to the model specified as `model2`, store it under the name `model3` and interpret the categorical coefficients of this new model.**\n\n```{r, include = params$answers}\n# Create 3rd model with categorical predictor\nmodel3 <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Species,\n             data = data)\n\n# Ask for summary data\nsummary(model3)\n\n# In the output, we see that 'Species' has multiple rows of output, and that one species (Setosa) does not seem to show. Setosa is the reference group. The other two lines are those respecitve groups compared to the setosa group. This means that that the predicted sepal length of a versicolor would be .9558 lower than the predicted value of a setosas with the same values on the other variables.\n\n```\n\n# Part 3: Model comparison\n\nNow you have created multiple models, you can compare how well these models function (compare the model fit). There are multiple ways of testing the model fit and to compare models, as explained in the lecture and the reading material. In this practical, we use the following:\n\n-   AIC (use the function `AIC()` on the model object)\n-   BIC (use the function `BIC()` on the model object)\n-   MSE (use `MSE()` of the `MLmetrics` package, or calculate by transforming the `model$residuals`)\n-   Deviance test (use `anova()` to compare 2 models)\n\n3.  **Compare the fit of the model specified under question 5 and the model specified under question 8. Use all four fit comparison methods listed above. Interpret the fit statistics you obtain/tests you use to compare the fit.**\n\n```{r, include = params$answers}\nAICvalues <- rbind(AIC(model1), AIC(model2))\nBICvalues <- rbind(BIC(model1), BIC(model2))\nMSEvalues <- rbind(mean(model1$residuals^2), \n                    mean(model2$residuals^2))\n\nmodelfitvalues <- cbind(AICvalues, BICvalues, MSEvalues)\nrownames(modelfitvalues) <- c(\"model1\", \"model2\")\ncolnames(modelfitvalues) <- c(\"AIC\", \"BIC\", \"MSE\")\nmodelfitvalues \n\n# We see that the second AIC is lower, and thus this model has a better fit-complexity trade-off. The BIC has the same conclusion as the AIC in this case. The MSE of the second model is lower, and therefore indicates less error and a better fit.\n\n# R2 difference test (deviance)\nanova(model1, model2)\n\n# The residual sum of squares is significantly lower for model 2, indicating a better fit for this model\n```\n\n# Part 4: Residuals: observed vs. predicted\n\nWhen fitting a regression line, the predicted values have some error in comparison to the observed values. The sum of the squared values of these errors is the sum of squares. A regression analysis finds the line such that the lowest sum of squares possible is obtained.\n\nThe image below shows how the predicted (on the blue regression line) and observed values (black dots) differ and how the predicted values have some error (red vertical lines).\n\n![](errorterms.PNG)\n\nWhen having multiple predictors, it becomes harder or impossible to make such a plot as above (you need a plot with more dimensions). You can, however, still plot the observed values against the predicted values and infer the error terms from there.\n\n4.  **Create a dataset of predicted values for model 1 by taking the outcome variable `Sepal.Length` and the `fitted.values` from the model.**\n\n```{r, include = params$answers}\npredvals1           <- cbind(data$Sepal.Length, model1$fitted.values)\ncolnames(predvals1) <- c(\"observed\", \"predicted\")\npredvals1           <- as.data.frame(predvals1)\n```\n\n5.  **Create an observed vs. predicted plot for model 1 (the red vertial lines are no must).**\n\n```{r, include = params$answers}\nobspred1 <- ggplot(data = predvals1, aes(x = observed, y = predicted)) +\n  geom_segment(aes(xend = observed, yend = observed), col = \"red\") +\n  geom_abline(slope = 1, intercept = 0, col = \"blue\") +\n  geom_point() +\n  ggtitle(\"Observed vs. Predicted - model 1\")\n```\n\n6.  **Create a dataset of predicted values and create a plot for model 2.**\n\n```{r, include = params$answers}\npredvals2 <- cbind(data$Sepal.Length, model2$fitted.values)\ncolnames(predvals2) <- c(\"observed\", \"predicted\")\npredvals2 <- as.data.frame(predvals2)\n\nobspred2 <- ggplot(data = predvals2, aes(x = observed, y = predicted)) +\n  geom_segment(aes(xend = observed, yend = observed), col = \"red\") +\n  geom_abline(slope = 1, intercept = 0, col = \"blue\") +\n  geom_point() +\n  ggtitle(\"Observed vs. Predicted - model 2\")\n\n```\n\n7.  **Compare the two plots and discuss the fit of the models based on what you see in the plots. You can combine them in one figure using the `grid.arrange()` function.**\n\n```{r, include = params$answers}\ngrid.arrange(obspred1, obspred2, ncol = 2)\n\n# Above, the observed vs. predicted plots for both model 1 (1 predictor) and model 2 (an additional predictor) are shown. In the second plot, it can be seen that all the red lines are shorter, indicating less error, a lower sum of squares, and thus a better fit.\n```\n\n## Calculating new predicted values\n\nA regression model can be used to predict values for new cases that were not used to built the model. The regression equation always consists of coefficients ($\\beta$s) and observed variables ($X$):\n\n<br>\n\n$$\\hat{y} = \\beta_0 + \\beta_1 * X_{a}* + \\beta_2 * X_b +  \\ldots  + \\beta_n * X_n$$\n","srcMarkdownNoYaml":"\n\n# Part 1: Repeat from last week\n\nWe will use the following packages in this practical:\n\n```{r, message=FALSE, warning=FALSE}\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(gridExtra)\n```\n\nWe will use the same data that we used last week to perform a simple linear regression, the Iris dataset. But now, we will extend on this simple model with multiple variables.\n\nIn order to do this, we first need to load the data again and run the simple model where Sepal length is predicted by Petal width.\n\n```{r}\ndata <- iris # load the data\nhead(iris)   # inspect the data\n\n```\n\n```{r}\n# specify model\nmodel1 <- lm(Sepal.Length ~ Petal.Width, \n             data = data)\n\n# ask for summary\nsummary(model1)\n```\n\n# Part 2: Multiple linear regression\n\nYou can add additional predictors to a model. This can improve the fit and the predictions. When multiple predictors are used in a regression model, it's called a Multiple linear regression. You specify this model as `outcome_variable ~ predictor_1 + predictor_2 + ... + predictor_n`.\n\n1.  **Add Petal length as a second predictor to the model specified as `model1` and store this under the name `model2`, and supply summary statistics. Again, give a substantive interpretation of the coefficients and the model.**\n\n```{r, include = params$answers}\n# Specify additional predictors\nmodel2 <- lm(Sepal.Length ~ Petal.Width + Petal.Length, \n             data = data)\n\n# Ask for summary statistics again\nsummary(model2)\n\n# When comparing the coefficients of model 2 with the coefficients of model 1, we can see that adding a predictor can change the coefficients of other predictors as well (it is a new model after all). In this example, it is notable that the coefficient for petal width has become a negative number, while it was positive in model 1.\n```\n\n## Categorical predictors\n\nUp to here, we only included continuous predictors in our models. We will now include a categorical predictor in the model as well.\n\nWhen a categorical predictor is added, this predictor is split in several contrasts (or dummies), where each group is compared to a reference group. In our example Iris data, the variable 'Species' is a categorical variable that indicate the species of flower. This variable can be added as example for a categorical predictor. Contrasts, and thus the dummy coding, can be inspected through `contrasts()`.\n\n2.  **Add species as a predictor to the model specified as `model2`, store it under the name `model3` and interpret the categorical coefficients of this new model.**\n\n```{r, include = params$answers}\n# Create 3rd model with categorical predictor\nmodel3 <- lm(Sepal.Length ~ Sepal.Width + Petal.Length + Species,\n             data = data)\n\n# Ask for summary data\nsummary(model3)\n\n# In the output, we see that 'Species' has multiple rows of output, and that one species (Setosa) does not seem to show. Setosa is the reference group. The other two lines are those respecitve groups compared to the setosa group. This means that that the predicted sepal length of a versicolor would be .9558 lower than the predicted value of a setosas with the same values on the other variables.\n\n```\n\n# Part 3: Model comparison\n\nNow you have created multiple models, you can compare how well these models function (compare the model fit). There are multiple ways of testing the model fit and to compare models, as explained in the lecture and the reading material. In this practical, we use the following:\n\n-   AIC (use the function `AIC()` on the model object)\n-   BIC (use the function `BIC()` on the model object)\n-   MSE (use `MSE()` of the `MLmetrics` package, or calculate by transforming the `model$residuals`)\n-   Deviance test (use `anova()` to compare 2 models)\n\n3.  **Compare the fit of the model specified under question 5 and the model specified under question 8. Use all four fit comparison methods listed above. Interpret the fit statistics you obtain/tests you use to compare the fit.**\n\n```{r, include = params$answers}\nAICvalues <- rbind(AIC(model1), AIC(model2))\nBICvalues <- rbind(BIC(model1), BIC(model2))\nMSEvalues <- rbind(mean(model1$residuals^2), \n                    mean(model2$residuals^2))\n\nmodelfitvalues <- cbind(AICvalues, BICvalues, MSEvalues)\nrownames(modelfitvalues) <- c(\"model1\", \"model2\")\ncolnames(modelfitvalues) <- c(\"AIC\", \"BIC\", \"MSE\")\nmodelfitvalues \n\n# We see that the second AIC is lower, and thus this model has a better fit-complexity trade-off. The BIC has the same conclusion as the AIC in this case. The MSE of the second model is lower, and therefore indicates less error and a better fit.\n\n# R2 difference test (deviance)\nanova(model1, model2)\n\n# The residual sum of squares is significantly lower for model 2, indicating a better fit for this model\n```\n\n# Part 4: Residuals: observed vs. predicted\n\nWhen fitting a regression line, the predicted values have some error in comparison to the observed values. The sum of the squared values of these errors is the sum of squares. A regression analysis finds the line such that the lowest sum of squares possible is obtained.\n\nThe image below shows how the predicted (on the blue regression line) and observed values (black dots) differ and how the predicted values have some error (red vertical lines).\n\n![](errorterms.PNG)\n\nWhen having multiple predictors, it becomes harder or impossible to make such a plot as above (you need a plot with more dimensions). You can, however, still plot the observed values against the predicted values and infer the error terms from there.\n\n4.  **Create a dataset of predicted values for model 1 by taking the outcome variable `Sepal.Length` and the `fitted.values` from the model.**\n\n```{r, include = params$answers}\npredvals1           <- cbind(data$Sepal.Length, model1$fitted.values)\ncolnames(predvals1) <- c(\"observed\", \"predicted\")\npredvals1           <- as.data.frame(predvals1)\n```\n\n5.  **Create an observed vs. predicted plot for model 1 (the red vertial lines are no must).**\n\n```{r, include = params$answers}\nobspred1 <- ggplot(data = predvals1, aes(x = observed, y = predicted)) +\n  geom_segment(aes(xend = observed, yend = observed), col = \"red\") +\n  geom_abline(slope = 1, intercept = 0, col = \"blue\") +\n  geom_point() +\n  ggtitle(\"Observed vs. Predicted - model 1\")\n```\n\n6.  **Create a dataset of predicted values and create a plot for model 2.**\n\n```{r, include = params$answers}\npredvals2 <- cbind(data$Sepal.Length, model2$fitted.values)\ncolnames(predvals2) <- c(\"observed\", \"predicted\")\npredvals2 <- as.data.frame(predvals2)\n\nobspred2 <- ggplot(data = predvals2, aes(x = observed, y = predicted)) +\n  geom_segment(aes(xend = observed, yend = observed), col = \"red\") +\n  geom_abline(slope = 1, intercept = 0, col = \"blue\") +\n  geom_point() +\n  ggtitle(\"Observed vs. Predicted - model 2\")\n\n```\n\n7.  **Compare the two plots and discuss the fit of the models based on what you see in the plots. You can combine them in one figure using the `grid.arrange()` function.**\n\n```{r, include = params$answers}\ngrid.arrange(obspred1, obspred2, ncol = 2)\n\n# Above, the observed vs. predicted plots for both model 1 (1 predictor) and model 2 (an additional predictor) are shown. In the second plot, it can be seen that all the red lines are shorter, indicating less error, a lower sum of squares, and thus a better fit.\n```\n\n## Calculating new predicted values\n\nA regression model can be used to predict values for new cases that were not used to built the model. The regression equation always consists of coefficients ($\\beta$s) and observed variables ($X$):\n\n<br>\n\n$$\\hat{y} = \\beta_0 + \\beta_1 * X_{a}* + \\beta_2 * X_b +  \\ldots  + \\beta_n * X_n$$\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../styles.css"],"toc":true,"output-file":"lab3.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.32","editor":"visual","theme":["lumen"],"title":"Lab 3","params":{"answers":false}},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}