{
  "hash": "fcf6a420510ac7a7f3abb4feed65d559",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 5\"\noutput-file: \"lab5_answers.html\"\n#output-file: \"lab5.html\"\nparams:\n  answers: true\n---\n\n\n\n\n\n\n\n\n\n\nWe will use the following packages in this practical:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(foreign)\nlibrary(kableExtra)\nlibrary(janitor)\nlibrary(readr)\n```\n:::\n\n\nIn this practical, you will perform regression analyses using `glm()` and inspect variables by plotting these variables, using `ggplot()`.\n\n---\n\n# Part 1: Logistic regression\n\nLogistic regression is a supervised learning algorithm that classifies data into categories, by predicting the probability that an observation falls into a particular category based on its features. In this tutorial we will consider binary classification, where we determine which of two categories a data point belongs to.\n\nThe logistic function can be described as:\n\n$$ P(y = 1|X) = sigmoid(z) = \\frac{1}{1+e^{-z}}   $$\nwhere\n\n$$ z = \\hat{\\beta_0} + \\hat{\\beta_1}x_1 + \\hat{\\beta_2}x_2 + ... + \\hat{\\beta_k}x_k $$\n$z$ is like the linear predictor in linear regression, but it is transformed by the sigmoid function so that results can be interpreted as probabilities (between 0 and 1). The probability is compared to a threshold to determine what class $y$ belongs to based on $X$. You can choose what this threshold is and it is context dependent. For example, if you are predicting the chances of recovery in a clinical trial you might set a very high threshold of 0.90. A common threshold for low-stakes research is 0.50. \n\nThe `glm()` function is used to specify several different models, among which the logistic regression model. The logistic regression model can be specified by setting the `family` argument to _\"binomial\"_. You can save a model in an object and request summary statistics with the `summary()` command. \n\nFor logistic regression, it important to know and check what category the predicted probabilities refer to, so you can interpret the model and it's coefficients correctly. If your outcome variable is coded as a factor, the `glm()` function predicts the 2nd category, which is by default the alphabetical latter one. For example, if the categories are coded as 0 and 1, the probability of belonging to \"1\" is predicted by the model. \n\nWhen a model is stored in an object you can ask for the coefficients _(model\\$coeffients)_, the predicted probabilities of belonging to the 'higher' category category _(model\\$fitted.values)_, and the aic _(model\\$aic)_. To investigate all additional model information that is stored in the object, check out the list of the model by selecting it in the environment-list.\n\n---\n\n## Working with odds and log-odds \n\nBefore we get started with logistic modelling it helps to understand how odds, log-odds, and probability are related. Essentially, they are all just different expressions of the same thing and converting between them involve simple formulas. \n\nCoefficients calculated using the `glm()` function returns log-odds by default. Most of us find it difficult to think in terms of log-odds, so instead we convert them to odds (or odds-ratios) using the `exp()` function. If we want to go from odds to log-odds, we just take the logarithm using `log()`. \n\nAn odds-ratio is the probability of success and is defined as $Odds = \\frac{P}{1-P}$, where $P$ is the probability of an event happening and $1-P$ is the probability that it does not happen. For example, if we have an 80% chance of a sunny day, then we have a 20% chance of a rainy day. The odds would then equal $\\frac{.80}{.20} = 4$, meaning the odds of a sunny day are 4 to 1. \nLet's consider this further with an example. \n\nThe code below creates a data frame called `data` with a column called `conc` showing the number of trials wherein different concentrations of the peptide-C protein inhibited the flow of current across a membrane. The `yes` column contains counts of trials where this occured.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- data.frame(conc = c(0.1, 0.5, 1, 10, 20, 30, 50, 70, 80, 100, 150),\n                   no = c(7, 1, 10, 9, 2, 9, 13, 1, 1, 4, 3),\n                   yes = c(0, 0, 3, 4, 0, 6, 7, 0, 0, 1 ,7)\n                   ) \ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    conc no yes\n1    0.1  7   0\n2    0.5  1   0\n3    1.0 10   3\n4   10.0  9   4\n5   20.0  2   0\n6   30.0  9   6\n7   50.0 13   7\n8   70.0  1   0\n9   80.0  1   0\n10 100.0  4   1\n11 150.0  3   7\n```\n\n\n:::\n:::\n\n\n1. __Add the following variables to the dataset:__\n\n* **the total number of trials for each observation (i.e., the sum of the `no` and `yes` trials for each row)**\n* **the proportion of yes trials in each row (i.e. yes divided by the total)**\n* **the log-odds of inhibition for each row (i.e. the log-odds of `yes` vs `no`)**\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata <- \n  data %>% \n  mutate(total = no + yes,\n         prop = yes / total,\n         logit = qlogis(prop)\n         )\n\n# The `qlogis()` function is equivalent to the log-odds (i.e, logit) function.\n```\n:::\n\n\n2. __Inspect the new columns. Do you notice anything unusual?__\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  conc no yes total      prop      logit\n1  0.1  7   0     7 0.0000000       -Inf\n2  0.5  1   0     1 0.0000000       -Inf\n3  1.0 10   3    13 0.2307692 -1.2039728\n4 10.0  9   4    13 0.3076923 -0.8109302\n5 20.0  2   0     2 0.0000000       -Inf\n6 30.0  9   6    15 0.4000000 -0.4054651\n```\n\n\n:::\n\n```{.r .cell-code}\n#There are many zero proportions which produce logit values of infinity. We can work around this issue by adding a constant (usually 0.5) to all cells before calculating the log-odds. We add the same value to the numerator and denominator of our odds formula, so we don't change the relative interpretations of the odds. We could also add a 1 to each cell. This option is conceptually interesting because the log of 1 equals 0. It's almost like we're adding zero to the odds and still correcting the issue.\n```\n:::\n\n\n\n3. __Add a new column to your dataset containing the corrected odds.__\n\nYou can compute the value of this column using the following formulation of the log-odds:\n\n$$ log(odds) = log(\\frac{yes + 0.5} {no + 0.5}) $$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrobustLogit <- function(x, y) log((x + 0.5) / (y + 0.5))\n\ndata <- data %>% \n  mutate(logit2 = robustLogit(yes, no))\n\ndata\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    conc no yes total      prop      logit     logit2\n1    0.1  7   0     7 0.0000000       -Inf -2.7080502\n2    0.5  1   0     1 0.0000000       -Inf -1.0986123\n3    1.0 10   3    13 0.2307692 -1.2039728 -1.0986123\n4   10.0  9   4    13 0.3076923 -0.8109302 -0.7472144\n5   20.0  2   0     2 0.0000000       -Inf -1.6094379\n6   30.0  9   6    15 0.4000000 -0.4054651 -0.3794896\n7   50.0 13   7    20 0.3500000 -0.6190392 -0.5877867\n8   70.0  1   0     1 0.0000000       -Inf -1.0986123\n9   80.0  1   0     1 0.0000000       -Inf -1.0986123\n10 100.0  4   1     5 0.2000000 -1.3862944 -1.0986123\n11 150.0  3   7    10 0.7000000  0.8472979  0.7621401\n```\n\n\n:::\n:::\n\n\n4. __Fit a logistic regression model where:__\n\n* **`prop` is the outcome**\n* **`conc` is the only predictor**\n* **the number of total trials per row are used as weights (we need this because a different number of trials can go into defining each observation of `prop`)**\n\n**Interpret the slope estimate.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(glm(prop ~ conc, \n            family = binomial, \n            weights = total, \n            data = data))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = prop ~ conc, family = binomial, data = data, weights = total)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -1.32701    0.33837  -3.922 8.79e-05 ***\nconc         0.01215    0.00496   2.450   0.0143 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 16.683  on 10  degrees of freedom\nResidual deviance: 10.389  on  9  degrees of freedom\nAIC: 30.988\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# A unit increase in conc increases the log-odds of inhibition by 0.0121 units, and this increase is statistically significant.\n\n# If we exponentiate the slope estimate, we can get an interpretation in odds units, but the effect becomes multiplicative instead of additive. So for every unit increase in conc, the odds of inhibition are 1.01215 times higher. Note then that odds above 1 indicate inhibition is x-times higher, while odds below 1 indicate inhibition is x-times less.\n```\n:::\n\n\n---\n\n## Titanic data\n\nYou will work with the `titanic` data set which you can find in the surfdrive folder, containing information on the fate of passengers on the infamous voyage. \n\n* `Survived`: this is the outcome variable that you are trying to predict, with 1 meaning a passenger survived and 0 meaning they did not\n* `Pclass`: this is the ticket class the passenger was travelling on, with 1, 2, and 3 representing 1st, 2nd and 3rd class respectively\n* `Age`: this is the age of the passenger in years\n* `Sex`: this is the sex of the passenger, either male or female\n\n5. __Read in the data from the \"titanic.csv\" file, selecting only the variables `Survived`, `Pclass`, `Sex` and `Age`. If necessary, correct the class of the variables.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic <- read_csv(\"titanic.csv\") %>% \n  mutate(Survived = as.factor(Survived),\n         Sex = as.factor(Sex),\n         Pclass = as.factor(Pclass))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 891 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Sex\ndbl (3): Survived, Pclass, Age\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n:::\n\n\n\n6. __What relationships do you expect to find between the predictor variables and the outcome?__\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# We could say that:\n# class is related to the outcome as passengers travelling on a higher class ticket have a higher probability of survival\n# sex is related to the outcome as women have a higher probability of survival\n# age is related to the outcome as younger passengers have a higher probability of survival\n```\n:::\n\n\n\n7. __Investigate how many passengers survived in each class. You can do this visually by creating a bar plot, or by using the `table()` function. Search `??table` for more information.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic %>% \n  ggplot(aes(Pclass, fill = Survived)) +\n  geom_bar(position = \"dodge\") +\n  labs(x = \"Passenger Class\",\n       y = \"Count\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](lab5_answers_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# The bar plot clearly shows that people in lower class were less likely to survive.\n# We can also use the `prop.table()` function to investigate this. The argument `margin = 1` turns the counts to marginal proportions.\n\ntitanic %$% \n  table(Pclass, Survived) %>% \n  prop.table(margin = 1) %>% \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      Survived\nPclass    0    1\n     1 0.37 0.63\n     2 0.53 0.47\n     3 0.76 0.24\n```\n\n\n:::\n:::\n\n\n8. __Similarly, investigate the relationship between survival and sex by creating a bar plot and a table.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic %$% \n  table(Sex, Survived) %>% \n  prop.table(margin = 1) %>% \n  round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        Survived\nSex         0    1\n  female 0.26 0.74\n  male   0.81 0.19\n```\n\n\n:::\n\n```{.r .cell-code}\n# The table shows the proportion of males and females that survived versus those who did not survive. Females are much more likely to have survived than males.\n\ntitanic %>% \n  ggplot(aes(Sex, fill = Survived)) +\n  geom_bar(position = \"dodge\") +\n  labs(x = \"Sex\",\n       y = \"Count\") +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](lab5_answers_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\n9. __Investigate the relationship between age and survival by creating a histogram of the age of survivors versus non-survivors.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic %>% \n  ggplot(aes(Age, fill = Survived)) +\n  geom_histogram(colour = \"white\") +\n  labs(x = \"Age\",\n       y = \"Count\") +\n  facet_wrap(~Survived) +\n  theme_bw()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](lab5_answers_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# The distribution of age is different for survivors and non-survivors. Younger passengers have higher chances of survival compared to older passengers.\n```\n:::\n\n\n\n---\n\n## No predictors\n\n10. __Specify a logistic regression model where \"Survived\" is the outcome and there are no predictors.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm(Survived ~ 1, \n    family = binomial, \n    data = titanic) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Survived ~ 1, family = binomial, data = titanic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -0.47329    0.06889   -6.87  6.4e-12 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.7  on 890  degrees of freedom\nResidual deviance: 1186.7  on 890  degrees of freedom\nAIC: 1188.7\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n```{.r .cell-code}\n# A logistic regression without any predictors is simply modelling the log-odds of survival for the entire population (the intercept, beta0).\n# The log-odds are -0.473, and the odds are $exp(-0.473) = 0.623$.\n\n# We can also get the odds from a frequency table: the probability of survival is $342/549 = 0.623$. The log-odds equals exp(beta0) = -0.473.\n\ntitanic %>% \n  count(Survived) %>% \n  mutate(prop = prop.table(n)) %>% \n  kbl(digits = 2) %>% \n  kable_paper(bootstrap_options = \"striped\", full_width = FALSE)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\" lightable-paper\" style='font-family: \"Arial Narrow\", arial, helvetica, sans-serif; width: auto !important; margin-left: auto; margin-right: auto;'>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Survived </th>\n   <th style=\"text-align:right;\"> n </th>\n   <th style=\"text-align:right;\"> prop </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 0 </td>\n   <td style=\"text-align:right;\"> 549 </td>\n   <td style=\"text-align:right;\"> 0.62 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 1 </td>\n   <td style=\"text-align:right;\"> 342 </td>\n   <td style=\"text-align:right;\"> 0.38 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n---\n\n## Binary predictor \n\n11. __Specify a logistic regression model where \"Survived\" is the outcome and \"Sex\" is the only predictor.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm(Survived ~ Sex, \n    family = binomial, \n    data = titanic) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Survived ~ Sex, family = binomial, data = titanic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   1.0566     0.1290   8.191 2.58e-16 ***\nSexmale      -2.5137     0.1672 -15.036  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.7  on 890  degrees of freedom\nResidual deviance:  917.8  on 889  degrees of freedom\nAIC: 921.8\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n12. __What does the intercept mean? What are the odds and what are the log-odds of survival for males?__\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# In the model with one dichotomous predictor we are modelling logit(p) = beta0 + beta1*male.\n\n# The intercept is the log-odds of survival for women (1.0566), since the reference group is female.\n\n# The log-odds of survival for men is -2.5137 lower than for women. The odds of survival for men is 0.081, or 92% lower than females.\n```\n:::\n\n\n---\n\n## Categorical predictor (more than 2 categories)\n\n13. __Specify a logistic regression model where \"Survived\" is the outcome and \"Pclass\" is the only predictor.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglm(Survived ~ Pclass, \n    family = binomial, \n    data = titanic) %>% \n  summary()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Survived ~ Pclass, family = binomial, data = titanic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)   0.5306     0.1409   3.766 0.000166 ***\nPclass2      -0.6394     0.2041  -3.133 0.001731 ** \nPclass3      -1.6704     0.1759  -9.496  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.7  on 890  degrees of freedom\nResidual deviance: 1083.1  on 888  degrees of freedom\nAIC: 1089.1\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n14. __Which category is the reference group? What are their odds of survival?__\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The reference group are 1st class passengers, represented by the intercept.\n# The log-odds of survival for 1st class passengers is 0.5306.\n# The odds are 1.70, meaning 1st class passengers are 70% more likely to survive.\n```\n:::\n\n\n15. __What are the chances of survival for 2nd and 3rd class passengers?__\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For 2nd class passengers, the log-odds of survival is -0.6394.\n# The odds are  0.527, meaning 2nd class passengers are 47% less likely to survive than 1st class passengers.\n\n# For 3rd class passengers, the log-odds of survival is -1.646.\n# The odds are 0.188, meaning 3nd class passengers are 81% less likely to survive than 1st class passengers.\n```\n:::\n\n\n---\n\n## Continuous predictor\n\n16. __Specify a logistic regression model where \"Survived\" is the outcome and \"Age\" is the only predictor.__\n\n**Save this model as you will come back to it later.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit1 <- glm(Survived ~ Age, \n            family = binomial, \n            data = titanic)\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Survived ~ Age, family = binomial, data = titanic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept) -0.14327    0.17209  -0.832   0.4051  \nAge         -0.01120    0.00539  -2.077   0.0378 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.7  on 890  degrees of freedom\nResidual deviance: 1182.3  on 889  degrees of freedom\nAIC: 1186.3\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n:::\n\n\n17. __What does the intercept mean when there is a continuous predictor?__\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# In the case of a continuous predictor there is no real reference group. Instead, the intercept is the log-odds of survival when age = 0. In this model, the log-odds of survival for passengers of age 0 is -0.143, corresponding with the odds of survival at 0.867 (= exp(log odds)).\n```\n:::\n\n\n18. __How are the odds and log-odds interpreted for a continuous predictor?__\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For continuous predictors, the log-odds either increase or decrease with every unit increase in the continuous predictor. So, in our model:\n# For every increase in age of one year, the log-odds of survival decrease by -0.011, meaning that as age increases the chances of survival decrease.\n# For every increase in age of one year, the odds of survival are 0.99 (= exp(-0.0112)) times the odds of those with one age unit less, or -1.09%.\n```\n:::\n\n\n---\n\n## Multinomial model with an interaction term\n\n19. __Specify a logistic regression model `Survived` is the outcome and `Pclass` plus an interaction between `Sex` and `Age` as the predictor.__\n\n**Save this model as we will return to it later.**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit2 <- glm(Survived ~ Pclass + Sex*Age, family = binomial, data = titanic)\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Survived ~ Pclass + Sex * Age, family = binomial, \n    data = titanic)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.834344   0.414982   6.830 8.49e-12 ***\nPclass2     -1.264399   0.273220  -4.628 3.70e-06 ***\nPclass3     -2.412614   0.250004  -9.650  < 2e-16 ***\nSexmale     -1.262875   0.433364  -2.914 0.003567 ** \nAge         -0.004202   0.011426  -0.368 0.713083    \nSexmale:Age -0.048460   0.014576  -3.325 0.000885 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  793.82  on 885  degrees of freedom\nAIC: 805.82\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n:::\n\n\n20. __How is the significant interaction term interpreted in this model?__\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# The interaction between age and sex is significant, suggesting the slopes for age on survival are different for males and females.\n```\n:::\n\n\n---\n\n# Part 2: Model fit \n\nModel selection is an important step and there are several metrics for assessing model fit to help us select the best performing model. We will use deviance and information criterion to compare the fit of two models you saved before: `fit1` and `fit2`. \n\n---\n\n## Deviance \n\n**Deviance** is measure of the goodness-of-fit in a GLM where lower deviance indicates a better fitting model. R reports two types of deviance:\n\n* **null deviance:** how well the outcome is predicted by the intercept-only model\n* **residual deviance:** how well the outcome is predicted by the model with the predictors added\n\n21. __Get the model summaries and indicate what the null and residual deviance are.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# You can use the `summary()` command to get the deviance statistics for each model. The null and residual deviance are below the model coefficients. \n\nsummary(fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Survived ~ Age, family = binomial, data = titanic)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)  \n(Intercept) -0.14327    0.17209  -0.832   0.4051  \nAge         -0.01120    0.00539  -2.077   0.0378 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.7  on 890  degrees of freedom\nResidual deviance: 1182.3  on 889  degrees of freedom\nAIC: 1186.3\n\nNumber of Fisher Scoring iterations: 4\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = Survived ~ Pclass + Sex * Age, family = binomial, \n    data = titanic)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  2.834344   0.414982   6.830 8.49e-12 ***\nPclass2     -1.264399   0.273220  -4.628 3.70e-06 ***\nPclass3     -2.412614   0.250004  -9.650  < 2e-16 ***\nSexmale     -1.262875   0.433364  -2.914 0.003567 ** \nAge         -0.004202   0.011426  -0.368 0.713083    \nSexmale:Age -0.048460   0.014576  -3.325 0.000885 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1186.66  on 890  degrees of freedom\nResidual deviance:  793.82  on 885  degrees of freedom\nAIC: 805.82\n\nNumber of Fisher Scoring iterations: 5\n```\n\n\n:::\n\n```{.r .cell-code}\n# For model 1, the null deviance is 1186.7 and the residual deviance is 1182.3 For model 2, the null deviance is 1186.66 and the residual deviance is 793.82\n```\n:::\n\n\nWe can use the `anova()` function to perform an analysis of deviance that compares the difference in deviances between competing models.  \n\n22. __Compare the fit of model 1 with the fit of model 2 using `anova() and `test = \"Chisq\"`.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(fit1, fit2, test = \"Chisq\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Deviance Table\n\nModel 1: Survived ~ Age\nModel 2: Survived ~ Pclass + Sex * Age\n  Resid. Df Resid. Dev Df Deviance  Pr(>Chi)    \n1       889    1182.28                          \n2       885     793.82  4   388.45 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# The analysis of deviance indicates that there is a reduction in residual deviance of 388.46 that is statistically significant. Model 2 is a better model. For a binomial model, the statistical test should be the chi-square difference test.\n```\n:::\n\n\n---\n\n## Information criteria \n\n**AIC** is the *Akaike's Information Criterion*, a method for assessing model quality through comparison of related models. AIC is based on the deviance but introduces a penalty for more complex models. The number itself is not meaninful, and it is only useful when comparing models against one another. Like deviance, the model with the lowest AIC is best.\n\n23. __Use the `AIC()` function to get the AIC value for model 1 and model 2.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(fit1, fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     df       AIC\nfit1  2 1186.2782\nfit2  6  805.8245\n```\n\n\n:::\n\n```{.r .cell-code}\n# The AIC for model 2 is lower than the AIC for model 1, indicating that model 2 has a better fit\n```\n:::\n\n\n**BIC** is the Bayesian Information Criterion and is very similar to AIC, but penalises a complex model *more* than the AIC would. Complex models will have a larger score indicating worse fit. One difference to the AIC is that the probability of selecting the correct model with the BIC increases as the sample size of the training set increases. \n\n24. __Use the `BIC()` function to get the BIC value for model 1 and model 2.__\n\n::: {.cell}\n\n```{.r .cell-code}\nBIC(fit1, fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     df       BIC\nfit1  2 1195.8629\nfit2  6  834.5785\n```\n\n\n:::\n\n```{.r .cell-code}\n# The BIC for model 2 is lower than the BIC for model 1, indicating that model 2 has a better fit\n```\n:::\n\n\n\n25. __Which model should we proceed with?__\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Model 2, as it has lower residual deviance, AIC and BIC.\n```\n:::\n\n\n---\n\n# Part 3: Predicted probabilites\n\nOften with logistic regression we are interested in how well our model can predict the outcome. The `predict()` function allows us to do this, taking the model and some data as its main arguments. Additionally, you can specify whether you want `predict()` to give you predictions as logit or probabilities. \n\nProceed using the model you selected in the previous question. \n\n26. __Use the `predict()` function to generate predicted probabilities for the multivariate logistic model. `predict()` takes the following arguments:__\n\n* **`object`, i.e. the logistic model**\n* **`newdata`, i.e. a data set where we want to predict the outcome  (we will use `titanic`)**\n* **`type`, i.e. can be `\"logit\"` for log-odds or `\"response\"` for probabilities (we will use `type = \"response\"`)**\n* **`se.fit`, i.e. set to `TRUE` to estimate the standard error of the probabilities**\n\n**Remember to save the output to an object.**\n\n::: {.cell}\n\n```{.r .cell-code}\npreds <- data.frame(predict(fit2, type = \"response\", se.fit = TRUE))\n\n# The `type=\"response\"` option tells R to output probabilities of the form P(Y = 1|X), as opposed to other information such as the logit.\n```\n:::\n\n\n27. __Add the predicted probabilities and standard errors to the data set.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic$probs <- preds$fit\ntitanic$se    <- preds$se.fit\n\n# You can access the predicted probabilities using `$fit` and the standard errors as `$se.fit`.\n```\n:::\n\n\n24. __Calculate the confidence intervals for the predicted probabilities and add them to the data.__\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntitanic %<>% \n  mutate(ci_Lower = probs - 1.96 * se, \n         ci_Upper = probs + 1.96 * se)\n\n# Now we have all the information to visualize our predicted probabilities using `ggplot()`:\n# Note: `geom_ribbon()` allows us to display an interval for the `y` values using `ymin` and `ymax`.* \n\ntitanic %>% \n  ggplot(aes(x = Age, y = probs)) + \n    geom_ribbon(aes(ymin = ci_Lower, ymax = ci_Upper, fill = Pclass), alpha = 0.2) +\n    geom_line(aes(color = Pclass), lwd = 1) + \n    ylab(\"Probability of Survival\") +\n    theme_bw() +\n    facet_wrap(vars(Sex))\n```\n\n::: {.cell-output-display}\n![](lab5_answers_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n---\n",
    "supporting": [
      "lab5_answers_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}