{
  "hash": "21d9bfc7ba269d26c4fbbf65acb7d8e2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 6\"\n#output-file: \"lab6_answers.html\"\noutput-file: \"lab6.html\"\nparams:\n  answers: false\n---\n\n# Part 1: Repeat from last week \n\nWe will use the following packages in this practical:\n\n* `dplyr` for manipulation\n* `magrittr` for piping\n* `readr` for reading data\n* `ggplot` for plotting\n* `kableExtra` for tables\n* `library(pROC)`, `library(regclass)`, and `library(caret)` for model diagnostics\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(magrittr)\nlibrary(ggplot2)\nlibrary(kableExtra)\nlibrary(readr)\nlibrary(pROC)\nlibrary(regclass)\nlibrary(caret)\n```\n:::\n\n\n## Loading the data\n\nIn this practical, you will perform logistic regression analyses again using `glm()` and discuss model assumptions and diagnostics `titanic` data set.\n\n**1. Read in the data from the \"titanic.csv\" file, which we also used for the previous practical.**\n\n\n\n## Logistic regression \n\n2. __Fit the following two logistic regression models and save them as `fit1` and `fit2`:__\n\n* **`Survived ~ Pclass`**\n* **`Survived ~ Age + Pclass*Sex`**\n\n\n\n\n# Part 2: Model assumptions\n\n## Binary dependent variable \n\nThe first outcome in a logistic regression is that the outcome should be binary and therefore follow a binomial distribution. This is easy to check: you just need to be sure that the outcome can only take one of two responses. You can plot the responses of the outcome variable to visually check this if you want. In our case, the possible outcomes are:\n\n* Survived (coded 1)\n* Did not survive (coded 0)\n\n3. __Visualise the responses of the outcome variable `Survived` using `ggplot()`.__\n\n\n\n## Balanced outcomes\n\nIf you are using logistic regression to make predictions/classifications then the accuracy will be affected by imbalance in the outcome classes. Notice that in the plot you just made there are more people who did not survive than who did survive. A possible consequence is reduced accuracy in classification of survivors. \n\nA certain amount of imbalance is expected and can be handled well by the model in most cases. The effects of this imbalance is context-dependent. Some solutions to serious class imbalance are down-sampling or weighting the outcomes to balance the importance placed on the outcomes by the model. \n\n## Sufficiently large sample size\n\nSample size in logistic regression is a complex issue, but some suggest that it is ideal to have 10 cases per candidate predictor in your model. The minimum number of cases to include is $N = \\frac{10*k} {p}$, where $k$ is the number of predictors and $p$ is the smallest proportion of negative or positive cases in the population. \n\n4. __Calculate the minimum number of positive cases needed in the model `fit1`.__\n\n\n\n## Predictor matrix is full-rank\n\nYou learned about this assumption in the linear regression practicals, but to remind you:\n\n* there need to be more observations than predictors (n > P)\n* there should be no multicollinearity among the linear predictors\n\n5. __Check that there is no multicollinearity in the logistic model.__\n\n\n\n## Continuous predictors are linearly related to the $logit(\\pi)$\n\nLogistic regression models assume a linear relationship between predictor variables and the logit of the outcome variable. This assumption is mainly concerned with continuous predictors. Since we only have one continuous predictor (`Age`) we can plot the relationship between `Age` and the logit of `Survived`. \n\n6. __Get the predicted values of `fit2` on the logit scale and bind them to the `titanic` data.__\n\n\n\n7. __Plot the relationship between `Age` and `logit` and interpret it.__\n\n\n\n8. __How should we deal with variables that are not linearly related to the logit?__\n\n\n\n## No influential values or outliers\n\nInfluential values are extreme individual data points that can affect the fit of the logistic regression model. They can be visualised using Cook's distance and the Residuals vs Leverage plot. \n\n9. __Use the `plot()` function to visualise the outliers and influential points of `fit2`.__\n\n**Hint: you need to specify the correct plot with the `which` argument. Check the lecture slides or search `??plot` if you are unsure.**\n\n\n\n10. __Are there any influential cases in the Leverage vs Residuals plot? If so, what would you do?__\n\n\n\n## Differences to linear regressoin\n\nLastly, it is important to note that the assumptions of a linear regression do not *all* map to logistic regression. In logistic regression, we do not need:\n\n* constant, finite error variance\n* normally distributed errors\n\nHowever, deviance residuals are useful for determining if the individual points are not fit well by the model. \n\n*Hint: you can use some of the code from the lecture for the next few questions.*\n\n11. __Use the `resid()` function to get the deviance residuals for `fit2`.__\n\n\n\n12. __Compute the predicted logit values for the model.__\n\n\n\n13. __Plot the deviance residuals.__\n\n\nPearson residuals can also be useful in logistic regression. They measure deviations between the observed and fit1ted values. Pearson residuals are easier to plot than deviance residuals as the `plot()` function can be used. \n\n14. __Plot the pearson residuals for the model.__\n\n\n\n\n# Part 3: Predicted probabilities\n\nIn last week's practical, you learned how to use the `predict()` function to calculate predicted probabilites using the models. This week we will create predicted probabilities for the final two models from last week compare the results by using the confusion matrix. \n\n15. __Use the `predict()` function to get model predicted probabilities for `fit1` and `fit2`.__\n\n\n\n16. __Create model predicted classifications for survival, for `fit1` and `fit2`.__\n\n\n\n## Confusion matrix \n\nYou can read about the confusion matrix on [this Wikipedia page](https://en.wikipedia.org/w/index.php?title=Sensitivity_and_specificity&oldid=862159646#Confusion_matrix). This section tells you how to get some useful metrics from the confusion matrix to evaluate model performance.\n\n17. __Create two confusion matrices (one each for each model) using the classifications from the previous question. You can use the `table()` function, providing the modeled outcome as the `true` parameter and the classifications as the `pred` parameter.__\n\n\n\n\n\n\n18. __Based on the confusion matrices, which model do you think makes better predictions?__\n\n\n\n19. __Calculate the accuracy, sensitivity, and specificity, false positive rate, positive and negative predictive values from the confusion matrix of the model that makes the best predictions.__\n\n\n\n20. __Explain what the difference metrics mean in substantive terms?__\n\n\n\n21. __What does it mean for a model to have such low specificity, but high sensitivity?__\n\n\n\nThe `confusionMatrix()` function from the `caret` package can do a lot of this for us. The function takes three arguments:\n\n* `data` - a vector of predicted classes (in factor form)\n* `reference` - a vector of true classes (in factor from)\n* `positive` - a character string indicating the 'positive' outcome. If not specified, the confusion matrix assumes that the first specified category is the positive outcome.\n\nYou can type `??confusionMatrix` into the console to learn more. \n\n\n\n",
    "supporting": [
      "lab6_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}